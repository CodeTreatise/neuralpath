{
  "metadata": {
    "title": "AI Evolution Timeline",
    "description": "A comprehensive timeline of AI history from mathematical foundations to modern LLMs. Designed as a learning resource.",
    "version": "2.0",
    "created": "2026-01-12",
    "author": "GenAI Course Research",
    "target_learner": "Developer learning to become RAG/GenAI engineer"
  },
  "events": [
    {
      "id": "ada-lovelace-1843",
      "year": 1843,
      "month": null,
      "type": "concept",
      "title": "Ada Lovelace - First Algorithm",
      "what": "Ada Lovelace wrote the first computer algorithm for Charles Babbage's Analytical Engine. She envisioned machines could go beyond pure calculation to create music and art.",
      "why_it_matters": "First person to recognize computers could do more than arithmetic. Her notes contain what is considered the first computer program ever written.",
      "impact": "Conceptual foundation for general-purpose computing. Programming languages and AI Day named after her.",
      "authors": [
        {
          "name": "Ada Lovelace",
          "born": 1815,
          "died": 1852,
          "nationality": "British",
          "role": "Mathematician",
          "affiliation": "Independent",
          "contribution": "First computer algorithm, vision of creative machines"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Notes on the Analytical Engine",
        "year": 1843
      },
      "learn": [
        {
          "title": "Ada Lovelace - Computerphile",
          "url": "https://www.youtube.com/watch?v=QgUVrzkQgds",
          "type": "video",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Ada Lovelace - Biography",
          "url": "https://www.britannica.com/biography/Ada-Lovelace",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "history",
        "computing",
        "foundational",
        "algorithm"
      ],
      "era": "prerequisites"
    },
    {
      "id": "boolean-algebra-1847",
      "year": 1847,
      "month": null,
      "type": "concept",
      "title": "Boolean Algebra",
      "what": "George Boole created a system of algebra using only TRUE (1) and FALSE (0). Defined operations AND, OR, NOT that can express any logical statement mathematically.",
      "why_it_matters": "All digital computers are built from boolean logic gates. Every CPU operation, every IF statement, every bit in memory is boolean. Without this, no computers, no AI.",
      "impact": "Enabled digital circuit design 100 years later. Every computer chip uses billions of boolean gates.",
      "authors": [
        {
          "name": "George Boole",
          "born": 1815,
          "died": 1864,
          "nationality": "British",
          "role": "Mathematician",
          "affiliation": "Lincoln (1847), Queen's College Cork (1849-1864)",
          "background": "Self-taught mathematician from working-class family. Published 'Mathematical Analysis of Logic' while working as a schoolmaster in Lincoln. Became first professor of mathematics at Queen's College Cork in 1849.",
          "contribution": "Invented boolean algebra, showed logic could be mathematical"
        }
      ],
      "work": {
        "type": "book",
        "title": "The Mathematical Analysis of Logic",
        "year": 1847,
        "url": "https://www.gutenberg.org/ebooks/36884"
      },
      "learn": [
        {
          "title": "Boolean Logic & Logic Gates - Crash Course",
          "url": "https://www.youtube.com/watch?v=gI-qXk7XojA",
          "type": "video",
          "duration": "11 min",
          "difficulty": "beginner"
        },
        {
          "title": "Logic Gates - Khan Academy",
          "url": "https://www.khanacademy.org/computing/ap-computer-science-principles/computers-101/logic-gates-and-circuits/a/logic-gates",
          "type": "tutorial",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "George Boole - Stanford Encyclopedia of Philosophy",
          "url": "https://plato.stanford.edu/entries/boole/",
          "type": "encyclopedia"
        },
        {
          "title": "Boolean Algebra - Britannica",
          "url": "https://www.britannica.com/topic/Boolean-algebra",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "logic",
        "mathematics",
        "foundational",
        "hardware"
      ],
      "era": "prerequisites"
    },
    {
      "id": "predicate-logic-1879",
      "year": 1879,
      "month": null,
      "type": "concept",
      "title": "Predicate Logic (First-Order Logic)",
      "what": "Gottlob Frege created a formal language for expressing logical statements precisely. Introduced quantifiers: ∀ (for all), ∃ (there exists). Allowed statements like 'For all X, if X is human, X is mortal.'",
      "why_it_matters": "Early AI (1950s-1980s) was entirely based on formal logic. Expert systems used IF-THEN rules. Knowledge representation, theorem provers, and Prolog all use predicate logic.",
      "impact": "Foundation for symbolic AI, knowledge graphs, and logical reasoning systems still used today.",
      "authors": [
        {
          "name": "Gottlob Frege",
          "born": 1848,
          "died": 1925,
          "nationality": "German",
          "role": "Mathematician & Philosopher",
          "affiliation": "University of Jena, Germany",
          "background": "Created modern mathematical logic. Considered father of analytic philosophy. Work ignored during his lifetime, recognized posthumously.",
          "contribution": "Invented predicate logic, formalized mathematical reasoning"
        }
      ],
      "work": {
        "type": "book",
        "title": "Begriffsschrift (Concept Script)",
        "year": 1879,
        "url": "https://en.wikipedia.org/wiki/Begriffsschrift"
      },
      "learn": [
        {
          "title": "Propositional Logic - Brilliant.org",
          "url": "https://brilliant.org/wiki/propositional-logic/",
          "type": "tutorial",
          "difficulty": "beginner"
        },
        {
          "title": "First-Order Logic - Stanford Introduction to Logic",
          "url": "https://intrologic.stanford.edu/chapters/chapter_08.html",
          "type": "course",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Gottlob Frege - Stanford Encyclopedia of Philosophy",
          "url": "https://plato.stanford.edu/entries/frege/",
          "type": "encyclopedia"
        },
        {
          "title": "Frege's Logic - Stanford Encyclopedia",
          "url": "https://plato.stanford.edu/entries/frege-logic/",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "logic",
        "mathematics",
        "foundational",
        "symbolic-ai"
      ],
      "era": "prerequisites"
    },
    {
      "id": "hilbert-problems-1900",
      "year": 1900,
      "month": 8,
      "type": "event",
      "title": "Hilbert's 23 Problems",
      "what": "David Hilbert presented 23 unsolved problems at the International Congress of Mathematicians. Problem #2 asked if mathematics could be complete and consistent. Problem #10 (Entscheidungsproblem) asked for a decision procedure for all mathematical statements.",
      "why_it_matters": "These problems drove mathematical logic research for decades. Gödel's incompleteness and Turing's computability work directly addressed these challenges.",
      "impact": "Set the agenda for 20th century mathematics and indirectly led to computer science.",
      "authors": [
        {
          "name": "David Hilbert",
          "born": 1862,
          "died": 1943,
          "nationality": "German",
          "role": "Mathematician",
          "affiliation": "University of Göttingen",
          "contribution": "Formalist program, Hilbert's problems"
        }
      ],
      "work": {
        "type": "lecture",
        "title": "Mathematical Problems",
        "year": 1900
      },
      "learn": [
        {
          "title": "Hilbert's Problems Explained",
          "url": "https://www.youtube.com/watch?v=zTt_MvjKFzc",
          "type": "video",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Hilbert's Problems - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Hilbert%27s_problems",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "mathematics",
        "logic",
        "foundational"
      ],
      "era": "prerequisites"
    },
    {
      "id": "godel-incompleteness-1931",
      "year": 1931,
      "month": null,
      "type": "paper",
      "title": "Gödel's Incompleteness Theorems",
      "what": "Kurt Gödel proved that any consistent formal system powerful enough to express arithmetic will contain true statements that cannot be proven within that system. Mathematics has inherent limits.",
      "why_it_matters": "Shows fundamental limits to what rule-based systems can do. You cannot build a complete, consistent reasoning system. Influenced philosophical debates about whether machines can truly 'think' or just follow rules.",
      "impact": "Sparked work on computability (Turing). Philosophical foundation for understanding limits of AI.",
      "authors": [
        {
          "name": "Kurt Gödel",
          "born": 1906,
          "died": 1978,
          "nationality": "Austrian-American",
          "role": "Logician & Mathematician",
          "affiliation": "University of Vienna, later Institute for Advanced Study (Princeton)",
          "background": "One of greatest logicians in history. Close friend of Einstein at Princeton. Died of starvation due to paranoia about food poisoning.",
          "contribution": "Proved incompleteness theorems, showed limits of formal systems"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Über formal unentscheidbare Sätze (On Formally Undecidable Propositions)",
        "year": 1931,
        "published_in": "Monatshefte für Mathematik und Physik",
        "url": "https://www.w-k-essler.de/pdfs/goedel.pdf"
      },
      "learn": [
        {
          "title": "Gödel's Incompleteness Theorem - Veritasium",
          "url": "https://www.youtube.com/watch?v=HeQX2HjkcNo",
          "type": "video",
          "duration": "22 min",
          "difficulty": "beginner"
        },
        {
          "title": "Gödel's Theorems - Numberphile",
          "url": "https://www.youtube.com/watch?v=O4ndIDcDSGc",
          "type": "video",
          "duration": "14 min",
          "difficulty": "beginner"
        },
        {
          "title": "Gödel Without Tears (free book)",
          "url": "https://www.logicmatters.net/igt/",
          "type": "book",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Gödel's Incompleteness Theorems - Stanford Encyclopedia",
          "url": "https://plato.stanford.edu/entries/goedel-incompleteness/",
          "type": "encyclopedia"
        },
        {
          "title": "Kurt Gödel - Britannica",
          "url": "https://www.britannica.com/biography/Kurt-Godel",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "logic",
        "mathematics",
        "foundational",
        "limits",
        "philosophy"
      ],
      "era": "prerequisites"
    },
    {
      "id": "turing-machine-1936",
      "year": 1936,
      "month": null,
      "type": "paper",
      "title": "Turing Machine & Computability",
      "what": "Alan Turing invented a theoretical machine (tape + head + rules) that can compute anything computable. Proved some problems are undecidable (Halting Problem). Defined what 'computation' means.",
      "why_it_matters": "Defines the limits of what ANY computer can do. Every programming language is equivalent to a Turing machine. The Halting Problem shows some things are mathematically impossible to compute.",
      "impact": "Theoretical foundation of computer science. Led directly to building real computers. Influenced Church, von Neumann, and all of computing.",
      "authors": [
        {
          "name": "Alan Turing",
          "born": 1912,
          "died": 1954,
          "nationality": "British",
          "role": "Mathematician & Computer Scientist",
          "affiliation": "Cambridge University, later University of Manchester",
          "background": "Father of computer science. WWII codebreaker (Enigma machine). Persecuted for homosexuality, died by suicide or accident. Pardoned posthumously in 2013.",
          "contribution": "Invented Turing Machine, proved Halting Problem, laid foundation for AI"
        }
      ],
      "work": {
        "type": "paper",
        "title": "On Computable Numbers, with an Application to the Entscheidungsproblem",
        "year": 1936,
        "published_in": "Proceedings of the London Mathematical Society",
        "url": "https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf"
      },
      "learn": [
        {
          "title": "Turing Machines Explained - Computerphile",
          "url": "https://www.youtube.com/watch?v=dNRDvLACg5Q",
          "type": "video",
          "duration": "10 min",
          "difficulty": "beginner"
        },
        {
          "title": "Turing Machine Interactive Simulator",
          "url": "https://turingmachine.io/",
          "type": "interactive",
          "difficulty": "beginner"
        },
        {
          "title": "Halting Problem - Computerphile",
          "url": "https://www.youtube.com/watch?v=macM_MtS_w4",
          "type": "video",
          "duration": "8 min",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (PDF)",
          "url": "https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf",
          "type": "paper"
        },
        {
          "title": "Turing Machine - Stanford Encyclopedia",
          "url": "https://plato.stanford.edu/entries/turing-machine/",
          "type": "encyclopedia"
        },
        {
          "title": "Alan Turing - Britannica",
          "url": "https://www.britannica.com/biography/Alan-Turing",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "computation",
        "mathematics",
        "foundational",
        "theory"
      ],
      "era": "prerequisites"
    },
    {
      "id": "mcculloch-pitts-neuron-1943",
      "year": 1943,
      "month": null,
      "type": "paper",
      "title": "McCulloch-Pitts Artificial Neuron",
      "what": "First mathematical model of a biological neuron. A neuron takes binary inputs (0 or 1), applies weights, sums them, and fires (outputs 1) if sum exceeds threshold. Showed neurons work like logic gates.",
      "why_it_matters": "THE foundation of neural networks. The core idea (weighted inputs → activation → output) is EXACTLY what modern deep learning uses. Every neuron in ChatGPT follows this pattern, just with continuous values instead of binary.",
      "impact": "Inspired Rosenblatt's Perceptron. Led to all neural network research. Modern deep learning is a direct descendant.",
      "authors": [
        {
          "name": "Warren McCulloch",
          "born": 1898,
          "died": 1969,
          "nationality": "American",
          "role": "Neurophysiologist",
          "affiliation": "University of Illinois, later MIT",
          "background": "Studied how brains represent ideas. Combined neuroscience with mathematical logic.",
          "contribution": "Provided biological insight on how neurons compute"
        },
        {
          "name": "Walter Pitts",
          "born": 1923,
          "died": 1969,
          "nationality": "American",
          "role": "Logician",
          "affiliation": "University of Chicago, MIT",
          "background": "Self-taught genius. Ran away from home at 12, lived in library. Joined McCulloch at age 18. Tragic life, died in obscurity.",
          "contribution": "Provided mathematical formalization of neuron model"
        }
      ],
      "work": {
        "type": "paper",
        "title": "A Logical Calculus of the Ideas Immanent in Nervous Activity",
        "year": 1943,
        "published_in": "Bulletin of Mathematical Biophysics",
        "url": "https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf"
      },
      "learn": [
        {
          "title": "But what is a neural network? - 3Blue1Brown",
          "url": "https://www.youtube.com/watch?v=aircAruvnKk",
          "type": "video",
          "duration": "19 min",
          "difficulty": "beginner"
        },
        {
          "title": "McCulloch-Pitts Neuron Explained",
          "url": "https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1",
          "type": "article",
          "difficulty": "beginner"
        },
        {
          "title": "Neural Networks from Scratch (book)",
          "url": "https://nnfs.io/",
          "type": "book",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (PDF)",
          "url": "https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf",
          "type": "paper"
        },
        {
          "title": "McCulloch-Pitts Neuron - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Artificial_neuron#McCulloch-Pitts_cell",
          "type": "article"
        }
      ],
      "tags": [
        "neural-networks",
        "foundational",
        "biology",
        "theory"
      ],
      "era": "foundations"
    },
    {
      "id": "von-neumann-architecture-1945",
      "year": 1945,
      "month": 6,
      "type": "paper",
      "title": "Von Neumann Computer Architecture",
      "what": "Design for modern computers: CPU (processor), Memory (RAM), Storage, and Input/Output, all connected by buses. Key insight: store PROGRAMS in memory alongside data (stored-program concept).",
      "why_it_matters": "Every computer running AI uses this architecture. Understanding the von Neumann bottleneck (CPU-memory bandwidth) explains why GPUs are needed for AI - they have different architecture optimized for parallel math.",
      "impact": "Blueprint for all modern computers. EDVAC, ENIAC successors, and every computer since follows this design.",
      "authors": [
        {
          "name": "John von Neumann",
          "born": 1903,
          "died": 1957,
          "nationality": "Hungarian-American",
          "role": "Mathematician & Polymath",
          "affiliation": "Institute for Advanced Study, Princeton",
          "background": "One of greatest mathematicians of 20th century. Contributed to quantum mechanics, game theory, nuclear weapons (Manhattan Project), and computing. Could memorize entire books.",
          "contribution": "Designed stored-program computer architecture, consulted on ENIAC/EDVAC"
        }
      ],
      "work": {
        "type": "paper",
        "title": "First Draft of a Report on the EDVAC",
        "year": 1945,
        "url": "https://web.archive.org/web/20130314123032/http://qss.stanford.edu/~godfrey/vonNeumann/vnedvac.pdf"
      },
      "learn": [
        {
          "title": "Von Neumann Architecture - Computerphile",
          "url": "https://www.youtube.com/watch?v=Ml3-kVYLNr8",
          "type": "video",
          "duration": "8 min",
          "difficulty": "beginner"
        },
        {
          "title": "How Computers Work - Crash Course",
          "url": "https://www.youtube.com/watch?v=tpIctyqH29Q",
          "type": "video",
          "duration": "10 min",
          "difficulty": "beginner"
        },
        {
          "title": "CPU vs GPU for Machine Learning",
          "url": "https://www.youtube.com/watch?v=6stDhEA0wFQ",
          "type": "video",
          "duration": "9 min",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "First Draft Report (PDF)",
          "url": "https://web.archive.org/web/20130314123032/http://qss.stanford.edu/~godfrey/vonNeumann/vnedvac.pdf",
          "type": "paper"
        },
        {
          "title": "Von Neumann Architecture - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Von_Neumann_architecture",
          "type": "article"
        },
        {
          "title": "John von Neumann - Britannica",
          "url": "https://www.britannica.com/biography/John-von-Neumann",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "hardware",
        "foundational",
        "computing",
        "architecture"
      ],
      "era": "foundations"
    },
    {
      "id": "shannon-information-theory-1948",
      "year": 1948,
      "month": 7,
      "type": "paper",
      "title": "Information Theory",
      "what": "Claude Shannon created the mathematical theory of information. Defined the 'bit' as basic unit. Entropy measures uncertainty. Showed fundamental limits of compression and communication.",
      "why_it_matters": "Cross-entropy loss function (used to train ALL neural networks) comes directly from Shannon. Language models work by predicting next token = compression = understanding. 'Compression is intelligence.'",
      "impact": "Foundation of digital communication, compression (MP3, JPEG, ZIP), and machine learning loss functions.",
      "key_concepts": [
        {
          "name": "Bit",
          "definition": "Binary digit. 0 or 1. The fundamental unit of information."
        },
        {
          "name": "Entropy",
          "definition": "Measure of uncertainty/randomness. Higher entropy = harder to predict."
        },
        {
          "name": "Information",
          "definition": "Measured by surprise. Unlikely events carry more information than likely ones."
        },
        {
          "name": "Cross-Entropy",
          "definition": "Measures how well a predicted distribution matches true distribution. THE loss function for LLMs."
        }
      ],
      "authors": [
        {
          "name": "Claude Shannon",
          "born": 1916,
          "died": 2001,
          "nationality": "American",
          "role": "Mathematician & Electrical Engineer",
          "affiliation": "Bell Labs, MIT",
          "background": "Father of information theory. Also juggled, built robots, rode unicycle through Bell Labs. His 1937 master's thesis showed boolean algebra could design circuits.",
          "contribution": "Created information theory, defined bits, entropy, channel capacity"
        }
      ],
      "work": {
        "type": "paper",
        "title": "A Mathematical Theory of Communication",
        "year": 1948,
        "published_in": "Bell System Technical Journal",
        "url": "https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf"
      },
      "learn": [
        {
          "title": "Information Theory - Khan Academy (full course)",
          "url": "https://www.khanacademy.org/computing/computer-science/informationtheory",
          "type": "course",
          "difficulty": "beginner"
        },
        {
          "title": "Claude Shannon - Father of Information Theory",
          "url": "https://www.youtube.com/watch?v=z2Whj_nL-x8",
          "type": "video",
          "duration": "11 min",
          "difficulty": "beginner"
        },
        {
          "title": "Visual Information Theory (excellent visual guide)",
          "url": "https://colah.github.io/posts/2015-09-Visual-Information/",
          "type": "article",
          "difficulty": "intermediate"
        },
        {
          "title": "Cross-Entropy Loss Explained",
          "url": "https://www.youtube.com/watch?v=6ArSys5qHAU",
          "type": "video",
          "duration": "8 min",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (PDF)",
          "url": "https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf",
          "type": "paper"
        },
        {
          "title": "Claude Shannon - Britannica",
          "url": "https://www.britannica.com/biography/Claude-Shannon",
          "type": "encyclopedia"
        },
        {
          "title": "Information Theory - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Information_theory",
          "type": "article"
        }
      ],
      "tags": [
        "information-theory",
        "foundational",
        "mathematics",
        "loss-functions"
      ],
      "era": "foundations"
    },
    {
      "id": "hebbian-learning-1949",
      "year": 1949,
      "month": null,
      "type": "book",
      "title": "Hebbian Learning Rule",
      "what": "Donald Hebb proposed: 'Neurons that fire together, wire together.' If two neurons activate simultaneously and repeatedly, the connection between them strengthens. This is how brains learn.",
      "why_it_matters": "First theory of how learning happens in the brain. Directly inspired neural network training algorithms. Backpropagation is a sophisticated form of this idea - adjusting weights based on what works.",
      "impact": "Foundation of connectionism (learning through connection weights). Influenced all neural network learning rules.",
      "authors": [
        {
          "name": "Donald Hebb",
          "born": 1904,
          "died": 1985,
          "nationality": "Canadian",
          "role": "Psychologist",
          "affiliation": "McGill University, Canada",
          "background": "Studied how the brain organizes behavior. His book was influential across psychology, neuroscience, and AI.",
          "contribution": "Proposed biological learning rule that neurons strengthen connections when co-activated"
        }
      ],
      "work": {
        "type": "book",
        "title": "The Organization of Behavior: A Neuropsychological Theory",
        "year": 1949,
        "url": "https://pure.mpg.de/rest/items/item_2346268_3/component/file_2346267/content"
      },
      "learn": [
        {
          "title": "Hebbian Learning Explained",
          "url": "https://www.youtube.com/watch?v=v3_NxU0gVeY",
          "type": "video",
          "duration": "6 min",
          "difficulty": "beginner"
        },
        {
          "title": "How Neural Networks Learn - 3Blue1Brown",
          "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w",
          "type": "video",
          "duration": "21 min",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Hebbian Theory - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Hebbian_theory",
          "type": "article"
        },
        {
          "title": "Donald Hebb - Famous Psychologists",
          "url": "https://www.britannica.com/biography/Donald-Olding-Hebb",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "neural-networks",
        "learning",
        "biology",
        "foundational"
      ],
      "era": "foundations"
    },
    {
      "id": "turing-test-1950",
      "year": 1950,
      "month": 10,
      "type": "paper",
      "title": "Turing Test - 'Can Machines Think?'",
      "what": "Alan Turing proposed the 'Imitation Game': if a human judge cannot tell whether they're conversing (via text) with a human or machine, the machine is intelligent. Also addressed objections to machine intelligence.",
      "why_it_matters": "THE philosophical foundation for AI. Shifted the question from 'Can machines think?' to 'Can machines behave intelligently?' When ChatGPT passes as human, is it intelligent? Still debated today.",
      "impact": "Defined the goal for AI for decades. Inspired ELIZA, chatbots, and eventually conversational AI. The Loebner Prize (annual Turing Test competition) ran 1990-2019.",
      "authors": [
        {
          "name": "Alan Turing",
          "born": 1912,
          "died": 1954,
          "nationality": "British",
          "role": "Mathematician & Computer Scientist",
          "affiliation": "University of Manchester",
          "background": "Same Turing who invented Turing Machine. This paper, 'Computing Machinery and Intelligence', is one of most cited AI papers ever.",
          "contribution": "Proposed operational test for machine intelligence"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Computing Machinery and Intelligence",
        "year": 1950,
        "published_in": "Mind (journal)",
        "url": "https://academic.oup.com/mind/article/LIX/236/433/986238"
      },
      "learn": [
        {
          "title": "The Turing Test - Computerphile",
          "url": "https://www.youtube.com/watch?v=3wLqsRLvV-c",
          "type": "video",
          "duration": "12 min",
          "difficulty": "beginner"
        },
        {
          "title": "Chinese Room Argument (counter-argument)",
          "url": "https://www.youtube.com/watch?v=D0MD4sRHj1M",
          "type": "video",
          "duration": "10 min",
          "difficulty": "intermediate"
        },
        {
          "title": "Original Paper (readable)",
          "url": "https://redirect.cs.umbc.edu/courses/471/papers/turing.pdf",
          "type": "paper",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper - Mind Journal (paywalled)",
          "url": "https://academic.oup.com/mind/article/LIX/236/433/986238",
          "type": "paper"
        },
        {
          "title": "Turing Test - Stanford Encyclopedia of Philosophy",
          "url": "https://plato.stanford.edu/entries/turing-test/",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "philosophy",
        "foundational",
        "testing",
        "intelligence"
      ],
      "era": "foundations"
    },
    {
      "id": "dartmouth-conference-1956",
      "year": 1956,
      "month": 8,
      "type": "event",
      "title": "Dartmouth Conference - AI is Born",
      "what": "A 2-month summer workshop at Dartmouth College. John McCarthy coined the term 'Artificial Intelligence.' Organizers proposed that 'every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.'",
      "why_it_matters": "THE founding event of AI as a field. Named the field, gathered the founders, set the research agenda. Every AI researcher traces their intellectual lineage here.",
      "impact": "Created AI as a formal discipline. Attendees became the field's leaders for 20+ years. Established symbolic AI as dominant paradigm.",
      "authors": [
        {
          "name": "John McCarthy",
          "born": 1927,
          "died": 2011,
          "nationality": "American",
          "role": "Computer Scientist",
          "affiliation": "Dartmouth, MIT, Stanford",
          "background": "Coined 'Artificial Intelligence'. Invented LISP. Created time-sharing. Turing Award winner (1971).",
          "contribution": "Organized conference, coined the term AI, wrote the proposal"
        },
        {
          "name": "Marvin Minsky",
          "born": 1927,
          "died": 2016,
          "nationality": "American",
          "role": "Cognitive Scientist",
          "affiliation": "MIT",
          "background": "Co-founder of MIT AI Lab. Pioneer of neural networks AND symbolic AI. Turing Award winner (1969).",
          "contribution": "Co-organizer, later led MIT AI Lab"
        },
        {
          "name": "Claude Shannon",
          "born": 1916,
          "died": 2001,
          "nationality": "American",
          "role": "Mathematician",
          "affiliation": "Bell Labs, MIT",
          "background": "Father of information theory. Attended as established figure.",
          "contribution": "Co-organizer, provided theoretical credibility"
        },
        {
          "name": "Nathaniel Rochester",
          "born": 1919,
          "died": 2001,
          "nationality": "American",
          "role": "Computer Scientist",
          "affiliation": "IBM",
          "background": "Designed IBM 701 computer. Brought industry perspective.",
          "contribution": "Co-organizer, IBM representative"
        }
      ],
      "work": {
        "type": "proposal",
        "title": "A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence",
        "year": 1955,
        "url": "http://jmc.stanford.edu/articles/dartmouth/dartmouth.pdf"
      },
      "learn": [
        {
          "title": "The Birth of AI - Dartmouth 1956",
          "url": "https://www.youtube.com/watch?v=P18EdAKuC1U",
          "type": "video",
          "duration": "10 min",
          "difficulty": "beginner"
        },
        {
          "title": "Original Proposal (readable)",
          "url": "http://jmc.stanford.edu/articles/dartmouth/dartmouth.pdf",
          "type": "paper",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Dartmouth Workshop - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Dartmouth_workshop",
          "type": "article"
        },
        {
          "title": "John McCarthy's Stanford Page",
          "url": "http://jmc.stanford.edu/",
          "type": "website"
        }
      ],
      "tags": [
        "milestone",
        "foundational",
        "symbolic-ai",
        "history"
      ],
      "era": "early-ai"
    },
    {
      "id": "perceptron-1957",
      "year": 1957,
      "month": null,
      "type": "invention",
      "title": "The Perceptron",
      "what": "Frank Rosenblatt built the first machine that could LEARN. The Perceptron takes inputs, multiplies by weights, sums them, and outputs 1 or 0. Crucially: it adjusts weights based on errors (the learning rule).",
      "why_it_matters": "First practical learning algorithm! The perceptron learning rule is the ancestor of backpropagation. Modern neural networks are just many perceptrons stacked together with different activation functions.",
      "impact": "Sparked massive excitement. Navy funded it. Media called it a 'thinking machine.' Later criticism nearly killed neural network research.",
      "authors": [
        {
          "name": "Frank Rosenblatt",
          "born": 1928,
          "died": 1971,
          "nationality": "American",
          "role": "Psychologist & Computer Scientist",
          "affiliation": "Cornell Aeronautical Laboratory",
          "background": "Built the Mark I Perceptron hardware. Died in boating accident at 43, just before neural networks revived.",
          "contribution": "Invented perceptron, first learning algorithm, built hardware implementation"
        }
      ],
      "work": {
        "type": "paper",
        "title": "The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain",
        "year": 1958,
        "published_in": "Psychological Review",
        "url": "https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=65a69d3c97a0a40ab4a0e28c2eb86e220e0c5f4e"
      },
      "learn": [
        {
          "title": "Perceptrons - Computerphile",
          "url": "https://www.youtube.com/watch?v=GVsUOuSjvcg",
          "type": "video",
          "duration": "12 min",
          "difficulty": "beginner"
        },
        {
          "title": "The Perceptron Algorithm - StatQuest",
          "url": "https://www.youtube.com/watch?v=4Gac5I64LM4",
          "type": "video",
          "duration": "10 min",
          "difficulty": "beginner"
        },
        {
          "title": "Build a Perceptron in Python",
          "url": "https://towardsdatascience.com/perceptron-algorithm-in-python-f3ac89d2e537",
          "type": "tutorial",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Perceptron - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Perceptron",
          "type": "article"
        },
        {
          "title": "Frank Rosenblatt - IEEE History",
          "url": "https://ethw.org/Frank_Rosenblatt",
          "type": "biography"
        }
      ],
      "tags": [
        "neural-networks",
        "learning",
        "milestone",
        "hardware"
      ],
      "era": "early-ai"
    },
    {
      "id": "lisp-1958",
      "year": 1958,
      "month": null,
      "type": "language",
      "title": "LISP Programming Language",
      "what": "John McCarthy created LISP (LISt Processing) - second oldest high-level language still in use. Features: code as data (homoiconicity), garbage collection, recursion, dynamic typing. Everything is a list: (function arg1 arg2).",
      "why_it_matters": "THE language of AI for 30 years. Symbolic AI, expert systems, and early ML were all written in LISP. Many ideas (garbage collection, REPL, closures) came to all languages from LISP.",
      "impact": "Dominated AI research 1960s-1980s. Emacs, AutoCAD, early expert systems. Influenced Scheme, Clojure, and functional programming.",
      "authors": [
        {
          "name": "John McCarthy",
          "born": 1927,
          "died": 2011,
          "nationality": "American",
          "role": "Computer Scientist",
          "affiliation": "MIT, Stanford",
          "background": "Same McCarthy who coined 'AI'. Also invented time-sharing and contributed to ALGOL.",
          "contribution": "Designed LISP language, implemented first version"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Recursive Functions of Symbolic Expressions and Their Computation by Machine",
        "year": 1960,
        "published_in": "Communications of the ACM",
        "url": "http://jmc.stanford.edu/articles/recursive/recursive.pdf"
      },
      "learn": [
        {
          "title": "LISP in 100 Seconds",
          "url": "https://www.youtube.com/watch?v=INUHCQST7CU",
          "type": "video",
          "duration": "2 min",
          "difficulty": "beginner"
        },
        {
          "title": "Structure and Interpretation of Computer Programs (free book)",
          "url": "https://mitp-content-server.mit.edu/books/content/sectbyfn/books_pres_0/6515/sicp.zip/index.html",
          "type": "book",
          "difficulty": "intermediate"
        },
        {
          "title": "Practical Common Lisp (free book)",
          "url": "https://gigamonkeys.com/book/",
          "type": "book",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "LISP - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Lisp_(programming_language)",
          "type": "article"
        },
        {
          "title": "History of LISP - McCarthy",
          "url": "http://jmc.stanford.edu/articles/lisp/lisp.pdf",
          "type": "paper"
        }
      ],
      "tags": [
        "programming",
        "symbolic-ai",
        "language",
        "foundational"
      ],
      "era": "early-ai"
    },
    {
      "id": "gps-1959",
      "year": 1959,
      "month": null,
      "type": "program",
      "title": "General Problem Solver (GPS)",
      "what": "Newell and Simon created a program that could solve problems by means-ends analysis: identify goal, identify current state, find operators that reduce the difference. First attempt at general-purpose AI.",
      "why_it_matters": "Introduced the idea that intelligence = search through problem space. This 'state space search' underlies game-playing AI, planning, and even modern reasoning in LLMs (chain-of-thought).",
      "impact": "Established 'physical symbol system hypothesis' - intelligence emerges from symbol manipulation. Dominated AI thinking until connectionism rose.",
      "authors": [
        {
          "name": "Allen Newell",
          "born": 1927,
          "died": 1992,
          "nationality": "American",
          "role": "Computer Scientist & Cognitive Psychologist",
          "affiliation": "Carnegie Mellon",
          "background": "Pioneer of AI and cognitive science. Co-created Logic Theorist, GPS, SOAR. Turing Award (1975).",
          "contribution": "Co-designed GPS, developed means-ends analysis"
        },
        {
          "name": "Herbert Simon",
          "born": 1916,
          "died": 2001,
          "nationality": "American",
          "role": "Polymath",
          "affiliation": "Carnegie Mellon",
          "background": "Rare winner of both Turing Award (1975) AND Nobel Prize in Economics (1978). Studied decision-making.",
          "contribution": "Co-designed GPS, theories of bounded rationality"
        }
      ],
      "work": {
        "type": "paper",
        "title": "GPS, A Program that Simulates Human Thought",
        "year": 1961,
        "published_in": "Computers and Thought",
        "url": "https://stacks.stanford.edu/file/druid:mn750fy2768/mn750fy2768.pdf"
      },
      "learn": [
        {
          "title": "Means-Ends Analysis Explained",
          "url": "https://www.youtube.com/watch?v=d8zF4Q_BNqo",
          "type": "video",
          "duration": "8 min",
          "difficulty": "beginner"
        },
        {
          "title": "Problem Solving as Search - MIT OCW",
          "url": "https://ocw.mit.edu/courses/6-034-artificial-intelligence-fall-2010/video_galleries/lecture-videos/",
          "type": "course",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "General Problem Solver - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/General_Problem_Solver",
          "type": "article"
        },
        {
          "title": "Herbert Simon - Nobel Prize",
          "url": "https://www.nobelprize.org/prizes/economic-sciences/1978/simon/biographical/",
          "type": "biography"
        }
      ],
      "tags": [
        "symbolic-ai",
        "search",
        "problem-solving",
        "cognitive"
      ],
      "era": "early-ai"
    },
    {
      "id": "eliza-1966",
      "year": 1966,
      "month": null,
      "type": "program",
      "title": "ELIZA - First Chatbot",
      "what": "Joseph Weizenbaum created ELIZA, a program that simulated a Rogerian psychotherapist. Used simple pattern matching: if input contains 'mother', respond with 'Tell me more about your family.' No understanding, just tricks.",
      "why_it_matters": "First demonstration that people will attribute intelligence to machines that seem to understand them (ELIZA effect). Users KNEW it was a program but still felt it understood them. Foreshadowed ChatGPT's impact.",
      "impact": "Started the chatbot industry. Weizenbaum was horrified that people treated ELIZA seriously - wrote 'Computer Power and Human Reason' warning about AI.",
      "authors": [
        {
          "name": "Joseph Weizenbaum",
          "born": 1923,
          "died": 2008,
          "nationality": "German-American",
          "role": "Computer Scientist",
          "affiliation": "MIT",
          "background": "Created ELIZA but became AI's harshest critic. Warned about computers replacing human judgment.",
          "contribution": "Created ELIZA, later became prominent AI critic"
        }
      ],
      "work": {
        "type": "paper",
        "title": "ELIZA - A Computer Program For the Study of Natural Language Communication",
        "year": 1966,
        "published_in": "Communications of the ACM",
        "url": "https://dl.acm.org/doi/pdf/10.1145/365153.365168"
      },
      "learn": [
        {
          "title": "ELIZA - The First Chatbot (with demo)",
          "url": "https://www.youtube.com/watch?v=RMK9AphfLco",
          "type": "video",
          "duration": "6 min",
          "difficulty": "beginner"
        },
        {
          "title": "Try ELIZA Online",
          "url": "https://web.njit.edu/~ronkowit/eliza.html",
          "type": "interactive",
          "difficulty": "beginner"
        },
        {
          "title": "Build ELIZA in Python",
          "url": "https://www.nltk.org/howto/chat.html",
          "type": "tutorial",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "ELIZA - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/ELIZA",
          "type": "article"
        },
        {
          "title": "Original Paper (ACM)",
          "url": "https://dl.acm.org/doi/10.1145/365153.365168",
          "type": "paper"
        }
      ],
      "tags": [
        "nlp",
        "chatbot",
        "milestone",
        "psychology"
      ],
      "era": "early-ai"
    },
    {
      "id": "perceptrons-book-1969",
      "year": 1969,
      "month": null,
      "type": "book",
      "title": "Perceptrons Book - The Critique",
      "what": "Minsky and Papert proved that single-layer perceptrons cannot learn XOR (exclusive or) or other non-linearly separable problems. They implied multi-layer networks were also hopeless (they were wrong about this).",
      "why_it_matters": "Nearly killed neural network research for 15 years. Funding dried up. Researchers switched to symbolic AI. The lesson: one influential paper can derail an entire field.",
      "impact": "Started the first AI Winter for neural networks. Perceptron research stopped. Backpropagation (the solution) wouldn't be popularized until 1986.",
      "authors": [
        {
          "name": "Marvin Minsky",
          "born": 1927,
          "died": 2016,
          "nationality": "American",
          "role": "Computer Scientist",
          "affiliation": "MIT",
          "background": "Ironically, had built first neural network hardware in 1951! Later became neural network's harshest critic.",
          "contribution": "Provided mathematical proofs of perceptron limitations"
        },
        {
          "name": "Seymour Papert",
          "born": 1928,
          "died": 2016,
          "nationality": "South African-American",
          "role": "Mathematician & Educator",
          "affiliation": "MIT",
          "background": "Pioneered educational computing. Created LOGO programming language for children.",
          "contribution": "Co-authored proofs, wrote accessible explanations"
        }
      ],
      "work": {
        "type": "book",
        "title": "Perceptrons: An Introduction to Computational Geometry",
        "year": 1969,
        "url": "https://mitpress.mit.edu/9780262630221/perceptrons/"
      },
      "learn": [
        {
          "title": "XOR Problem Explained",
          "url": "https://www.youtube.com/watch?v=kNPGXgzxoHw",
          "type": "video",
          "duration": "7 min",
          "difficulty": "beginner"
        },
        {
          "title": "Why Single Perceptrons Fail at XOR",
          "url": "https://towardsdatascience.com/the-xor-problem-in-neural-networks-50006411840b",
          "type": "article",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Perceptrons (book) - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Perceptrons_(book)",
          "type": "article"
        },
        {
          "title": "MIT Press - Perceptrons",
          "url": "https://mitpress.mit.edu/9780262630221/perceptrons/",
          "type": "book"
        }
      ],
      "tags": [
        "neural-networks",
        "criticism",
        "ai-winter",
        "history"
      ],
      "era": "early-ai"
    },
    {
      "id": "shakey-robot-1969",
      "year": 1969,
      "month": null,
      "type": "invention",
      "title": "Shakey the Robot",
      "what": "SRI International built Shakey, the first mobile robot that could reason about its actions. Used A* search, computer vision, and planning. Could navigate rooms and push boxes.",
      "why_it_matters": "First robot to combine locomotion, perception, and problem solving. Pioneered many AI techniques still used today.",
      "impact": "Proved AI could work in physical world. Led to robotics and autonomous systems research.",
      "authors": [
        {
          "name": "Charles Rosen",
          "nationality": "American",
          "role": "Project Lead",
          "affiliation": "SRI International",
          "contribution": "Led Shakey project"
        }
      ],
      "work": {
        "type": "project",
        "title": "Shakey the Robot",
        "year": 1969,
        "url": "https://www.sri.com/hoi/shakey-the-robot/"
      },
      "learn": [
        {
          "title": "Shakey Documentary",
          "url": "https://www.youtube.com/watch?v=qXdn6ynwpiI",
          "type": "video",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Shakey - Computer History Museum",
          "url": "https://www.computerhistory.org/revolution/artificial-intelligence-robotics/13/289",
          "type": "museum"
        }
      ],
      "tags": [
        "robotics",
        "planning",
        "computer-vision",
        "early-ai"
      ],
      "era": "early-ai"
    },
    {
      "id": "prolog-1972",
      "year": 1972,
      "month": null,
      "type": "language",
      "title": "Prolog Programming Language",
      "what": "Alain Colmerauer and Robert Kowalski created Prolog (PROgramming in LOGic), a logic programming language based on predicate calculus. Programs are written as logical rules.",
      "why_it_matters": "Prolog became the language of choice for expert systems and symbolic AI. Used extensively in NLP, theorem proving, and knowledge systems.",
      "impact": "Foundation for Japan's 5th Generation Computer Project. Still used in AI research.",
      "authors": [
        {
          "name": "Alain Colmerauer",
          "nationality": "French",
          "role": "Creator",
          "affiliation": "University of Marseille",
          "contribution": "Designed and implemented Prolog"
        }
      ],
      "work": {
        "type": "language",
        "title": "Prolog",
        "year": 1972
      },
      "learn": [
        {
          "title": "Learn Prolog Now!",
          "url": "https://www.learnprolognow.org/",
          "type": "course",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Prolog - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Prolog",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "programming-language",
        "logic",
        "symbolic-ai",
        "expert-systems"
      ],
      "era": "early-ai"
    },
    {
      "id": "lighthill-report-1973",
      "year": 1973,
      "month": null,
      "type": "report",
      "title": "Lighthill Report - AI Funding Collapse",
      "what": "British mathematician James Lighthill wrote a devastating government report saying AI had failed to achieve its goals. Called AI research 'grandiose' and unable to solve real problems.",
      "why_it_matters": "Official government criticism of AI. Led to massive funding cuts in UK. Example of how AI hype cycles lead to backlash when promises aren't met. Same pattern repeated with expert systems.",
      "impact": "Killed AI research funding in UK for a decade. US also cut funding. First major 'AI Winter' began.",
      "authors": [
        {
          "name": "James Lighthill",
          "born": 1924,
          "died": 1998,
          "nationality": "British",
          "role": "Mathematician",
          "affiliation": "Cambridge University",
          "background": "Expert in fluid mechanics, not AI. Commissioned by UK Science Research Council to evaluate AI.",
          "contribution": "Wrote critical evaluation that ended UK AI funding"
        }
      ],
      "work": {
        "type": "report",
        "title": "Artificial Intelligence: A General Survey",
        "year": 1973,
        "url": "http://www.chilton-computing.org.uk/inf/literature/reports/lighthill_report/p001.htm"
      },
      "learn": [
        {
          "title": "AI Winters Explained",
          "url": "https://www.youtube.com/watch?v=4kDXKnZPkrU",
          "type": "video",
          "duration": "12 min",
          "difficulty": "beginner"
        },
        {
          "title": "History of AI - Cycles of Boom and Bust",
          "url": "https://ourworldindata.org/brief-history-of-ai",
          "type": "article",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Lighthill Report - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Lighthill_report",
          "type": "article"
        },
        {
          "title": "Original Report (archived)",
          "url": "http://www.chilton-computing.org.uk/inf/literature/reports/lighthill_report/p001.htm",
          "type": "report"
        }
      ],
      "tags": [
        "ai-winter",
        "funding",
        "criticism",
        "history"
      ],
      "era": "early-ai"
    },
    {
      "id": "expert-systems-mycin-1976",
      "year": 1976,
      "month": null,
      "type": "system",
      "title": "MYCIN Expert System",
      "what": "Stanford developed MYCIN, an expert system for diagnosing bacterial infections. Used 600 IF-THEN rules from human experts. Achieved 69% accuracy - better than most doctors!",
      "why_it_matters": "Showed 'knowledge is power' - expert systems could outperform humans in narrow domains. Sparked the Expert Systems boom (1980s). But also showed limitations: hard to maintain, brittle, no learning.",
      "impact": "Launched the expert systems industry worth billions in 1980s. Companies like Symbolics, Intellicorp. Eventually crashed in second AI Winter.",
      "authors": [
        {
          "name": "Edward Shortliffe",
          "born": 1947,
          "died": null,
          "nationality": "American",
          "role": "Physician & Computer Scientist",
          "affiliation": "Stanford",
          "background": "MD and PhD. Pioneer of medical AI. Later led Columbia biomedical informatics.",
          "contribution": "Created MYCIN, developed certainty factors for handling uncertainty"
        }
      ],
      "work": {
        "type": "thesis",
        "title": "MYCIN: Computer-Based Medical Consultations",
        "year": 1976,
        "url": "https://www.shortliffe.net/Shortliffe-1976/MYCIN%20thesis.htm"
      },
      "learn": [
        {
          "title": "Expert Systems Explained",
          "url": "https://www.youtube.com/watch?v=cP1fsCCaMq4",
          "type": "video",
          "duration": "8 min",
          "difficulty": "beginner"
        },
        {
          "title": "Rule-Based Systems - IBM",
          "url": "https://www.ibm.com/topics/rule-based-systems",
          "type": "article",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "MYCIN - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/MYCIN",
          "type": "article"
        },
        {
          "title": "Expert Systems History",
          "url": "https://www.britannica.com/technology/expert-system",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "expert-systems",
        "medical",
        "symbolic-ai",
        "rules"
      ],
      "era": "first-winter"
    },
    {
      "id": "japan-5th-gen-1982",
      "year": 1982,
      "month": null,
      "type": "project",
      "title": "Japan's Fifth Generation Computer Project",
      "what": "Japan's MITI launched ambitious $400M project to build intelligent computers using Prolog and parallel processing. Aimed to leapfrog US in computing.",
      "why_it_matters": "Sparked global AI race. US and UK launched competing projects. Its failure in 1992 contributed to the second AI winter.",
      "impact": "Advanced parallel computing but failed to deliver promised AI. Contributed to disillusionment with symbolic AI.",
      "authors": [
        {
          "name": "MITI",
          "role": "Organization",
          "affiliation": "Japanese Government",
          "contribution": "Funded and coordinated 5th Generation Project"
        }
      ],
      "work": {
        "type": "project",
        "title": "Fifth Generation Computer Systems",
        "year": 1982
      },
      "learn": [
        {
          "title": "The Rise and Fall of the Fifth Generation",
          "url": "https://www.technologyreview.com/",
          "type": "article",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Fifth Generation Computer - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Fifth_Generation_Computer_Systems",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "japan",
        "symbolic-ai",
        "parallel-computing",
        "project"
      ],
      "era": "expert-systems"
    },
    {
      "id": "backpropagation-1986",
      "year": 1986,
      "month": null,
      "type": "paper",
      "title": "Backpropagation Popularized",
      "what": "Rumelhart, Hinton, and Williams showed how to train multi-layer neural networks using backpropagation. The algorithm: run forward, compute error, propagate error backward through layers, adjust all weights. Solved the XOR problem!",
      "why_it_matters": "THE algorithm that makes deep learning possible. Every modern neural network (GPT, BERT, etc.) trains using backpropagation + gradient descent. Without this, no ChatGPT.",
      "impact": "Revived neural network research. Started connectionism movement. Led eventually to deep learning revolution 25 years later.",
      "authors": [
        {
          "name": "David Rumelhart",
          "born": 1942,
          "died": 2011,
          "nationality": "American",
          "role": "Psychologist",
          "affiliation": "UC San Diego, Stanford",
          "background": "Cognitive psychologist interested in how brain represents information.",
          "contribution": "Led the team, co-wrote influential PDP books"
        },
        {
          "name": "Geoffrey Hinton",
          "born": 1947,
          "died": null,
          "nationality": "British-Canadian",
          "role": "Computer Scientist",
          "affiliation": "Toronto, Google",
          "background": "Godfather of Deep Learning. Turing Award 2018. Left Google 2023 to warn about AI risks.",
          "contribution": "Key contributor to backprop, later invented deep belief networks, dropout"
        },
        {
          "name": "Ronald Williams",
          "born": 1948,
          "died": null,
          "nationality": "American",
          "role": "Computer Scientist",
          "affiliation": "Northeastern University",
          "background": "Reinforcement learning pioneer. REINFORCE algorithm.",
          "contribution": "Derived mathematical framework for backpropagation"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Learning representations by back-propagating errors",
        "year": 1986,
        "published_in": "Nature",
        "url": "https://www.nature.com/articles/323533a0"
      },
      "learn": [
        {
          "title": "Backpropagation Explained - 3Blue1Brown",
          "url": "https://www.youtube.com/watch?v=Ilg3gGewQ5U",
          "type": "video",
          "duration": "14 min",
          "difficulty": "beginner"
        },
        {
          "title": "Backpropagation Calculus - 3Blue1Brown",
          "url": "https://www.youtube.com/watch?v=tIeHLnjs5U8",
          "type": "video",
          "duration": "10 min",
          "difficulty": "intermediate"
        },
        {
          "title": "Backprop from Scratch in Python",
          "url": "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/",
          "type": "tutorial",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (Nature)",
          "url": "https://www.nature.com/articles/323533a0",
          "type": "paper"
        },
        {
          "title": "Backpropagation - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Backpropagation",
          "type": "article"
        }
      ],
      "tags": [
        "neural-networks",
        "learning",
        "breakthrough",
        "foundational"
      ],
      "era": "expert-systems"
    },
    {
      "id": "ai-winter-collapse-1987",
      "year": 1987,
      "month": null,
      "type": "event",
      "title": "AI Winter - Expert Systems Collapse",
      "what": "The AI industry crashed. LISP machine companies folded. Expert systems failed to scale beyond narrow domains. Maintenance costs exploded. The 'AI winter' set in.",
      "why_it_matters": "Expert systems couldn't handle real-world complexity. Knowledge engineering was too expensive. Set AI research back by years.",
      "impact": "AI funding collapsed. Researchers avoided the term 'AI'. Statistical methods began to replace symbolic approaches.",
      "authors": [
        {
          "name": "Industry-wide",
          "role": "Event",
          "affiliation": "Multiple organizations",
          "contribution": "Collective failure of expert systems and LISP machine market"
        }
      ],
      "work": {
        "type": "event",
        "title": "Second AI Winter Begins",
        "year": 1987
      },
      "learn": [
        {
          "title": "AI Winter Documentary",
          "url": "https://www.youtube.com/watch?v=s0dMTAQM4cw",
          "type": "video",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "AI Winter - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/AI_winter",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "ai-winter",
        "expert-systems",
        "history"
      ],
      "era": "second-winter"
    },
    {
      "id": "cnn-lenet-1989",
      "year": 1989,
      "month": null,
      "type": "paper",
      "title": "Convolutional Neural Networks (LeNet)",
      "what": "Yann LeCun applied backpropagation to convolutional networks for reading handwritten digits. Key insight: use local connections (filters) that share weights, mimicking how eyes process images. Won't process entire image at once.",
      "why_it_matters": "CNNs dominate computer vision. Every image classifier, face detector, self-driving car uses CNNs. This architecture led to AlexNet (2012) which started the deep learning revolution.",
      "impact": "Used by US Post Office to read ZIP codes. Basis for all modern image AI. LeCun won Turing Award (2018).",
      "authors": [
        {
          "name": "Yann LeCun",
          "born": 1960,
          "died": null,
          "nationality": "French-American",
          "role": "Computer Scientist",
          "affiliation": "Bell Labs, NYU, Meta",
          "background": "One of the 'Godfathers of Deep Learning'. Chief AI Scientist at Meta. Turing Award 2018.",
          "contribution": "Invented CNNs, applied them to real-world problems"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "year": 1989,
        "published_in": "Neural Computation",
        "url": "http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf"
      },
      "learn": [
        {
          "title": "CNNs Explained - 3Blue1Brown",
          "url": "https://www.youtube.com/watch?v=KuXjwB4LzSA",
          "type": "video",
          "duration": "26 min",
          "difficulty": "beginner"
        },
        {
          "title": "CNN from Scratch - Andrej Karpathy",
          "url": "https://cs231n.github.io/",
          "type": "course",
          "difficulty": "intermediate"
        },
        {
          "title": "Interactive CNN Visualizer",
          "url": "https://poloclub.github.io/cnn-explainer/",
          "type": "interactive",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Original Paper (PDF)",
          "url": "http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf",
          "type": "paper"
        },
        {
          "title": "CNN - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
          "type": "article"
        }
      ],
      "tags": [
        "neural-networks",
        "vision",
        "breakthrough",
        "deep-learning"
      ],
      "era": "second-winter"
    },
    {
      "id": "svm-1995",
      "year": 1995,
      "month": null,
      "type": "paper",
      "title": "Support Vector Machines (SVM)",
      "what": "Cortes and Vapnik developed SVMs - a powerful algorithm for classification. Key idea: find the hyperplane that maximizes margin between classes. The 'kernel trick' allows linear methods to work on non-linear problems.",
      "why_it_matters": "SVMs were THE algorithm before deep learning. Won many competitions. Still useful when you have small data. Theoretical foundations influenced understanding of generalization.",
      "impact": "Dominated ML from 1995-2012. Used in text classification, image recognition, bioinformatics.",
      "authors": [
        {
          "name": "Corinna Cortes",
          "born": 1961,
          "died": null,
          "nationality": "Danish-American",
          "role": "Computer Scientist",
          "affiliation": "Bell Labs, Google Research",
          "background": "VP of Research at Google. Led SVM development.",
          "contribution": "Developed soft-margin SVM for noisy data"
        },
        {
          "name": "Vladimir Vapnik",
          "born": 1936,
          "died": null,
          "nationality": "Russian-American",
          "role": "Statistician",
          "affiliation": "Bell Labs, Columbia",
          "background": "Developed VC theory (Vapnik-Chervonenkis dimension). Theoretical giant of ML.",
          "contribution": "Created statistical learning theory foundation for SVMs"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Support-Vector Networks",
        "year": 1995,
        "published_in": "Machine Learning",
        "url": "https://link.springer.com/article/10.1007/BF00994018"
      },
      "learn": [
        {
          "title": "SVM Explained - StatQuest",
          "url": "https://www.youtube.com/watch?v=efR1C6CvhmE",
          "type": "video",
          "duration": "20 min",
          "difficulty": "beginner"
        },
        {
          "title": "SVM with Kernel Trick Visualization",
          "url": "https://www.youtube.com/watch?v=3liCbRZPrZA",
          "type": "video",
          "duration": "12 min",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "SVM - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Support-vector_machine",
          "type": "article"
        },
        {
          "title": "Original Paper",
          "url": "https://link.springer.com/article/10.1007/BF00994018",
          "type": "paper"
        }
      ],
      "tags": [
        "machine-learning",
        "classification",
        "kernel",
        "statistics"
      ],
      "era": "ml-rise"
    },
    {
      "id": "lstm-1997",
      "year": 1997,
      "month": null,
      "type": "paper",
      "title": "LSTM - Long Short-Term Memory",
      "what": "Hochreiter and Schmidhuber invented LSTM, a special RNN architecture with 'gates' that control information flow. Solves the vanishing gradient problem: can remember information over very long sequences.",
      "why_it_matters": "Before Transformers, LSTMs were THE architecture for sequences: text, speech, time series. Google Translate, Siri, and early language models all used LSTMs. Understanding LSTMs helps understand why Transformers are better.",
      "impact": "Dominated sequence modeling 1997-2017. Still used in edge cases where Transformers are too heavy.",
      "authors": [
        {
          "name": "Sepp Hochreiter",
          "born": 1967,
          "died": null,
          "nationality": "German",
          "role": "Computer Scientist",
          "affiliation": "TU Munich, later JKU Linz",
          "background": "Identified vanishing gradient problem in 1991 thesis. Invented LSTM as solution.",
          "contribution": "Designed LSTM architecture, identified vanishing gradient problem"
        },
        {
          "name": "Jürgen Schmidhuber",
          "born": 1963,
          "died": null,
          "nationality": "German",
          "role": "Computer Scientist",
          "affiliation": "IDSIA, Switzerland",
          "background": "Pioneer of deep learning. Claims credit for many inventions. Controversial but influential.",
          "contribution": "Co-invented LSTM, supervised Hochreiter"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Long Short-Term Memory",
        "year": 1997,
        "published_in": "Neural Computation",
        "url": "https://www.bioinf.jku.at/publications/older/2604.pdf"
      },
      "learn": [
        {
          "title": "Understanding LSTM Networks - Chris Olah",
          "url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/",
          "type": "article",
          "difficulty": "intermediate"
        },
        {
          "title": "LSTM Explained - StatQuest",
          "url": "https://www.youtube.com/watch?v=YCzL96nL7j0",
          "type": "video",
          "duration": "20 min",
          "difficulty": "beginner"
        },
        {
          "title": "Vanishing Gradient Problem",
          "url": "https://www.youtube.com/watch?v=qhXZsFVxGKo",
          "type": "video",
          "duration": "8 min",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (PDF)",
          "url": "https://www.bioinf.jku.at/publications/older/2604.pdf",
          "type": "paper"
        },
        {
          "title": "LSTM - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Long_short-term_memory",
          "type": "article"
        }
      ],
      "tags": [
        "neural-networks",
        "sequences",
        "rnn",
        "breakthrough"
      ],
      "era": "ml-rise"
    },
    {
      "id": "deep-blue-1997",
      "year": 1997,
      "month": 5,
      "type": "event",
      "title": "Deep Blue Beats Kasparov",
      "what": "IBM's Deep Blue defeated world chess champion Garry Kasparov. Used brute-force search (200 million positions/second), opening book, and handcrafted evaluation functions. NOT machine learning - just fast search.",
      "why_it_matters": "Showed computers could beat humans at intellectual tasks. But also showed the limitations: Deep Blue couldn't do anything else. Modern AI (AlphaZero) learns to play without human knowledge.",
      "impact": "Major media event. Made AI real to public. Kasparov became AI commentator. Led to question: is beating humans at games = intelligence?",
      "authors": [
        {
          "name": "Murray Campbell",
          "born": 1957,
          "died": null,
          "nationality": "Canadian",
          "role": "Computer Scientist",
          "affiliation": "IBM Research",
          "background": "Led Deep Blue project. PhD from Carnegie Mellon.",
          "contribution": "Project lead for Deep Blue"
        },
        {
          "name": "Feng-hsiung Hsu",
          "born": 1959,
          "died": null,
          "nationality": "Taiwanese-American",
          "role": "Computer Engineer",
          "affiliation": "IBM, Microsoft",
          "background": "Designed custom chess chips. Started project as PhD student.",
          "contribution": "Designed hardware that enabled 200M positions/second"
        }
      ],
      "work": {
        "type": "system",
        "title": "Deep Blue",
        "year": 1997,
        "url": "https://www.ibm.com/history/deep-blue"
      },
      "learn": [
        {
          "title": "Deep Blue Documentary",
          "url": "https://www.youtube.com/watch?v=NJarxpYyoFI",
          "type": "video",
          "duration": "45 min",
          "difficulty": "beginner"
        },
        {
          "title": "How Chess Computers Work",
          "url": "https://www.youtube.com/watch?v=DpXy041BIlA",
          "type": "video",
          "duration": "10 min",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Deep Blue - IBM",
          "url": "https://www.ibm.com/history/deep-blue",
          "type": "website"
        },
        {
          "title": "Deep Blue vs Kasparov - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov",
          "type": "article"
        }
      ],
      "tags": [
        "games",
        "chess",
        "milestone",
        "search"
      ],
      "era": "ml-rise"
    },
    {
      "id": "google-founded-1998",
      "year": 1998,
      "month": 9,
      "type": "company",
      "title": "Google Founded - PageRank Algorithm",
      "what": "Larry Page and Sergey Brin founded Google based on PageRank algorithm. Treated web as a graph and used link analysis (a form of ML) to rank pages by importance.",
      "why_it_matters": "PageRank was machine learning applied at massive scale. Google became the world's largest AI company, later acquiring DeepMind and developing Transformer.",
      "impact": "Revolutionized information access. Google's scale drove advances in distributed ML, eventually leading to BERT and Gemini.",
      "authors": [
        {
          "name": "Larry Page",
          "nationality": "American",
          "role": "Co-founder",
          "affiliation": "Google/Stanford",
          "contribution": "Co-invented PageRank"
        },
        {
          "name": "Sergey Brin",
          "nationality": "American",
          "role": "Co-founder",
          "affiliation": "Google/Stanford",
          "contribution": "Co-invented PageRank"
        }
      ],
      "work": {
        "type": "paper",
        "title": "The Anatomy of a Large-Scale Hypertextual Web Search Engine",
        "year": 1998,
        "url": "http://infolab.stanford.edu/~backrub/google.html"
      },
      "learn": [
        {
          "title": "How Google Works",
          "url": "https://www.youtube.com/watch?v=mTBShTwCnD4",
          "type": "video",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Google History",
          "url": "https://about.google/our-story/",
          "type": "website"
        }
      ],
      "tags": [
        "company",
        "search",
        "machine-learning",
        "foundational"
      ],
      "era": "ml-rise"
    },
    {
      "id": "random-forests-2001",
      "year": 2001,
      "month": null,
      "type": "paper",
      "title": "Random Forests",
      "what": "Leo Breiman introduced Random Forests, an ensemble learning method that builds many decision trees and combines their predictions. Simple, robust, and highly effective.",
      "why_it_matters": "Random Forests became one of the most popular ML algorithms. Works well out-of-the-box, handles missing data, provides feature importance.",
      "impact": "Dominant algorithm for structured/tabular data. Still widely used in production alongside deep learning.",
      "authors": [
        {
          "name": "Leo Breiman",
          "born": 1928,
          "died": 2005,
          "nationality": "American",
          "role": "Statistician",
          "affiliation": "UC Berkeley",
          "contribution": "Created Random Forests and CART"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Random Forests",
        "year": 2001,
        "url": "https://link.springer.com/article/10.1023/A:1010933404324"
      },
      "learn": [
        {
          "title": "Random Forest - StatQuest",
          "url": "https://www.youtube.com/watch?v=J4Wdy0Wc_xQ",
          "type": "video",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Random Forests Paper",
          "url": "https://link.springer.com/article/10.1023/A:1010933404324",
          "type": "paper"
        }
      ],
      "tags": [
        "machine-learning",
        "ensemble",
        "algorithm",
        "tabular-data"
      ],
      "era": "ml-rise"
    },
    {
      "id": "neural-language-model-2003",
      "year": 2003,
      "month": null,
      "type": "paper",
      "title": "Neural Probabilistic Language Model",
      "what": "Yoshua Bengio introduced neural network-based language models. Words represented as learned vectors (embeddings). Network predicts next word from previous words.",
      "why_it_matters": "Foundation for all modern NLP. Introduced word embeddings concept. Showed neural nets could model language.",
      "impact": "Led to Word2Vec, GloVe, and eventually Transformers. Bengio won 2018 Turing Award partly for this work.",
      "authors": [
        {
          "name": "Yoshua Bengio",
          "nationality": "Canadian",
          "role": "Lead Author",
          "affiliation": "University of Montreal",
          "contribution": "Pioneered neural language models, 2018 Turing Award"
        }
      ],
      "work": {
        "type": "paper",
        "title": "A Neural Probabilistic Language Model",
        "year": 2003,
        "url": "https://www.jmlr.org/papers/v3/bengio03a.html"
      },
      "learn": [
        {
          "title": "Neural Language Models Intro",
          "url": "https://www.youtube.com/watch?v=OHyygJro46w",
          "type": "video",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper",
          "url": "https://www.jmlr.org/papers/v3/bengio03a.html",
          "type": "paper"
        }
      ],
      "tags": [
        "nlp",
        "language-model",
        "embeddings",
        "foundational",
        "turing-award"
      ],
      "era": "ml-rise"
    },
    {
      "id": "deep-belief-nets-2006",
      "year": 2006,
      "month": null,
      "type": "paper",
      "title": "Deep Belief Networks",
      "what": "Hinton showed how to train deep neural networks by 'pre-training' each layer unsupervised, then fine-tuning with backprop. Proved deep networks could work - they just needed better training.",
      "why_it_matters": "Reignited deep learning research after years of dominance by SVMs. Showed the problem wasn't depth, but training. Led directly to the deep learning revolution.",
      "impact": "Launched modern deep learning era. Hinton's students (Sutskever, Krizhevsky) went on to create breakthrough systems.",
      "authors": [
        {
          "name": "Geoffrey Hinton",
          "born": 1947,
          "died": null,
          "nationality": "British-Canadian",
          "role": "Computer Scientist",
          "affiliation": "Toronto, Google",
          "background": "Godfather of Deep Learning. Kept working on neural nets through AI winters when everyone else gave up.",
          "contribution": "Developed pre-training method, proved deep learning was viable"
        },
        {
          "name": "Simon Osindero",
          "born": null,
          "died": null,
          "nationality": "British",
          "role": "Researcher",
          "affiliation": "Toronto, later DeepMind",
          "background": "Hinton's PhD student. Later worked at DeepMind.",
          "contribution": "Co-developed training algorithm"
        },
        {
          "name": "Yee-Whye Teh",
          "born": 1974,
          "died": null,
          "nationality": "Malaysian-British",
          "role": "Statistician",
          "affiliation": "Toronto, Oxford, DeepMind",
          "background": "Professor at Oxford. DeepMind researcher.",
          "contribution": "Theoretical foundations for pre-training"
        }
      ],
      "work": {
        "type": "paper",
        "title": "A Fast Learning Algorithm for Deep Belief Nets",
        "year": 2006,
        "published_in": "Neural Computation",
        "url": "https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf"
      },
      "learn": [
        {
          "title": "Deep Belief Networks Explained",
          "url": "https://www.youtube.com/watch?v=p8xSMXAZ2Bc",
          "type": "video",
          "duration": "15 min",
          "difficulty": "intermediate"
        },
        {
          "title": "Hinton's Coursera Course on Neural Nets",
          "url": "https://www.cs.toronto.edu/~hinton/coursera_lectures.html",
          "type": "course",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (PDF)",
          "url": "https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf",
          "type": "paper"
        },
        {
          "title": "Deep Belief Network - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Deep_belief_network",
          "type": "article"
        }
      ],
      "tags": [
        "neural-networks",
        "deep-learning",
        "breakthrough",
        "unsupervised"
      ],
      "era": "ml-rise"
    },
    {
      "id": "netflix-prize-2006",
      "year": 2006,
      "month": 10,
      "type": "event",
      "title": "Netflix Prize Competition",
      "what": "Netflix offered $1 million prize to improve their recommendation algorithm by 10%. Released massive dataset. Competition ran until 2009, won by ensemble of many ML techniques.",
      "why_it_matters": "Popularized ML competitions (led to Kaggle). Showed business value of ML. Proved ensemble methods outperform single models.",
      "impact": "Created competitive ML culture. Techniques developed influenced all modern recommendation systems.",
      "authors": [
        {
          "name": "Netflix",
          "role": "Organization",
          "affiliation": "Netflix",
          "contribution": "Hosted Netflix Prize competition"
        }
      ],
      "work": {
        "type": "competition",
        "title": "Netflix Prize",
        "year": 2006,
        "url": "https://www.netflixprize.com/"
      },
      "learn": [
        {
          "title": "Netflix Prize Story",
          "url": "https://www.youtube.com/watch?v=ImpV70uLxyw",
          "type": "video",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Netflix Prize - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Netflix_Prize",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "competition",
        "recommendation",
        "machine-learning",
        "ensemble"
      ],
      "era": "ml-rise"
    },
    {
      "id": "nvidia-cuda-2006",
      "year": 2006,
      "month": 11,
      "type": "technology",
      "title": "NVIDIA CUDA - GPU Computing",
      "what": "NVIDIA released CUDA (Compute Unified Device Architecture), allowing programmers to use GPUs for general-purpose computing. Made parallel processing accessible.",
      "why_it_matters": "CUDA enabled deep learning revolution. Neural networks require massive parallelization that GPUs provide. Without CUDA, AlexNet couldn't have happened.",
      "impact": "Made NVIDIA the backbone of AI industry. GPUs became essential AI hardware. NVIDIA became world's most valuable company.",
      "authors": [
        {
          "name": "NVIDIA",
          "role": "Company",
          "affiliation": "NVIDIA Corporation",
          "contribution": "Created CUDA platform"
        }
      ],
      "work": {
        "type": "technology",
        "title": "CUDA Toolkit",
        "year": 2006,
        "url": "https://developer.nvidia.com/cuda-toolkit"
      },
      "learn": [
        {
          "title": "CUDA Explained",
          "url": "https://www.youtube.com/watch?v=IzU4AVcMFys",
          "type": "video",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "CUDA - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/CUDA",
          "type": "encyclopedia"
        }
      ],
      "tags": [
        "hardware",
        "gpu",
        "nvidia",
        "infrastructure",
        "foundational"
      ],
      "era": "ml-rise"
    },
    {
      "id": "imagenet-2009",
      "year": 2009,
      "month": null,
      "type": "dataset",
      "title": "ImageNet Dataset",
      "what": "Fei-Fei Li and team created ImageNet: 14 million images labeled with 20,000+ categories. Massive, diverse, hierarchical. Started ImageNet Large Scale Visual Recognition Challenge (ILSVRC).",
      "why_it_matters": "THE benchmark that enabled and measured the deep learning revolution. AlexNet (2012), VGG, ResNet all competed here. 'Benchmark drives progress.' Without ImageNet, no modern computer vision.",
      "impact": "Created standard benchmark for vision. Error rate went from 26% (2011) to 3.5% (2015) to near human-level. Competition ended 2017 - problem 'solved'.",
      "authors": [
        {
          "name": "Fei-Fei Li",
          "born": 1976,
          "died": null,
          "nationality": "Chinese-American",
          "role": "Computer Scientist",
          "affiliation": "Stanford",
          "background": "Stanford AI Lab co-director. Founded ImageNet. Google Cloud AI chief scientist (2017-2018). Advocate for human-centered AI.",
          "contribution": "Led ImageNet creation, organized challenges"
        },
        {
          "name": "Jia Deng",
          "born": null,
          "died": null,
          "nationality": "Chinese-American",
          "role": "Computer Scientist",
          "affiliation": "Princeton, later Michigan",
          "background": "Built ImageNet infrastructure. Now professor at Michigan.",
          "contribution": "Technical implementation of ImageNet"
        }
      ],
      "work": {
        "type": "paper",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": 2009,
        "published_in": "CVPR",
        "url": "https://ieeexplore.ieee.org/document/5206848"
      },
      "learn": [
        {
          "title": "How ImageNet Changed Computer Vision - TED Talk by Fei-Fei Li",
          "url": "https://www.youtube.com/watch?v=40riCqvRoMs",
          "type": "video",
          "duration": "18 min",
          "difficulty": "beginner"
        },
        {
          "title": "ImageNet Website",
          "url": "https://www.image-net.org/",
          "type": "website",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "ImageNet - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/ImageNet",
          "type": "article"
        },
        {
          "title": "ImageNet Official Site",
          "url": "https://www.image-net.org/",
          "type": "website"
        }
      ],
      "tags": [
        "dataset",
        "vision",
        "benchmark",
        "foundational"
      ],
      "era": "ml-rise"
    },
    {
      "id": "deepmind-founded-2010",
      "year": 2010,
      "month": 9,
      "type": "company",
      "title": "DeepMind Founded",
      "what": "DeepMind Technologies was founded in London by Demis Hassabis, Shane Legg, and Mustafa Suleyman with the mission to 'solve intelligence' and use it to solve everything else.",
      "why_it_matters": "DeepMind pioneered deep reinforcement learning and created breakthrough systems like AlphaGo and AlphaFold. Became the world's leading AI research lab.",
      "impact": "AlphaFold won Nobel Prize. AlphaGo was cultural milestone. Influenced entire field of AI research.",
      "authors": [
        {
          "name": "Demis Hassabis",
          "nationality": "British",
          "role": "Co-founder & CEO",
          "affiliation": "DeepMind",
          "contribution": "Founded DeepMind, 2024 Nobel Prize in Chemistry"
        },
        {
          "name": "Shane Legg",
          "nationality": "New Zealand",
          "role": "Co-founder & Chief Scientist",
          "affiliation": "DeepMind",
          "contribution": "AI safety researcher, co-authored AGI definitions"
        },
        {
          "name": "Mustafa Suleyman",
          "nationality": "British",
          "role": "Co-founder",
          "affiliation": "DeepMind, later Inflection AI, then Microsoft",
          "contribution": "Applied AI and policy"
        }
      ],
      "work": {
        "type": "organization",
        "title": "DeepMind Founded",
        "year": 2010,
        "url": "https://www.deepmind.com/"
      },
      "learn": [
        {
          "title": "DeepMind: The Story",
          "url": "https://www.deepmind.com/about",
          "type": "website",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "DeepMind Company",
          "url": "https://www.deepmind.com/",
          "type": "website"
        }
      ],
      "tags": [
        "deepmind",
        "company",
        "founding",
        "research-lab"
      ],
      "era": "ml-rise"
    },
    {
      "id": "alexnet-2012",
      "year": 2012,
      "month": 9,
      "type": "paper",
      "title": "AlexNet - Deep Learning Revolution Begins",
      "what": "Krizhevsky, Sutskever, and Hinton trained a deep CNN on ImageNet using GPUs. Won ImageNet challenge with 15.3% error (vs 26.2% for second place). Used ReLU activation, dropout, data augmentation.",
      "why_it_matters": "THE moment deep learning became undeniable. 10% improvement was unheard of. Proved GPUs + big data + deep networks = breakthrough results. Every major AI company started investing heavily.",
      "impact": "Started the deep learning revolution. GPU computing became standard. Launched careers of Sutskever (OpenAI), Krizhevsky. Google acquired Hinton's company.",
      "authors": [
        {
          "name": "Alex Krizhevsky",
          "born": 1986,
          "died": null,
          "nationality": "Ukrainian-Canadian",
          "role": "Computer Scientist",
          "affiliation": "Toronto, Google",
          "background": "PhD student of Hinton. Built AlexNet. Later at Google.",
          "contribution": "Implemented network, optimized GPU training"
        },
        {
          "name": "Ilya Sutskever",
          "born": 1985,
          "died": null,
          "nationality": "Russian-Canadian",
          "role": "Computer Scientist",
          "affiliation": "Toronto, Google Brain, OpenAI",
          "background": "Co-founder and Chief Scientist of OpenAI. Left in 2024. One of most important figures in AI.",
          "contribution": "Training methodology, theoretical insights"
        },
        {
          "name": "Geoffrey Hinton",
          "born": 1947,
          "died": null,
          "nationality": "British-Canadian",
          "role": "Computer Scientist",
          "affiliation": "Toronto, Google",
          "background": "Supervised both Krizhevsky and Sutskever. This paper vindicated decades of his work.",
          "contribution": "Supervision, key architectural decisions"
        }
      ],
      "work": {
        "type": "paper",
        "title": "ImageNet Classification with Deep Convolutional Neural Networks",
        "year": 2012,
        "published_in": "NeurIPS",
        "url": "https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html"
      },
      "learn": [
        {
          "title": "AlexNet Explained",
          "url": "https://www.youtube.com/watch?v=Nq3auVtvd9Q",
          "type": "video",
          "duration": "15 min",
          "difficulty": "beginner"
        },
        {
          "title": "ImageNet Moment - The AI Revolution",
          "url": "https://www.youtube.com/watch?v=qGt-1qSV0Po",
          "type": "video",
          "duration": "12 min",
          "difficulty": "beginner"
        },
        {
          "title": "Original Paper (PDF)",
          "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf",
          "type": "paper",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "AlexNet - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/AlexNet",
          "type": "article"
        },
        {
          "title": "Original Paper (NeurIPS)",
          "url": "https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
          "type": "paper"
        }
      ],
      "tags": [
        "deep-learning",
        "vision",
        "breakthrough",
        "gpu",
        "milestone"
      ],
      "era": "deep-learning"
    },
    {
      "id": "word2vec-2013",
      "year": 2013,
      "month": 1,
      "type": "paper",
      "title": "Word2Vec - Words as Vectors",
      "what": "Mikolov et al. at Google created Word2Vec: train a neural net to predict words from context (or vice versa). Result: each word becomes a vector where similar words are close. Famous: king - man + woman = queen.",
      "why_it_matters": "THE breakthrough that made NLP work with neural networks. Words became numbers that captured meaning. Enabled transfer learning: train once, use everywhere. Foundation for all modern NLP.",
      "impact": "Every modern language model uses embeddings. Led to GloVe, FastText, and eventually contextual embeddings (BERT, GPT). Essential for RAG systems.",
      "key_concepts": [
        {
          "name": "Embedding",
          "definition": "A dense vector representation of a word. Similar words have similar vectors."
        },
        {
          "name": "Skip-gram",
          "definition": "Predict context words from center word."
        },
        {
          "name": "CBOW",
          "definition": "Predict center word from context words."
        },
        {
          "name": "Cosine Similarity",
          "definition": "Measure similarity by angle between vectors. Used in RAG for retrieval."
        }
      ],
      "authors": [
        {
          "name": "Tomas Mikolov",
          "born": 1982,
          "died": null,
          "nationality": "Czech",
          "role": "Computer Scientist",
          "affiliation": "Google, Facebook AI",
          "background": "Invented Word2Vec and FastText. Pioneer of word embeddings.",
          "contribution": "Designed Word2Vec, developed efficient training"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Efficient Estimation of Word Representations in Vector Space",
        "year": 2013,
        "published_in": "arXiv",
        "url": "https://arxiv.org/abs/1301.3781"
      },
      "learn": [
        {
          "title": "Word2Vec Explained - StatQuest",
          "url": "https://www.youtube.com/watch?v=viZrOnJclY0",
          "type": "video",
          "duration": "15 min",
          "difficulty": "beginner"
        },
        {
          "title": "Word Embeddings Visual Guide",
          "url": "https://jalammar.github.io/illustrated-word2vec/",
          "type": "article",
          "difficulty": "beginner"
        },
        {
          "title": "Gensim Word2Vec Tutorial",
          "url": "https://radimrehurek.com/gensim/models/word2vec.html",
          "type": "tutorial",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (arXiv)",
          "url": "https://arxiv.org/abs/1301.3781",
          "type": "paper"
        },
        {
          "title": "Word2Vec - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Word2vec",
          "type": "article"
        }
      ],
      "tags": [
        "nlp",
        "embeddings",
        "breakthrough",
        "vectors",
        "rag-related"
      ],
      "era": "deep-learning"
    },
    {
      "id": "dqn-atari-2013",
      "year": 2013,
      "month": 12,
      "type": "paper",
      "title": "DQN - Deep Q-Network (Atari)",
      "what": "DeepMind's Deep Q-Network learned to play Atari games from raw pixels, achieving superhuman performance on many games using only screen input and score. Combined deep learning with reinforcement learning.",
      "why_it_matters": "First demonstration that deep learning + RL could master complex tasks from raw sensory input. Launched deep reinforcement learning field.",
      "impact": "Directly led to AlphaGo. Proved AI could learn general skills. DeepMind acquired by Google months later.",
      "authors": [
        {
          "name": "Volodymyr Mnih",
          "nationality": "Ukrainian-Canadian",
          "role": "Lead Author",
          "affiliation": "DeepMind",
          "contribution": "Led DQN research"
        },
        {
          "name": "Demis Hassabis",
          "nationality": "British",
          "role": "Co-author",
          "affiliation": "DeepMind",
          "contribution": "Research direction"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Playing Atari with Deep Reinforcement Learning",
        "year": 2013,
        "url": "https://arxiv.org/abs/1312.5602"
      },
      "learn": [
        {
          "title": "DQN Explained",
          "url": "https://www.youtube.com/watch?v=rFwQDDbYTm4",
          "type": "video",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "DQN Paper",
          "url": "https://www.nature.com/articles/nature14236",
          "type": "paper"
        }
      ],
      "tags": [
        "deepmind",
        "reinforcement-learning",
        "deep-learning",
        "games",
        "breakthrough"
      ],
      "era": "deep-learning"
    },
    {
      "id": "deepmind-acquired-2014",
      "year": 2014,
      "month": 1,
      "type": "company",
      "title": "DeepMind Acquired by Google",
      "what": "Google acquired DeepMind for approximately $500 million. DeepMind, founded in 2010 by Demis Hassabis, Shane Legg, and Mustafa Suleyman, became Google's primary AI research lab alongside Google Brain.",
      "why_it_matters": "Signaled big tech's massive investment in AI research. DeepMind went on to create AlphaGo, AlphaFold, and other breakthrough systems that pushed the boundaries of what AI could achieve.",
      "impact": "Established DeepMind as world's leading AI research lab with resources to pursue ambitious long-term projects.",
      "authors": [
        {
          "name": "Demis Hassabis",
          "nationality": "British",
          "role": "Co-founder & CEO",
          "affiliation": "DeepMind",
          "contribution": "Founded DeepMind, led development of AlphaGo and AlphaFold"
        }
      ],
      "work": {
        "type": "acquisition",
        "title": "Google Acquires DeepMind",
        "year": 2014
      },
      "learn": [
        {
          "title": "DeepMind: The Podcast",
          "url": "https://www.deepmind.com/the-podcast",
          "type": "podcast",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Google Acquires DeepMind",
          "url": "https://www.theguardian.com/technology/2014/jan/27/google-acquires-uk-artificial-intelligence-startup-deepmind",
          "type": "news"
        }
      ],
      "tags": [
        "deepmind",
        "google",
        "acquisition",
        "company"
      ],
      "era": "deep-learning"
    },
    {
      "id": "gan-2014",
      "year": 2014,
      "month": 6,
      "type": "paper",
      "title": "GANs - Generative Adversarial Networks",
      "what": "Ian Goodfellow invented GANs: two networks compete - Generator creates fake images, Discriminator tries to detect fakes. Through competition, Generator learns to create increasingly realistic images.",
      "why_it_matters": "First practical generative AI for images. 'Most interesting idea in ML in last 10 years' - Yann LeCun. Led to deepfakes, DALL-E precursors, and image synthesis revolution.",
      "impact": "Launched generative AI for images. StyleGAN, CycleGAN, Pix2Pix all followed. Eventually evolved into diffusion models (DALL-E, Stable Diffusion).",
      "authors": [
        {
          "name": "Ian Goodfellow",
          "born": 1985,
          "died": null,
          "nationality": "American",
          "role": "Computer Scientist",
          "affiliation": "Google, OpenAI, Apple, DeepMind",
          "background": "Invented GANs reportedly after a bar conversation. Co-authored influential Deep Learning textbook.",
          "contribution": "Invented GAN architecture and training procedure"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Generative Adversarial Nets",
        "year": 2014,
        "published_in": "NeurIPS",
        "url": "https://arxiv.org/abs/1406.2661"
      },
      "learn": [
        {
          "title": "GANs Explained - Computerphile",
          "url": "https://www.youtube.com/watch?v=Sw9r8CL98N0",
          "type": "video",
          "duration": "12 min",
          "difficulty": "beginner"
        },
        {
          "title": "GANs from Scratch - PyTorch",
          "url": "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html",
          "type": "tutorial",
          "difficulty": "intermediate"
        },
        {
          "title": "GAN Lab - Interactive Visualization",
          "url": "https://poloclub.github.io/ganlab/",
          "type": "interactive",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Original Paper (arXiv)",
          "url": "https://arxiv.org/abs/1406.2661",
          "type": "paper"
        },
        {
          "title": "GAN - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Generative_adversarial_network",
          "type": "article"
        }
      ],
      "tags": [
        "generative",
        "images",
        "breakthrough",
        "deep-learning"
      ],
      "era": "deep-learning"
    },
    {
      "id": "seq2seq-attention-2014",
      "year": 2014,
      "month": 9,
      "type": "paper",
      "title": "Sequence-to-Sequence with Attention",
      "what": "Sutskever et al. showed neural networks can translate sentences. Bahdanau et al. added 'attention': instead of compressing entire sentence to fixed vector, let decoder look at relevant parts of input for each output word.",
      "why_it_matters": "Attention is THE key innovation in modern AI. It's what 'Transformer' and 'GPT' are built on. Attention lets models focus on relevant information, enabling much longer contexts.",
      "impact": "Revolutionized machine translation. Attention mechanism became foundation for Transformers (2017). Direct path to ChatGPT.",
      "authors": [
        {
          "name": "Dzmitry Bahdanau",
          "born": 1988,
          "died": null,
          "nationality": "Belarusian",
          "role": "Computer Scientist",
          "affiliation": "Mila (Montreal), Apple",
          "background": "Invented attention mechanism. Published influential paper during internship.",
          "contribution": "Invented attention mechanism for neural MT"
        },
        {
          "name": "Ilya Sutskever",
          "born": 1985,
          "died": null,
          "nationality": "Russian-Canadian",
          "role": "Computer Scientist",
          "affiliation": "Google, OpenAI",
          "background": "Same Sutskever from AlexNet. Wrote separate Seq2Seq paper same year.",
          "contribution": "Developed Seq2Seq architecture (without attention)"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "year": 2014,
        "published_in": "ICLR 2015",
        "url": "https://arxiv.org/abs/1409.0473"
      },
      "learn": [
        {
          "title": "Attention Mechanism Explained - Jay Alammar",
          "url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/",
          "type": "article",
          "difficulty": "intermediate"
        },
        {
          "title": "Attention in Neural Networks - StatQuest",
          "url": "https://www.youtube.com/watch?v=PSs6nxngL6k",
          "type": "video",
          "duration": "18 min",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Attention Paper (arXiv)",
          "url": "https://arxiv.org/abs/1409.0473",
          "type": "paper"
        },
        {
          "title": "Seq2Seq Paper (arXiv)",
          "url": "https://arxiv.org/abs/1409.3215",
          "type": "paper"
        }
      ],
      "tags": [
        "nlp",
        "attention",
        "translation",
        "breakthrough"
      ],
      "era": "deep-learning"
    },
    {
      "id": "glove-2014",
      "year": 2014,
      "month": 10,
      "type": "paper",
      "title": "GloVe - Global Word Vectors",
      "what": "Stanford's GloVe (Global Vectors for Word Representation) created word embeddings from word co-occurrence statistics. Efficient training on massive corpora.",
      "why_it_matters": "Provided pre-trained word vectors used by thousands of NLP systems. Complementary approach to Word2Vec.",
      "impact": "GloVe embeddings widely used until replaced by contextual embeddings (BERT). Still used for some applications.",
      "authors": [
        {
          "name": "Jeffrey Pennington",
          "nationality": "American",
          "role": "Lead Author",
          "affiliation": "Stanford",
          "contribution": "Developed GloVe algorithm"
        }
      ],
      "work": {
        "type": "paper",
        "title": "GloVe: Global Vectors for Word Representation",
        "year": 2014,
        "url": "https://nlp.stanford.edu/projects/glove/"
      },
      "learn": [
        {
          "title": "GloVe Explained",
          "url": "https://nlp.stanford.edu/projects/glove/",
          "type": "tutorial",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "GloVe Project",
          "url": "https://nlp.stanford.edu/projects/glove/",
          "type": "project"
        }
      ],
      "tags": [
        "nlp",
        "embeddings",
        "stanford",
        "word-vectors"
      ],
      "era": "deep-learning"
    },
    {
      "id": "resnet-2015",
      "year": 2015,
      "month": 12,
      "type": "paper",
      "title": "ResNet - Residual Networks",
      "what": "He et al. at Microsoft invented skip connections (residual connections): add input directly to output of each block. Allows training very deep networks (152 layers vs AlexNet's 8). Won ImageNet 2015.",
      "why_it_matters": "Skip connections are in EVERY modern deep network including Transformers. Without them, very deep networks don't train. ResNet showed depth = power when training works.",
      "impact": "Enabled much deeper networks. Skip connections became standard. ResNet architecture still used as backbone in many vision systems.",
      "authors": [
        {
          "name": "Kaiming He",
          "born": 1984,
          "died": null,
          "nationality": "Chinese",
          "role": "Computer Scientist",
          "affiliation": "Microsoft Research, Facebook AI, MIT",
          "background": "Lead author of ResNet. Also co-invented ReLU initialization and Mask R-CNN. Professor at MIT.",
          "contribution": "Invented residual connections, led ResNet development"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Deep Residual Learning for Image Recognition",
        "year": 2015,
        "published_in": "CVPR 2016",
        "url": "https://arxiv.org/abs/1512.03385"
      },
      "learn": [
        {
          "title": "ResNet Explained",
          "url": "https://www.youtube.com/watch?v=GWt6Fu05voI",
          "type": "video",
          "duration": "12 min",
          "difficulty": "beginner"
        },
        {
          "title": "Why ResNets Work - Gradient Flow",
          "url": "https://www.youtube.com/watch?v=RYth6EbBUqM",
          "type": "video",
          "duration": "15 min",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (arXiv)",
          "url": "https://arxiv.org/abs/1512.03385",
          "type": "paper"
        },
        {
          "title": "ResNet - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Residual_neural_network",
          "type": "article"
        }
      ],
      "tags": [
        "deep-learning",
        "vision",
        "architecture",
        "breakthrough"
      ],
      "era": "deep-learning"
    },
    {
      "id": "openai-founded-2015",
      "year": 2015,
      "month": 12,
      "type": "company",
      "title": "OpenAI Founded",
      "what": "OpenAI was founded as a non-profit AI research company by Sam Altman, Elon Musk, Greg Brockman, Ilya Sutskever, and others. Initial $1 billion commitment from founders and backers.",
      "why_it_matters": "OpenAI would go on to create GPT series, DALL-E, and ChatGPT - defining the generative AI era. Founded with mission to ensure AI benefits humanity.",
      "impact": "Became the most influential AI company of the 2020s, catalyzing the generative AI revolution.",
      "authors": [
        {
          "name": "Sam Altman",
          "nationality": "American",
          "role": "CEO",
          "affiliation": "OpenAI",
          "contribution": "Co-founder and CEO"
        },
        {
          "name": "Ilya Sutskever",
          "nationality": "Canadian-Israeli",
          "role": "Chief Scientist",
          "affiliation": "OpenAI",
          "contribution": "Co-founder, led research until 2024"
        }
      ],
      "work": {
        "type": "organization",
        "title": "OpenAI Founded",
        "year": 2015,
        "url": "https://openai.com/blog/introducing-openai"
      },
      "learn": [
        {
          "title": "OpenAI: Our Story",
          "url": "https://openai.com/about/",
          "type": "website",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Introducing OpenAI",
          "url": "https://openai.com/blog/introducing-openai",
          "type": "blog"
        }
      ],
      "tags": [
        "openai",
        "company",
        "founding",
        "ai-safety"
      ],
      "era": "deep-learning"
    },
    {
      "id": "alphago-2016",
      "year": 2016,
      "month": 3,
      "type": "event",
      "title": "AlphaGo Beats Lee Sedol",
      "what": "DeepMind's AlphaGo defeated world Go champion Lee Sedol 4-1. Used deep neural networks + Monte Carlo tree search + reinforcement learning. Move 37 in Game 2 was called the 'most beautiful move ever played.'",
      "why_it_matters": "Go was thought to be 10+ years away for AI. Showed reinforcement learning + deep learning could master complex tasks without explicit programming. Move 37 showed AI could be creative.",
      "impact": "Made DeepMind famous worldwide. Led to AlphaZero, AlphaFold. Demonstrated transfer of RL research to real applications. Google acquired DeepMind for $500M (2014).",
      "authors": [
        {
          "name": "David Silver",
          "born": 1976,
          "died": null,
          "nationality": "British",
          "role": "Computer Scientist",
          "affiliation": "DeepMind, UCL",
          "background": "Lead researcher on AlphaGo, AlphaZero. Professor at UCL. Pioneer of deep reinforcement learning.",
          "contribution": "Led AlphaGo research team, designed RL algorithms"
        },
        {
          "name": "Demis Hassabis",
          "born": 1976,
          "died": null,
          "nationality": "British",
          "role": "Neuroscientist & Entrepreneur",
          "affiliation": "DeepMind (CEO)",
          "background": "Child chess prodigy. Founded DeepMind. Nobel Prize 2024 for AlphaFold.",
          "contribution": "Founded DeepMind, set research direction"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Mastering the game of Go with deep neural networks and tree search",
        "year": 2016,
        "published_in": "Nature",
        "url": "https://www.nature.com/articles/nature16961"
      },
      "learn": [
        {
          "title": "AlphaGo Documentary (full film)",
          "url": "https://www.youtube.com/watch?v=WXuK6gekU1Y",
          "type": "video",
          "duration": "90 min",
          "difficulty": "beginner"
        },
        {
          "title": "Move 37 Explained",
          "url": "https://www.youtube.com/watch?v=JNrXgpSEEIE",
          "type": "video",
          "duration": "8 min",
          "difficulty": "beginner"
        },
        {
          "title": "How AlphaGo Works - DeepMind",
          "url": "https://deepmind.google/research/highlighted-research/alphago/",
          "type": "article",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "AlphaGo - DeepMind",
          "url": "https://deepmind.google/research/highlighted-research/alphago/",
          "type": "website"
        },
        {
          "title": "Original Paper (Nature)",
          "url": "https://www.nature.com/articles/nature16961",
          "type": "paper"
        }
      ],
      "tags": [
        "reinforcement-learning",
        "games",
        "milestone",
        "deepmind"
      ],
      "era": "deep-learning"
    },
    {
      "id": "openai-gym-2016",
      "year": 2016,
      "month": 4,
      "type": "tool",
      "title": "OpenAI Gym",
      "what": "OpenAI released Gym, a toolkit for developing and comparing reinforcement learning algorithms. Provided standardized environments from Atari games to robotics simulations.",
      "why_it_matters": "Standardized RL research. Made it easy to benchmark algorithms. Democratized RL research beyond large labs.",
      "impact": "Became de facto standard for RL research. Thousands of papers use Gym environments.",
      "authors": [
        {
          "name": "OpenAI",
          "role": "Organization",
          "affiliation": "OpenAI",
          "contribution": "Created OpenAI Gym"
        }
      ],
      "work": {
        "type": "tool",
        "title": "OpenAI Gym",
        "year": 2016,
        "url": "https://github.com/openai/gym"
      },
      "learn": [
        {
          "title": "OpenAI Gym Tutorial",
          "url": "https://www.gymlibrary.dev/",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "OpenAI Gym Paper",
          "url": "https://arxiv.org/abs/1606.01540",
          "type": "paper"
        }
      ],
      "tags": [
        "openai",
        "reinforcement-learning",
        "toolkit",
        "open-source"
      ],
      "era": "deep-learning"
    },
    {
      "id": "google-tpu-2016",
      "year": 2016,
      "month": 5,
      "type": "technology",
      "title": "Google TPU - AI-Specific Chips",
      "what": "Google announced Tensor Processing Units (TPUs), custom ASICs designed specifically for neural network computations. TPUs powered AlphaGo's match against Lee Sedol.",
      "why_it_matters": "First major custom AI accelerator. Showed general-purpose GPUs weren't optimal for AI. Started AI chip race.",
      "impact": "Launched AI hardware competition. AWS, Microsoft, startups all developing AI chips. Powers Google's AI services.",
      "authors": [
        {
          "name": "Google",
          "role": "Company",
          "affiliation": "Google",
          "contribution": "Designed TPU architecture"
        }
      ],
      "work": {
        "type": "technology",
        "title": "Tensor Processing Unit",
        "year": 2016,
        "url": "https://cloud.google.com/tpu"
      },
      "learn": [
        {
          "title": "TPU Explained - Google",
          "url": "https://www.youtube.com/watch?v=MXxN4fv01c8",
          "type": "video",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "TPU Paper",
          "url": "https://arxiv.org/abs/1704.04760",
          "type": "paper"
        }
      ],
      "tags": [
        "hardware",
        "google",
        "tpu",
        "infrastructure",
        "accelerator"
      ],
      "era": "deep-learning"
    },
    {
      "id": "transformer-2017",
      "year": 2017,
      "month": 6,
      "type": "paper",
      "title": "Transformer - 'Attention Is All You Need'",
      "what": "Google researchers replaced RNNs/LSTMs with pure attention mechanism. Key innovations: self-attention (each word attends to all other words), positional encoding, multi-head attention. Parallelizable = faster training.",
      "why_it_matters": "THE architecture behind all modern LLMs. GPT = Generative Pre-trained TRANSFORMER. BERT, T5, Claude, Llama - all Transformers. Understanding this architecture is essential for any AI engineer.",
      "impact": "Replaced RNNs everywhere. Enabled training on much larger datasets. Made GPT-scale models possible. Most cited ML paper of all time.",
      "key_concepts": [
        {
          "name": "Self-Attention",
          "definition": "Each token attends to all other tokens in sequence. O(n²) complexity."
        },
        {
          "name": "Multi-Head Attention",
          "definition": "Multiple attention patterns in parallel, then combined."
        },
        {
          "name": "Positional Encoding",
          "definition": "Since no recurrence, need to inject position information."
        },
        {
          "name": "Encoder-Decoder",
          "definition": "Original paper had both. BERT uses encoder only, GPT uses decoder only."
        }
      ],
      "authors": [
        {
          "name": "Ashish Vaswani",
          "born": null,
          "died": null,
          "nationality": "Indian-American",
          "role": "Researcher",
          "affiliation": "Google Brain, later founded Essential AI",
          "background": "Lead author of Attention paper. Left Google to start AI company.",
          "contribution": "Led Transformer design"
        },
        {
          "name": "Jakob Uszkoreit",
          "born": null,
          "died": null,
          "nationality": "German",
          "role": "Researcher",
          "affiliation": "Google, later founded Inceptive",
          "background": "Key contributor. Now applies transformers to biology.",
          "contribution": "Core architecture design"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Attention Is All You Need",
        "year": 2017,
        "published_in": "NeurIPS",
        "url": "https://arxiv.org/abs/1706.03762"
      },
      "learn": [
        {
          "title": "The Illustrated Transformer - Jay Alammar",
          "url": "https://jalammar.github.io/illustrated-transformer/",
          "type": "article",
          "difficulty": "intermediate"
        },
        {
          "title": "Attention in Transformers - 3Blue1Brown",
          "url": "https://www.youtube.com/watch?v=eMlx5fFNoYc",
          "type": "video",
          "duration": "26 min",
          "difficulty": "beginner"
        },
        {
          "title": "Let's Build GPT from Scratch - Andrej Karpathy",
          "url": "https://www.youtube.com/watch?v=kCc8FmEb1nY",
          "type": "video",
          "duration": "120 min",
          "difficulty": "intermediate"
        },
        {
          "title": "Annotated Transformer (PyTorch code)",
          "url": "https://nlp.seas.harvard.edu/annotated-transformer/",
          "type": "tutorial",
          "difficulty": "advanced"
        }
      ],
      "references": [
        {
          "title": "Original Paper (arXiv)",
          "url": "https://arxiv.org/abs/1706.03762",
          "type": "paper"
        },
        {
          "title": "Transformer - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
          "type": "article"
        }
      ],
      "tags": [
        "transformer",
        "attention",
        "breakthrough",
        "foundational",
        "architecture"
      ],
      "era": "transformers"
    },
    {
      "id": "alphazero-2017",
      "year": 2017,
      "month": 12,
      "type": "paper",
      "title": "AlphaZero - Mastering Games from Scratch",
      "what": "DeepMind's AlphaZero learned to play Chess, Shogi, and Go at superhuman level using only the rules of each game - no human data. Trained through self-play in just hours, defeating world champion programs in all three games.",
      "why_it_matters": "Proved that general-purpose learning algorithms could master any well-defined problem through self-play. The same algorithm worked across completely different games.",
      "impact": "Demonstrated the power of reinforcement learning and self-play. Influenced thinking about general AI systems.",
      "authors": [
        {
          "name": "David Silver",
          "nationality": "British",
          "role": "Lead Researcher",
          "affiliation": "DeepMind",
          "contribution": "Led AlphaGo and AlphaZero projects"
        },
        {
          "name": "Demis Hassabis",
          "nationality": "British",
          "role": "CEO & Co-author",
          "affiliation": "DeepMind",
          "contribution": "Overall vision and research direction"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
        "year": 2017,
        "url": "https://arxiv.org/abs/1712.01815"
      },
      "learn": [
        {
          "title": "AlphaZero Explained",
          "url": "https://www.youtube.com/watch?v=7L2sUGcOgh0",
          "type": "video",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "AlphaZero - DeepMind",
          "url": "https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go",
          "type": "blog"
        }
      ],
      "tags": [
        "deepmind",
        "reinforcement-learning",
        "games",
        "self-play"
      ],
      "era": "transformers"
    },
    {
      "id": "gpt1-2018",
      "year": 2018,
      "month": 6,
      "type": "paper",
      "title": "GPT-1 - Generative Pre-training",
      "what": "OpenAI trained a Transformer decoder on BooksCorpus (7000 books) using next-token prediction. Then fine-tuned for tasks. Showed generative pre-training transfers across tasks.",
      "why_it_matters": "THE ancestor of ChatGPT. Established the 'GPT' paradigm: unsupervised pre-training on massive text, then supervised fine-tuning. Simple but powerful.",
      "impact": "Started the GPT lineage. Led to GPT-2 (2019), GPT-3 (2020), ChatGPT (2022). Showed decoder-only models could be powerful.",
      "authors": [
        {
          "name": "Alec Radford",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Researcher",
          "affiliation": "OpenAI",
          "background": "Lead researcher on GPT series and CLIP. Key figure in OpenAI's early research.",
          "contribution": "Led GPT-1/2/3 development"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Improving Language Understanding by Generative Pre-Training",
        "year": 2018,
        "published_in": "OpenAI",
        "url": "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
      },
      "learn": [
        {
          "title": "GPT Explained - Jay Alammar",
          "url": "https://jalammar.github.io/illustrated-gpt2/",
          "type": "article",
          "difficulty": "intermediate"
        },
        {
          "title": "How GPT Works - Computerphile",
          "url": "https://www.youtube.com/watch?v=wjZofJX0v4M",
          "type": "video",
          "duration": "15 min",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Original Paper (PDF)",
          "url": "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf",
          "type": "paper"
        },
        {
          "title": "GPT-1 - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
          "type": "article"
        }
      ],
      "tags": [
        "transformer",
        "gpt",
        "openai",
        "breakthrough"
      ],
      "era": "transformers"
    },
    {
      "id": "bert-2018",
      "year": 2018,
      "month": 10,
      "type": "paper",
      "title": "BERT - Bidirectional Encoder",
      "what": "Google created BERT: pre-train a Transformer encoder on massive text using masked language modeling (predict [MASK] tokens). Then fine-tune on downstream tasks. Bidirectional = looks at context on both sides.",
      "why_it_matters": "Revolutionized NLP. BERT embeddings capture deep contextual meaning. 'Bank' near 'river' vs 'money' gets different vectors. BERT embeddings are still used in many RAG systems.",
      "impact": "Dominated NLP benchmarks for 2 years. Used in Google Search. Spawned RoBERTa, ALBERT, DistilBERT. Showed pre-training + fine-tuning paradigm.",
      "authors": [
        {
          "name": "Jacob Devlin",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Researcher",
          "affiliation": "Google AI",
          "background": "Lead author of BERT. Previous work on neural MT.",
          "contribution": "Designed BERT architecture and training"
        }
      ],
      "work": {
        "type": "paper",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "year": 2018,
        "published_in": "NAACL 2019",
        "url": "https://arxiv.org/abs/1810.04805"
      },
      "learn": [
        {
          "title": "BERT Explained - Jay Alammar",
          "url": "https://jalammar.github.io/illustrated-bert/",
          "type": "article",
          "difficulty": "intermediate"
        },
        {
          "title": "BERT - Hugging Face Course",
          "url": "https://huggingface.co/learn/nlp-course/chapter1/3",
          "type": "tutorial",
          "difficulty": "intermediate"
        },
        {
          "title": "Fine-tuning BERT Tutorial",
          "url": "https://huggingface.co/docs/transformers/training",
          "type": "tutorial",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (arXiv)",
          "url": "https://arxiv.org/abs/1810.04805",
          "type": "paper"
        },
        {
          "title": "BERT - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/BERT_(language_model)",
          "type": "article"
        }
      ],
      "tags": [
        "transformer",
        "nlp",
        "embeddings",
        "breakthrough",
        "rag-related"
      ],
      "era": "transformers"
    },
    {
      "id": "alphafold1-2018",
      "year": 2018,
      "month": 12,
      "type": "paper",
      "title": "AlphaFold 1 - Protein Folding Breakthrough",
      "what": "DeepMind's AlphaFold won the CASP13 (Critical Assessment of protein Structure Prediction) competition, outperforming all other methods in predicting 3D protein structures from amino acid sequences.",
      "why_it_matters": "Protein folding was a 50-year grand challenge in biology. Understanding protein structure is key to understanding disease, designing drugs, and engineering enzymes.",
      "impact": "Demonstrated AI could tackle fundamental scientific problems. Set stage for AlphaFold 2's breakthrough.",
      "authors": [
        {
          "name": "John Jumper",
          "nationality": "American",
          "role": "Lead Researcher",
          "affiliation": "DeepMind",
          "contribution": "Led AlphaFold development"
        },
        {
          "name": "Demis Hassabis",
          "nationality": "British",
          "role": "CEO & Co-author",
          "affiliation": "DeepMind",
          "contribution": "Vision for applying AI to science"
        }
      ],
      "work": {
        "type": "competition",
        "title": "AlphaFold wins CASP13",
        "year": 2018,
        "url": "https://www.deepmind.com/blog/alphafold"
      },
      "learn": [
        {
          "title": "The Protein Folding Problem Explained",
          "url": "https://www.youtube.com/watch?v=KpedmJdrTpY",
          "type": "video",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "AlphaFold - DeepMind",
          "url": "https://www.deepmind.com/research/highlighted-research/alphafold",
          "type": "blog"
        }
      ],
      "tags": [
        "deepmind",
        "biology",
        "protein-folding",
        "scientific-ai"
      ],
      "era": "transformers"
    },
    {
      "id": "gpt2-2019",
      "year": 2019,
      "month": 2,
      "type": "paper",
      "title": "GPT-2 - 'Too Dangerous to Release'",
      "what": "OpenAI scaled up: 1.5B parameters, 40GB of web text (WebText). Showed zero-shot learning on tasks never trained for. Generated remarkably coherent text. Initially withheld due to misuse fears.",
      "why_it_matters": "Showed scaling works. More data + bigger model = better performance. The 'staged release' sparked debate about AI safety and open-sourcing. Text quality shocked researchers.",
      "impact": "Proved scaling hypothesis. Led to GPT-3's massive scale-up. 'Staged release' set precedent for responsible AI disclosure.",
      "authors": [
        {
          "name": "Alec Radford",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Researcher",
          "affiliation": "OpenAI",
          "background": "Same lead as GPT-1.",
          "contribution": "Led research, scaling experiments"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Language Models are Unsupervised Multitask Learners",
        "year": 2019,
        "published_in": "OpenAI",
        "url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
      },
      "learn": [
        {
          "title": "GPT-2 Explained - Jay Alammar",
          "url": "https://jalammar.github.io/illustrated-gpt2/",
          "type": "article",
          "difficulty": "intermediate"
        },
        {
          "title": "Try GPT-2 Online",
          "url": "https://huggingface.co/gpt2",
          "type": "interactive",
          "difficulty": "beginner"
        },
        {
          "title": "nanoGPT - Train GPT-2 yourself",
          "url": "https://github.com/karpathy/nanoGPT",
          "type": "code",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (PDF)",
          "url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
          "type": "paper"
        },
        {
          "title": "GPT-2 Release Blog",
          "url": "https://openai.com/blog/better-language-models/",
          "type": "blog"
        }
      ],
      "tags": [
        "transformer",
        "gpt",
        "openai",
        "scaling"
      ],
      "era": "transformers"
    },
    {
      "id": "openai-five-2019",
      "year": 2019,
      "month": 4,
      "type": "event",
      "title": "OpenAI Five Beats Dota 2 World Champions",
      "what": "OpenAI Five, a team of 5 AI agents, defeated the reigning Dota 2 world champions OG in a best-of-3 series. Dota 2 requires long-term strategy, teamwork, and real-time decisions.",
      "why_it_matters": "Dota 2 is far more complex than chess or Go - imperfect information, continuous actions, long-term planning. Showed RL could handle real-world complexity.",
      "impact": "Demonstrated multi-agent coordination. Pushed boundaries of RL scale (800 petaflop-days training).",
      "authors": [
        {
          "name": "OpenAI",
          "role": "Organization",
          "affiliation": "OpenAI",
          "contribution": "Developed OpenAI Five"
        }
      ],
      "work": {
        "type": "project",
        "title": "OpenAI Five",
        "year": 2019,
        "url": "https://openai.com/research/openai-five-defeats-dota-2-world-champions"
      },
      "learn": [
        {
          "title": "OpenAI Five Documentary",
          "url": "https://www.youtube.com/watch?v=eHipy_j29Xw",
          "type": "video",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "OpenAI Five Blog",
          "url": "https://openai.com/research/openai-five",
          "type": "blog"
        }
      ],
      "tags": [
        "openai",
        "reinforcement-learning",
        "games",
        "multi-agent"
      ],
      "era": "transformers"
    },
    {
      "id": "gpt3-2020",
      "year": 2020,
      "month": 5,
      "type": "paper",
      "title": "GPT-3 - Emergent Abilities",
      "what": "OpenAI scaled to 175B parameters, trained on 300B tokens. Showed 'few-shot learning': describe a task in the prompt with a few examples, model performs it. No fine-tuning needed!",
      "why_it_matters": "Showed emergence: abilities appear at scale that don't exist in smaller models. Few-shot prompting = foundation of prompt engineering. API access democratized LLM usage.",
      "impact": "Launched the 'foundation model' paradigm. OpenAI API became product. GitHub Copilot built on GPT-3. Started commercial LLM era.",
      "authors": [
        {
          "name": "Tom Brown",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Researcher",
          "affiliation": "OpenAI, later Anthropic",
          "background": "Lead author on GPT-3. Later co-founded Anthropic.",
          "contribution": "Led GPT-3 scaling and evaluation"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Language Models are Few-Shot Learners",
        "year": 2020,
        "published_in": "NeurIPS",
        "url": "https://arxiv.org/abs/2005.14165"
      },
      "learn": [
        {
          "title": "GPT-3 Explained - AI Coffee Break",
          "url": "https://www.youtube.com/watch?v=SY5PvZrJhLE",
          "type": "video",
          "duration": "20 min",
          "difficulty": "beginner"
        },
        {
          "title": "OpenAI API Quickstart",
          "url": "https://platform.openai.com/docs/quickstart",
          "type": "tutorial",
          "difficulty": "beginner"
        },
        {
          "title": "Prompt Engineering Guide",
          "url": "https://www.promptingguide.ai/",
          "type": "guide",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Original Paper (arXiv)",
          "url": "https://arxiv.org/abs/2005.14165",
          "type": "paper"
        },
        {
          "title": "GPT-3 - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/GPT-3",
          "type": "article"
        }
      ],
      "tags": [
        "transformer",
        "gpt",
        "openai",
        "scaling",
        "emergence",
        "api"
      ],
      "era": "transformers"
    },
    {
      "id": "rag-original-2020",
      "year": 2020,
      "month": 5,
      "type": "paper",
      "title": "RAG - Retrieval-Augmented Generation",
      "what": "Facebook AI (Meta) combined retrieval with generation: given a query, retrieve relevant documents, then use them as context for the LLM. Model can access external knowledge not in its training data.",
      "why_it_matters": "THE technique you're learning in this course! RAG solves hallucination by grounding answers in retrieved documents. Enables LLMs to use private/recent data without retraining.",
      "impact": "Standard technique for enterprise LLM applications. Every chatbot with document access uses RAG. LangChain, LlamaIndex built around this pattern.",
      "key_concepts": [
        {
          "name": "Retriever",
          "definition": "Finds relevant documents given a query. Usually uses embeddings + vector similarity."
        },
        {
          "name": "Generator",
          "definition": "LLM that produces answer using retrieved context."
        },
        {
          "name": "Context Window",
          "definition": "Maximum text LLM can process. Retrieved docs must fit here."
        },
        {
          "name": "Grounding",
          "definition": "Basing LLM output on retrieved evidence to reduce hallucination."
        }
      ],
      "authors": [
        {
          "name": "Patrick Lewis",
          "born": null,
          "died": null,
          "nationality": "Irish",
          "role": "Researcher",
          "affiliation": "Facebook AI, UCL",
          "background": "Lead author of RAG paper. PhD at UCL.",
          "contribution": "Designed RAG architecture and training"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
        "year": 2020,
        "published_in": "NeurIPS",
        "url": "https://arxiv.org/abs/2005.11401"
      },
      "learn": [
        {
          "title": "RAG Explained - IBM",
          "url": "https://www.youtube.com/watch?v=T-D1OfcDW1M",
          "type": "video",
          "duration": "7 min",
          "difficulty": "beginner"
        },
        {
          "title": "Building RAG with LangChain",
          "url": "https://python.langchain.com/docs/tutorials/rag/",
          "type": "tutorial",
          "difficulty": "intermediate"
        },
        {
          "title": "RAG from Scratch - LangChain",
          "url": "https://www.youtube.com/watch?v=wd7TZ4w1mSw",
          "type": "video",
          "duration": "20 min",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Original Paper (arXiv)",
          "url": "https://arxiv.org/abs/2005.11401",
          "type": "paper"
        },
        {
          "title": "RAG - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Retrieval-augmented_generation",
          "type": "article"
        }
      ],
      "tags": [
        "rag",
        "retrieval",
        "nlp",
        "breakthrough",
        "course-core"
      ],
      "era": "transformers"
    },
    {
      "id": "alphafold2-2020",
      "year": 2020,
      "month": 11,
      "type": "paper",
      "title": "AlphaFold 2 - Protein Folding Solved",
      "what": "AlphaFold 2 achieved atomic-level accuracy in protein structure prediction, effectively solving the 50-year protein folding problem. Won CASP14 by a huge margin, with accuracy comparable to experimental methods.",
      "why_it_matters": "One of the most significant scientific breakthroughs of the decade. Revolutionized structural biology, enabling rapid understanding of protein function, drug discovery, and disease research.",
      "impact": "Named Science's 2021 Breakthrough of the Year. Hassabis and Jumper later won 2024 Nobel Prize in Chemistry.",
      "authors": [
        {
          "name": "John Jumper",
          "nationality": "American",
          "role": "Lead Researcher",
          "affiliation": "DeepMind",
          "contribution": "Led AlphaFold 2 development, 2024 Nobel Prize in Chemistry"
        },
        {
          "name": "Demis Hassabis",
          "nationality": "British",
          "role": "CEO & Co-author",
          "affiliation": "DeepMind",
          "contribution": "Vision and leadership, 2024 Nobel Prize in Chemistry"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Highly accurate protein structure prediction with AlphaFold",
        "year": 2021,
        "url": "https://www.nature.com/articles/s41586-021-03819-2"
      },
      "learn": [
        {
          "title": "AlphaFold Explained - Two Minute Papers",
          "url": "https://www.youtube.com/watch?v=B9PL__gVxLI",
          "type": "video",
          "difficulty": "beginner"
        },
        {
          "title": "AlphaFold Protein Structure Database",
          "url": "https://alphafold.ebi.ac.uk/",
          "type": "tool",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "AlphaFold 2 - Nature Paper",
          "url": "https://www.nature.com/articles/s41586-021-03819-2",
          "type": "paper"
        },
        {
          "title": "2024 Nobel Prize in Chemistry",
          "url": "https://www.nobelprize.org/prizes/chemistry/2024/summary/",
          "type": "award"
        }
      ],
      "tags": [
        "deepmind",
        "biology",
        "protein-folding",
        "nobel-prize",
        "scientific-ai"
      ],
      "era": "transformers"
    },
    {
      "id": "dalle-2021",
      "year": 2021,
      "month": 1,
      "type": "paper",
      "title": "DALL·E - Text to Image",
      "what": "OpenAI trained a 12B parameter Transformer to generate images from text descriptions. 'An armchair in the shape of an avocado' → image. Combined GPT-style training with image tokens.",
      "why_it_matters": "Showed transformers work for images too, not just text. Opened the door to multimodal AI. DALL·E 2 (2022) and DALL·E 3 (2023) dramatically improved quality.",
      "impact": "Launched text-to-image AI race. Followed by Midjourney, Stable Diffusion. Sparked debates about AI art and copyright.",
      "authors": [
        {
          "name": "Aditya Ramesh",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Researcher",
          "affiliation": "OpenAI",
          "background": "Lead researcher on DALL·E series.",
          "contribution": "Designed DALL·E architecture"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Zero-Shot Text-to-Image Generation",
        "year": 2021,
        "published_in": "ICML",
        "url": "https://arxiv.org/abs/2102.12092"
      },
      "learn": [
        {
          "title": "How DALL·E Works",
          "url": "https://www.youtube.com/watch?v=NnqSxwgxs-4",
          "type": "video",
          "duration": "12 min",
          "difficulty": "beginner"
        },
        {
          "title": "DALL·E 3 Prompt Guide",
          "url": "https://platform.openai.com/docs/guides/images",
          "type": "guide",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Original Paper (arXiv)",
          "url": "https://arxiv.org/abs/2102.12092",
          "type": "paper"
        },
        {
          "title": "DALL·E - OpenAI",
          "url": "https://openai.com/dall-e-3",
          "type": "website"
        }
      ],
      "tags": [
        "generative",
        "images",
        "multimodal",
        "openai"
      ],
      "era": "transformers"
    },
    {
      "id": "anthropic-founding-2021",
      "year": 2021,
      "month": 1,
      "type": "company",
      "title": "Anthropic Founded - AI Safety Focus",
      "what": "Dario and Daniela Amodei left OpenAI with 10+ researchers to found Anthropic. Focus on AI safety research and building safer AI systems. Raised $124M initial funding.",
      "why_it_matters": "Created major OpenAI competitor with different philosophy. Prioritizes interpretability, Constitutional AI, honest AI. Alternative approach to AI development.",
      "impact": "Built Claude, raised $7B+, became major AI lab. Proved safety-focused AI can be competitive.",
      "authors": [
        {
          "name": "Dario Amodei",
          "born": 1983,
          "died": null,
          "nationality": "American",
          "role": "CEO",
          "affiliation": "Anthropic",
          "background": "PhD in computational neuroscience (Princeton). Former VP of Research at OpenAI. Left over concerns about safety and commercialization.",
          "contribution": "Co-founded Anthropic, CEO"
        },
        {
          "name": "Daniela Amodei",
          "born": 1981,
          "died": null,
          "nationality": "American",
          "role": "President",
          "affiliation": "Anthropic",
          "background": "Former VP of Operations at OpenAI. Prior experience at Stripe. Handles business operations.",
          "contribution": "Co-founded Anthropic, President"
        },
        {
          "name": "Tom Brown",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Co-founder",
          "affiliation": "Anthropic",
          "background": "Lead author of GPT-3 paper at OpenAI. Key technical contributor.",
          "contribution": "Co-founded Anthropic"
        },
        {
          "name": "Chris Olah",
          "born": null,
          "died": null,
          "nationality": "Canadian",
          "role": "Co-founder",
          "affiliation": "Anthropic",
          "background": "Famous for neural network visualization work. Pioneer in AI interpretability at Google Brain.",
          "contribution": "Co-founded Anthropic, leads interpretability research"
        }
      ],
      "work": {
        "type": "company",
        "title": "Anthropic",
        "year": 2021,
        "url": "https://www.anthropic.com/"
      },
      "learn": [
        {
          "title": "Anthropic Research",
          "url": "https://www.anthropic.com/research",
          "type": "website",
          "difficulty": "intermediate"
        },
        {
          "title": "Constitutional AI Paper",
          "url": "https://arxiv.org/abs/2212.08073",
          "type": "paper",
          "difficulty": "advanced"
        }
      ],
      "references": [
        {
          "title": "Anthropic",
          "url": "https://www.anthropic.com/",
          "type": "website"
        }
      ],
      "tags": [
        "company",
        "safety",
        "anthropic",
        "founding"
      ],
      "era": "transformers"
    },
    {
      "id": "github-copilot-2021",
      "year": 2021,
      "month": 6,
      "type": "product",
      "title": "GitHub Copilot",
      "what": "GitHub/OpenAI launched Copilot: AI pair programmer powered by OpenAI Codex (GPT-3 fine-tuned on code). Autocompletes code in IDE. Trained on billions of lines of public GitHub code.",
      "why_it_matters": "DIRECTLY relevant to your work as a developer! First AI tool to genuinely change programming workflow. Showed LLMs could be useful for specialized domains.",
      "impact": "Millions of users. Changed how developers code. Sparked debate about training on open source. Led to code-specific models (StarCoder, CodeLlama).",
      "authors": [
        {
          "name": "GitHub/OpenAI Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Teams",
          "affiliation": "GitHub (Microsoft), OpenAI",
          "background": "Joint project between GitHub and OpenAI.",
          "contribution": "Built Codex model and IDE integrations"
        }
      ],
      "work": {
        "type": "product",
        "title": "GitHub Copilot",
        "year": 2021,
        "url": "https://github.com/features/copilot"
      },
      "learn": [
        {
          "title": "GitHub Copilot Tips & Tricks",
          "url": "https://www.youtube.com/watch?v=1qs6QKk0DVc",
          "type": "video",
          "duration": "15 min",
          "difficulty": "beginner"
        },
        {
          "title": "Copilot Documentation",
          "url": "https://docs.github.com/en/copilot",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Copilot - GitHub",
          "url": "https://github.com/features/copilot",
          "type": "website"
        },
        {
          "title": "Codex Paper (arXiv)",
          "url": "https://arxiv.org/abs/2107.03374",
          "type": "paper"
        }
      ],
      "tags": [
        "coding",
        "tools",
        "product",
        "openai",
        "practical"
      ],
      "era": "transformers"
    },
    {
      "id": "alphafold-db-2021",
      "year": 2021,
      "month": 7,
      "type": "release",
      "title": "AlphaFold Database - 200M Protein Structures",
      "what": "DeepMind partnered with EMBL-EBI to release predicted structures for nearly all known proteins (200+ million) for free. Previously, humanity had determined only ~190,000 structures over 50 years.",
      "why_it_matters": "Democratized access to protein structure data. Accelerated research across biology, medicine, agriculture, and sustainability. Every life scientist gained access to critical structural data.",
      "impact": "Downloaded millions of times. Accelerated drug discovery, enzyme engineering, and disease research worldwide.",
      "authors": [
        {
          "name": "DeepMind",
          "role": "Organization",
          "affiliation": "Google DeepMind",
          "contribution": "Developed and released AlphaFold database"
        }
      ],
      "work": {
        "type": "database",
        "title": "AlphaFold Protein Structure Database",
        "year": 2021,
        "url": "https://alphafold.ebi.ac.uk/"
      },
      "learn": [
        {
          "title": "How to Use AlphaFold Database",
          "url": "https://www.ebi.ac.uk/training/online/courses/alphafold/",
          "type": "tutorial",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "AlphaFold Database Announcement",
          "url": "https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe",
          "type": "blog"
        }
      ],
      "tags": [
        "deepmind",
        "biology",
        "open-source",
        "database"
      ],
      "era": "transformers"
    },
    {
      "id": "instructgpt-2022",
      "year": 2022,
      "month": 1,
      "type": "paper",
      "title": "InstructGPT - RLHF Alignment",
      "what": "OpenAI fine-tuned GPT-3 using Reinforcement Learning from Human Feedback (RLHF). Humans rank outputs, train a reward model, then use RL to optimize for human preferences. Makes model helpful and safe.",
      "why_it_matters": "RLHF is why ChatGPT is helpful, not just next-token-predictor. 'Alignment' became a major research area. Understanding RLHF explains model behavior.",
      "impact": "Direct precursor to ChatGPT. Made LLMs actually useful as assistants. Constitutional AI (Claude) and other alignment methods followed.",
      "key_concepts": [
        {
          "name": "RLHF",
          "definition": "Reinforcement Learning from Human Feedback. Train reward model from rankings, then optimize policy."
        },
        {
          "name": "Reward Model",
          "definition": "Predicts human preference scores for outputs."
        },
        {
          "name": "Alignment",
          "definition": "Making AI systems behave according to human intentions and values."
        },
        {
          "name": "PPO",
          "definition": "Proximal Policy Optimization - the RL algorithm used."
        }
      ],
      "authors": [
        {
          "name": "Long Ouyang",
          "born": null,
          "died": null,
          "nationality": "Chinese-American",
          "role": "Researcher",
          "affiliation": "OpenAI",
          "background": "Lead author of InstructGPT paper.",
          "contribution": "Designed RLHF training pipeline"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Training language models to follow instructions with human feedback",
        "year": 2022,
        "published_in": "NeurIPS",
        "url": "https://arxiv.org/abs/2203.02155"
      },
      "learn": [
        {
          "title": "RLHF Explained - Hugging Face",
          "url": "https://huggingface.co/blog/rlhf",
          "type": "article",
          "difficulty": "intermediate"
        },
        {
          "title": "How ChatGPT Was Trained - Computerphile",
          "url": "https://www.youtube.com/watch?v=VPRSBzXzavo",
          "type": "video",
          "duration": "18 min",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Original Paper (arXiv)",
          "url": "https://arxiv.org/abs/2203.02155",
          "type": "paper"
        },
        {
          "title": "InstructGPT Blog",
          "url": "https://openai.com/blog/instruction-following/",
          "type": "blog"
        }
      ],
      "tags": [
        "alignment",
        "rlhf",
        "openai",
        "breakthrough"
      ],
      "era": "genai"
    },
    {
      "id": "alphacode-2022",
      "year": 2022,
      "month": 2,
      "type": "paper",
      "title": "AlphaCode - Competitive Programming AI",
      "what": "DeepMind's AlphaCode achieved approximately median human performance on Codeforces competitive programming contests, solving novel problems requiring reasoning, math, and algorithm design.",
      "why_it_matters": "First AI to reach competitive levels in programming contests. Required solving problems never seen before, demonstrating genuine reasoning capability beyond pattern matching.",
      "impact": "Showed AI could tackle complex multi-step reasoning problems. Precursor to code generation in LLMs.",
      "authors": [
        {
          "name": "DeepMind",
          "role": "Organization",
          "affiliation": "Google DeepMind",
          "contribution": "Developed AlphaCode"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Competition-Level Code Generation with AlphaCode",
        "year": 2022,
        "url": "https://www.science.org/doi/10.1126/science.abq1158"
      },
      "learn": [
        {
          "title": "AlphaCode Explained",
          "url": "https://www.deepmind.com/blog/competitive-programming-with-alphacode",
          "type": "blog",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "AlphaCode Paper - Science",
          "url": "https://www.science.org/doi/10.1126/science.abq1158",
          "type": "paper"
        }
      ],
      "tags": [
        "deepmind",
        "code-generation",
        "reasoning",
        "competition"
      ],
      "era": "genai"
    },
    {
      "id": "nvidia-h100-2022",
      "year": 2022,
      "month": 3,
      "type": "technology",
      "title": "NVIDIA H100 - Transformer Engine",
      "what": "NVIDIA's Hopper H100 GPU with dedicated Transformer Engine. Optimized for large language model training with FP8 precision and 3x performance over A100.",
      "why_it_matters": "The H100 became the most sought-after chip in AI. Every major LLM trains on H100s. Shortages drove NVIDIA's market cap.",
      "impact": "H100 scarcity shaped AI industry dynamics. Became currency of AI training. Drove trillion-dollar NVIDIA valuation.",
      "authors": [
        {
          "name": "NVIDIA",
          "role": "Company",
          "affiliation": "NVIDIA Corporation",
          "contribution": "Hopper architecture"
        }
      ],
      "work": {
        "type": "technology",
        "title": "NVIDIA H100 Tensor Core GPU",
        "year": 2022,
        "url": "https://www.nvidia.com/en-us/data-center/h100/"
      },
      "learn": [
        {
          "title": "H100 Deep Dive",
          "url": "https://www.youtube.com/watch?v=JfqUjR_f7cI",
          "type": "video",
          "difficulty": "advanced"
        }
      ],
      "references": [
        {
          "title": "H100 Datasheet",
          "url": "https://www.nvidia.com/en-us/data-center/h100/",
          "type": "product"
        }
      ],
      "tags": [
        "hardware",
        "nvidia",
        "gpu",
        "llm",
        "infrastructure"
      ],
      "era": "genai"
    },
    {
      "id": "dalle2-2022",
      "year": 2022,
      "month": 4,
      "type": "release",
      "title": "DALL-E 2 - Text-to-Image Generation",
      "what": "OpenAI released DALL-E 2: generates photorealistic images from text descriptions. Uses diffusion model (not the autoregressive approach of DALL-E 1). Inpainting, outpainting, variations.",
      "why_it_matters": "Made AI image generation mainstream. Showed creative AI capabilities. Started the generative art revolution.",
      "impact": "Sparked image generation race. Led to Midjourney, Stable Diffusion. Changed design/art workflows.",
      "authors": [
        {
          "name": "Aditya Ramesh",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Research Scientist",
          "affiliation": "OpenAI",
          "background": "Lead researcher on DALL-E series. Works on multimodal generation.",
          "contribution": "Lead researcher on DALL-E 2"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
        "year": 2022,
        "url": "https://arxiv.org/abs/2204.06125"
      },
      "learn": [
        {
          "title": "DALL-E 2 Paper",
          "url": "https://arxiv.org/abs/2204.06125",
          "type": "paper",
          "difficulty": "advanced"
        },
        {
          "title": "OpenAI Image Generation",
          "url": "https://platform.openai.com/docs/guides/images",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "DALL-E 2 - OpenAI",
          "url": "https://openai.com/dall-e-2",
          "type": "website"
        }
      ],
      "tags": [
        "image-generation",
        "diffusion",
        "openai",
        "multimodal"
      ],
      "era": "genai"
    },
    {
      "id": "midjourney-2022",
      "year": 2022,
      "month": 7,
      "type": "product",
      "title": "Midjourney - AI Art Goes Viral",
      "what": "Midjourney launched public beta: Discord-based AI art generator. Known for artistic, stylized outputs. Created viral images that won art competitions.",
      "why_it_matters": "Made AI art accessible via simple Discord interface. Artistic quality sparked debates about AI creativity and artist displacement.",
      "impact": "100M+ images generated. Won art competition (controversy). Showed AI can have 'artistic style'.",
      "authors": [
        {
          "name": "David Holz",
          "born": 1984,
          "died": null,
          "nationality": "American",
          "role": "Founder & CEO",
          "affiliation": "Midjourney",
          "background": "Co-founder of Leap Motion (hand tracking). Founded Midjourney as independent research lab.",
          "contribution": "Founded Midjourney, led product development"
        }
      ],
      "work": {
        "type": "product",
        "title": "Midjourney",
        "year": 2022,
        "url": "https://www.midjourney.com/"
      },
      "learn": [
        {
          "title": "Midjourney Documentation",
          "url": "https://docs.midjourney.com/",
          "type": "documentation",
          "difficulty": "beginner"
        },
        {
          "title": "Midjourney Prompt Guide",
          "url": "https://docs.midjourney.com/docs/prompts",
          "type": "guide",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Midjourney",
          "url": "https://www.midjourney.com/",
          "type": "website"
        }
      ],
      "tags": [
        "image-generation",
        "art",
        "product",
        "discord"
      ],
      "era": "genai"
    },
    {
      "id": "stable-diffusion-2022",
      "year": 2022,
      "month": 8,
      "type": "release",
      "title": "Stable Diffusion - Open Source Image AI",
      "what": "Stability AI released Stable Diffusion publicly with open weights. Uses diffusion process (iteratively denoise random noise into image). Runs on consumer GPUs. Fully open source.",
      "why_it_matters": "First high-quality open-source image generation model. Showed open source could compete with closed models. Sparked explosion of fine-tuned models, ControlNet, LoRA.",
      "impact": "Created entire ecosystem of image AI tools. ComfyUI, Automatic1111. Thousands of fine-tuned models. Democratized image generation.",
      "authors": [
        {
          "name": "Robin Rombach",
          "born": null,
          "died": null,
          "nationality": "German",
          "role": "Researcher",
          "affiliation": "LMU Munich, Stability AI",
          "background": "Co-invented Latent Diffusion Models. Lead researcher.",
          "contribution": "Designed latent diffusion architecture"
        }
      ],
      "work": {
        "type": "paper",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "year": 2022,
        "published_in": "CVPR",
        "url": "https://arxiv.org/abs/2112.10752"
      },
      "learn": [
        {
          "title": "How Diffusion Models Work",
          "url": "https://www.youtube.com/watch?v=1CIpzeNxIhU",
          "type": "video",
          "duration": "20 min",
          "difficulty": "intermediate"
        },
        {
          "title": "Stable Diffusion Tutorial",
          "url": "https://huggingface.co/docs/diffusers/using-diffusers/sdxl",
          "type": "tutorial",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Stable Diffusion - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Stable_Diffusion",
          "type": "article"
        },
        {
          "title": "Stability AI",
          "url": "https://stability.ai/",
          "type": "website"
        }
      ],
      "tags": [
        "generative",
        "images",
        "open-source",
        "diffusion"
      ],
      "era": "genai"
    },
    {
      "id": "perplexity-2022",
      "year": 2022,
      "month": 8,
      "type": "product",
      "title": "Perplexity AI - Answer Engine",
      "what": "Perplexity launched as an 'answer engine' combining search with LLM generation. Provides sourced answers instead of links. Raised $500M+ valuation.",
      "why_it_matters": "Challenged Google's search paradigm. Showed LLMs could transform information retrieval. Popular with researchers.",
      "impact": "Grew to millions of users. Forced Google to add AI Overviews. New category of 'answer engines'.",
      "authors": [
        {
          "name": "Aravind Srinivas",
          "nationality": "Indian-American",
          "role": "CEO",
          "affiliation": "Perplexity AI",
          "contribution": "Founded Perplexity"
        }
      ],
      "work": {
        "type": "product",
        "title": "Perplexity",
        "year": 2022,
        "url": "https://www.perplexity.ai/"
      },
      "learn": [
        {
          "title": "Perplexity Website",
          "url": "https://www.perplexity.ai/",
          "type": "product",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Perplexity AI",
          "url": "https://www.perplexity.ai/",
          "type": "website"
        }
      ],
      "tags": [
        "search",
        "rag",
        "product",
        "startup"
      ],
      "era": "genai"
    },
    {
      "id": "whisper-2022",
      "year": 2022,
      "month": 9,
      "type": "release",
      "title": "Whisper - Open Source Speech Recognition",
      "what": "OpenAI released Whisper: open-source speech recognition trained on 680K hours of multilingual data. Near-human accuracy, handles accents, noise, and 99 languages.",
      "why_it_matters": "Made high-quality speech-to-text accessible to everyone. Runs locally, free. Essential for voice-based AI applications and transcription.",
      "impact": "Democratized speech recognition. Used in podcasts, meetings, accessibility tools. Foundation for voice AI apps.",
      "authors": [
        {
          "name": "Alec Radford",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Research Scientist",
          "affiliation": "OpenAI",
          "background": "Key contributor to GPT series. Lead researcher on Whisper. Also worked on CLIP.",
          "contribution": "Lead researcher on Whisper"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
        "year": 2022,
        "url": "https://arxiv.org/abs/2212.04356"
      },
      "learn": [
        {
          "title": "Whisper GitHub",
          "url": "https://github.com/openai/whisper",
          "type": "code",
          "difficulty": "beginner"
        },
        {
          "title": "Whisper API",
          "url": "https://platform.openai.com/docs/guides/speech-to-text",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Whisper - OpenAI",
          "url": "https://openai.com/research/whisper",
          "type": "website"
        }
      ],
      "tags": [
        "speech",
        "audio",
        "open-source",
        "openai"
      ],
      "era": "genai"
    },
    {
      "id": "chatgpt-2022",
      "year": 2022,
      "month": 11,
      "type": "product",
      "title": "ChatGPT - AI Goes Mainstream",
      "what": "OpenAI released ChatGPT: a conversational interface to GPT-3.5 fine-tuned with RLHF. Free to use. Fastest-growing app in history: 100M users in 2 months.",
      "why_it_matters": "THE moment AI became mainstream. Everyone now knows about LLMs. Changed every industry's AI strategy. You're taking this course because of ChatGPT.",
      "impact": "Launched AI arms race (Google Bard, Microsoft Bing). Every company now has AI strategy. Created the 'AI for everyone' era. $100B+ in AI investments followed.",
      "authors": [
        {
          "name": "OpenAI Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Company",
          "affiliation": "OpenAI",
          "background": "Built on GPT-3.5 + InstructGPT RLHF work.",
          "contribution": "Created ChatGPT product and interface"
        }
      ],
      "work": {
        "type": "product",
        "title": "ChatGPT",
        "year": 2022,
        "url": "https://chat.openai.com/"
      },
      "learn": [
        {
          "title": "How ChatGPT Works - Andrej Karpathy (1hr intro)",
          "url": "https://www.youtube.com/watch?v=zjkBMFhNj_g",
          "type": "video",
          "duration": "60 min",
          "difficulty": "beginner"
        },
        {
          "title": "ChatGPT Prompt Engineering - OpenAI",
          "url": "https://platform.openai.com/docs/guides/prompt-engineering",
          "type": "guide",
          "difficulty": "beginner"
        },
        {
          "title": "OpenAI API Documentation",
          "url": "https://platform.openai.com/docs/",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "ChatGPT - OpenAI",
          "url": "https://openai.com/chatgpt",
          "type": "website"
        },
        {
          "title": "ChatGPT - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/ChatGPT",
          "type": "article"
        }
      ],
      "tags": [
        "chatgpt",
        "openai",
        "milestone",
        "product",
        "mainstream"
      ],
      "era": "genai"
    },
    {
      "id": "langchain-rise-2023",
      "year": 2023,
      "month": null,
      "type": "framework",
      "title": "LangChain - LLM Framework Dominance",
      "what": "LangChain became the dominant framework for building LLM applications. Provides abstractions for: chains (sequences of calls), agents (LLM decides actions), RAG, memory, and tool use.",
      "why_it_matters": "THE framework you'll use for RAG in this course. Makes it easy to combine LLMs with retrieval, tools, and external data. Massive ecosystem.",
      "impact": "Standard for LLM application development. Raised $25M+ in funding. LangSmith for observability. Huge community.",
      "authors": [
        {
          "name": "Harrison Chase",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Founder",
          "affiliation": "LangChain",
          "background": "Former ML engineer at Robust Intelligence. Started LangChain in late 2022.",
          "contribution": "Created and leads LangChain"
        }
      ],
      "work": {
        "type": "framework",
        "title": "LangChain",
        "year": 2022,
        "url": "https://github.com/langchain-ai/langchain"
      },
      "learn": [
        {
          "title": "LangChain Documentation",
          "url": "https://python.langchain.com/docs/",
          "type": "documentation",
          "difficulty": "beginner"
        },
        {
          "title": "LangChain RAG Tutorial",
          "url": "https://python.langchain.com/docs/tutorials/rag/",
          "type": "tutorial",
          "difficulty": "intermediate"
        },
        {
          "title": "LangChain Course - DeepLearning.AI",
          "url": "https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/",
          "type": "course",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "LangChain GitHub",
          "url": "https://github.com/langchain-ai/langchain",
          "type": "code"
        },
        {
          "title": "LangChain Company",
          "url": "https://www.langchain.com/",
          "type": "website"
        }
      ],
      "tags": [
        "framework",
        "rag",
        "tools",
        "course-core",
        "practical"
      ],
      "era": "genai"
    },
    {
      "id": "vector-databases-2023",
      "year": 2023,
      "month": null,
      "type": "category",
      "title": "Vector Database Explosion",
      "what": "Vector databases became essential infrastructure: Pinecone (managed), Weaviate, Milvus, Qdrant, Chroma (lightweight). Store embeddings and enable similarity search at scale.",
      "why_it_matters": "ESSENTIAL for RAG! You need a vector database to store document embeddings and retrieve similar documents. This is Module 4 of your course.",
      "impact": "New database category. Pinecone valued at $750M+. Every RAG system needs vector storage. pgvector made PostgreSQL viable.",
      "authors": [
        {
          "name": "Various Teams",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Industry",
          "affiliation": "Pinecone, Weaviate, Qdrant, Chroma, Milvus",
          "background": "Multiple companies and open source projects building vector search infrastructure.",
          "contribution": "Built vector database ecosystem"
        }
      ],
      "work": {
        "type": "category",
        "title": "Vector Databases",
        "year": 2023,
        "url": "https://www.pinecone.io/learn/vector-database/"
      },
      "learn": [
        {
          "title": "What is a Vector Database? - Pinecone",
          "url": "https://www.pinecone.io/learn/vector-database/",
          "type": "article",
          "difficulty": "beginner"
        },
        {
          "title": "pgvector Tutorial",
          "url": "https://github.com/pgvector/pgvector",
          "type": "documentation",
          "difficulty": "intermediate"
        },
        {
          "title": "Choosing a Vector Database",
          "url": "https://www.youtube.com/watch?v=MHwG4w_KbZg",
          "type": "video",
          "duration": "20 min",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Vector Database - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Vector_database",
          "type": "article"
        },
        {
          "title": "Pinecone",
          "url": "https://www.pinecone.io/",
          "type": "website"
        },
        {
          "title": "Chroma",
          "url": "https://www.trychroma.com/",
          "type": "website"
        }
      ],
      "tags": [
        "database",
        "embeddings",
        "rag",
        "infrastructure",
        "course-core"
      ],
      "era": "genai"
    },
    {
      "id": "llama-2023",
      "year": 2023,
      "month": 2,
      "type": "release",
      "title": "LLaMA - Open Source Foundation Model",
      "what": "Meta released LLaMA (Large Language Model Meta AI). Weights leaked online. Showed smaller models (7B-65B) could match larger ones with better training. Sparked open-source LLM explosion.",
      "why_it_matters": "THE open-source LLM that changed everything. Made powerful models accessible to researchers/developers. Mistral, Vicuna, Alpaca all built on LLaMA. You can run this locally!",
      "impact": "Created open-source LLM ecosystem. Companies can now run local models. LLaMA 2/3 officially open. Thousands of fine-tuned variants.",
      "authors": [
        {
          "name": "Hugo Touvron",
          "born": null,
          "died": null,
          "nationality": "French",
          "role": "Researcher",
          "affiliation": "Meta AI (FAIR)",
          "background": "Lead author of LLaMA papers.",
          "contribution": "Led LLaMA training and architecture"
        }
      ],
      "work": {
        "type": "paper",
        "title": "LLaMA: Open and Efficient Foundation Language Models",
        "year": 2023,
        "published_in": "arXiv",
        "url": "https://arxiv.org/abs/2302.13971"
      },
      "learn": [
        {
          "title": "Run LLaMA Locally with Ollama",
          "url": "https://ollama.com/",
          "type": "tool",
          "difficulty": "beginner"
        },
        {
          "title": "LLaMA.cpp - Run on CPU",
          "url": "https://github.com/ggerganov/llama.cpp",
          "type": "code",
          "difficulty": "intermediate"
        },
        {
          "title": "Fine-tune LLaMA with QLoRA",
          "url": "https://huggingface.co/blog/4bit-transformers-bitsandbytes",
          "type": "tutorial",
          "difficulty": "advanced"
        }
      ],
      "references": [
        {
          "title": "LLaMA Paper (arXiv)",
          "url": "https://arxiv.org/abs/2302.13971",
          "type": "paper"
        },
        {
          "title": "Meta LLaMA",
          "url": "https://llama.meta.com/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "open-source",
        "meta",
        "foundational",
        "local"
      ],
      "era": "genai"
    },
    {
      "id": "bing-chat-2023",
      "year": 2023,
      "month": 2,
      "type": "product",
      "title": "Bing Chat - Search Meets LLM",
      "what": "Microsoft launched Bing Chat: GPT-4 integrated into Bing search. Conversational search with citations. Later rebranded to 'Copilot'. Free GPT-4 access.",
      "why_it_matters": "First major GPT-4 integration in consumer product. Showed search + LLM synergy. Made GPT-4 free.",
      "impact": "Pressured Google to rush Bard. 100M+ users. Template for AI search integration.",
      "authors": [
        {
          "name": "Satya Nadella",
          "born": 1967,
          "died": null,
          "nationality": "American (Indian-born)",
          "role": "CEO",
          "affiliation": "Microsoft",
          "background": "Microsoft CEO since 2014. Led Microsoft's AI transformation and OpenAI partnership.",
          "contribution": "Led Microsoft's OpenAI partnership and AI integration"
        },
        {
          "name": "Mikhail Parakhin",
          "born": null,
          "died": null,
          "nationality": "Russian-American",
          "role": "CEO of Advertising & Web Services",
          "affiliation": "Microsoft",
          "background": "Leads Bing and advertising. Previously at Yandex.",
          "contribution": "Led Bing Chat development"
        }
      ],
      "work": {
        "type": "product",
        "title": "Bing Chat / Microsoft Copilot",
        "year": 2023,
        "url": "https://www.bing.com/chat"
      },
      "learn": [
        {
          "title": "Microsoft Copilot",
          "url": "https://copilot.microsoft.com/",
          "type": "website",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Microsoft Copilot",
          "url": "https://copilot.microsoft.com/",
          "type": "website"
        }
      ],
      "tags": [
        "search",
        "microsoft",
        "product",
        "gpt-4"
      ],
      "era": "genai"
    },
    {
      "id": "gpt4-2023",
      "year": 2023,
      "month": 3,
      "type": "release",
      "title": "GPT-4 - Multimodal Intelligence",
      "what": "OpenAI released GPT-4: significantly more capable than GPT-3.5, can process images, scores 90th percentile on bar exam. Details kept secret (no paper on architecture/training).",
      "why_it_matters": "Showed LLMs approaching human-level performance on many tasks. Multimodal = text + images. Benchmark performance shook the AI field.",
      "impact": "Set new standard for LLM capability. Powers ChatGPT Plus. Every competitor benchmarks against GPT-4. Microsoft invested $10B.",
      "authors": [
        {
          "name": "OpenAI Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Company",
          "affiliation": "OpenAI",
          "background": "No detailed paper released, unlike previous models.",
          "contribution": "Built GPT-4"
        }
      ],
      "work": {
        "type": "report",
        "title": "GPT-4 Technical Report",
        "year": 2023,
        "url": "https://arxiv.org/abs/2303.08774"
      },
      "learn": [
        {
          "title": "GPT-4 API Guide",
          "url": "https://platform.openai.com/docs/guides/vision",
          "type": "documentation",
          "difficulty": "beginner"
        },
        {
          "title": "GPT-4 Capabilities Demo",
          "url": "https://www.youtube.com/watch?v=outcGtbnMuQ",
          "type": "video",
          "duration": "25 min",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "GPT-4 Technical Report (arXiv)",
          "url": "https://arxiv.org/abs/2303.08774",
          "type": "paper"
        },
        {
          "title": "GPT-4 - OpenAI",
          "url": "https://openai.com/gpt-4",
          "type": "website"
        }
      ],
      "tags": [
        "gpt",
        "openai",
        "multimodal",
        "milestone"
      ],
      "era": "genai"
    },
    {
      "id": "claude-2023",
      "year": 2023,
      "month": 3,
      "type": "product",
      "title": "Claude - Constitutional AI",
      "what": "Anthropic released Claude, trained with Constitutional AI (RLHF + AI feedback against principles). Founded by ex-OpenAI researchers focused on AI safety. Competes with GPT-4.",
      "why_it_matters": "Shows alternative approaches to alignment beyond RLHF. Longer context windows (100K+ tokens). Strong coding abilities. You might use Claude for RAG!",
      "impact": "Major competitor to OpenAI. Claude 2/3 showed continued improvement. Anthropic raised $7B+. Different safety philosophy.",
      "authors": [
        {
          "name": "Dario Amodei",
          "born": 1983,
          "died": null,
          "nationality": "American",
          "role": "CEO",
          "affiliation": "Anthropic",
          "background": "Former VP of Research at OpenAI. Left over safety concerns. Co-founded Anthropic with sister Daniela.",
          "contribution": "Founded Anthropic, leads Claude development"
        },
        {
          "name": "Daniela Amodei",
          "born": 1981,
          "died": null,
          "nationality": "American",
          "role": "President",
          "affiliation": "Anthropic",
          "background": "Former VP of Operations at OpenAI. Business leadership at Anthropic.",
          "contribution": "Co-founded Anthropic"
        }
      ],
      "work": {
        "type": "product",
        "title": "Claude",
        "year": 2023,
        "url": "https://claude.ai/"
      },
      "learn": [
        {
          "title": "Constitutional AI Paper",
          "url": "https://arxiv.org/abs/2212.08073",
          "type": "paper",
          "difficulty": "intermediate"
        },
        {
          "title": "Claude API Documentation",
          "url": "https://docs.anthropic.com/",
          "type": "documentation",
          "difficulty": "beginner"
        },
        {
          "title": "Anthropic Prompt Library",
          "url": "https://docs.anthropic.com/en/prompt-library/",
          "type": "guide",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Claude - Anthropic",
          "url": "https://www.anthropic.com/claude",
          "type": "website"
        },
        {
          "title": "Anthropic",
          "url": "https://www.anthropic.com/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "anthropic",
        "safety",
        "alignment",
        "product"
      ],
      "era": "genai"
    },
    {
      "id": "cursor-2023",
      "year": 2023,
      "month": 3,
      "type": "product",
      "title": "Cursor - AI-Native Code Editor",
      "what": "Cursor launched: VS Code fork with deep AI integration. Tab completion, inline editing, chat with codebase, @-mentions for context. Uses Claude/GPT-4 for code generation.",
      "why_it_matters": "First truly AI-native IDE. Changed how developers write code - conversation + context > autocomplete.",
      "impact": "Fastest growing dev tool. Forced GitHub Copilot to improve. Defined AI coding UX patterns.",
      "authors": [
        {
          "name": "Michael Truell",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Co-founder & CEO",
          "affiliation": "Anysphere (Cursor)",
          "background": "MIT. Co-founded Cursor. Focus on AI-assisted development.",
          "contribution": "Co-founded Cursor"
        },
        {
          "name": "Aman Sanger",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Co-founder",
          "affiliation": "Anysphere (Cursor)",
          "background": "MIT. Technical co-founder of Cursor.",
          "contribution": "Co-founded Cursor"
        }
      ],
      "work": {
        "type": "product",
        "title": "Cursor",
        "year": 2023,
        "url": "https://cursor.sh/"
      },
      "learn": [
        {
          "title": "Cursor Documentation",
          "url": "https://docs.cursor.sh/",
          "type": "documentation",
          "difficulty": "beginner"
        },
        {
          "title": "Cursor Features",
          "url": "https://cursor.sh/features",
          "type": "website",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Cursor",
          "url": "https://cursor.sh/",
          "type": "website"
        }
      ],
      "tags": [
        "coding",
        "ide",
        "product",
        "developer-tools"
      ],
      "era": "genai"
    },
    {
      "id": "bard-2023",
      "year": 2023,
      "month": 3,
      "type": "product",
      "title": "Google Bard - Google's ChatGPT Response",
      "what": "Google launched Bard: conversational AI powered by LaMDA/PaLM. Rushed response to ChatGPT. Initially rocky launch (hallucination in demo). Later replaced by Gemini.",
      "why_it_matters": "Showed even Google was caught off-guard by ChatGPT. Highlighted hallucination risks. Pushed Google to accelerate AI.",
      "impact": "Lost $100B market cap after demo error. Led to Google AI reorganization. Eventually evolved into Gemini.",
      "authors": [
        {
          "name": "Sundar Pichai",
          "born": 1972,
          "died": null,
          "nationality": "American (Indian-born)",
          "role": "CEO",
          "affiliation": "Google/Alphabet",
          "background": "Google CEO since 2015. Led Chrome, Android before becoming CEO.",
          "contribution": "Announced Bard, led Google's AI response"
        }
      ],
      "work": {
        "type": "product",
        "title": "Google Bard",
        "year": 2023,
        "url": "https://bard.google.com/"
      },
      "learn": [
        {
          "title": "Bard Announcement",
          "url": "https://blog.google/technology/ai/bard-google-ai-search-updates/",
          "type": "blog",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Google Bard",
          "url": "https://blog.google/technology/ai/bard-google-ai-search-updates/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "google",
        "product",
        "chatbot"
      ],
      "era": "genai"
    },
    {
      "id": "aider-2023",
      "year": 2023,
      "month": 5,
      "type": "tool",
      "title": "Aider - Terminal AI Pair Programmer",
      "what": "Aider released: command-line AI coding assistant. Chat with your codebase in terminal. Makes real git commits. Works with any LLM (GPT-4, Claude, local models).",
      "why_it_matters": "Showed AI coding doesn't need IDE. Git-native workflow. Popular with terminal power users.",
      "impact": "Top benchmarks for AI coding. Proved terminal-first approach works. Open source community.",
      "authors": [
        {
          "name": "Paul Gauthier",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Creator",
          "affiliation": "Independent",
          "background": "Experienced developer. Built aider as open source project.",
          "contribution": "Created and maintains Aider"
        }
      ],
      "work": {
        "type": "tool",
        "title": "Aider",
        "year": 2023,
        "url": "https://github.com/paul-gauthier/aider"
      },
      "learn": [
        {
          "title": "Aider Documentation",
          "url": "https://aider.chat/docs/",
          "type": "documentation",
          "difficulty": "beginner"
        },
        {
          "title": "Aider GitHub",
          "url": "https://github.com/paul-gauthier/aider",
          "type": "code",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Aider",
          "url": "https://aider.chat/",
          "type": "website"
        }
      ],
      "tags": [
        "coding",
        "terminal",
        "open-source",
        "developer-tools"
      ],
      "era": "genai"
    },
    {
      "id": "function-calling-2023",
      "year": 2023,
      "month": 6,
      "type": "feature",
      "title": "Function Calling / Tool Use",
      "what": "OpenAI added function calling to GPT models: describe functions in JSON schema, model decides when to call them and with what arguments. Enables structured outputs and API integration.",
      "why_it_matters": "Enables AGENTS. LLMs can now take actions, not just chat. RAG can be a 'tool' the model uses. Foundation for agentic workflows.",
      "impact": "Enabled AI agents. Every major model added tool use. LangChain agents, AutoGPT, and agentic patterns exploded.",
      "authors": [
        {
          "name": "OpenAI Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "OpenAI",
          "background": "Research team that introduced function calling to GPT models.",
          "contribution": "Designed function calling API"
        }
      ],
      "work": {
        "type": "feature",
        "title": "Function Calling",
        "year": 2023,
        "url": "https://platform.openai.com/docs/guides/function-calling"
      },
      "learn": [
        {
          "title": "Function Calling Guide - OpenAI",
          "url": "https://platform.openai.com/docs/guides/function-calling",
          "type": "documentation",
          "difficulty": "beginner"
        },
        {
          "title": "Building AI Agents - LangChain",
          "url": "https://python.langchain.com/docs/concepts/agents/",
          "type": "tutorial",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Function Calling - OpenAI",
          "url": "https://platform.openai.com/docs/guides/function-calling",
          "type": "documentation"
        },
        {
          "title": "AI Agents - LangChain",
          "url": "https://python.langchain.com/docs/concepts/agents/",
          "type": "documentation"
        }
      ],
      "tags": [
        "agents",
        "tools",
        "structured",
        "practical",
        "course-core"
      ],
      "era": "genai"
    },
    {
      "id": "llama2-2023",
      "year": 2023,
      "month": 7,
      "type": "release",
      "title": "LLaMA 2 - Open Source Goes Commercial",
      "what": "Meta released LLaMA 2 (7B, 13B, 70B) with commercial license. Trained on 2T tokens, 4K context. Chat-tuned versions available. Free for commercial use (under 700M users).",
      "why_it_matters": "First truly open commercial-grade LLM. Companies could build products without API costs. Local deployment possible.",
      "impact": "Explosion of LLaMA-2 based models. Enabled private AI deployments. Challenged closed model dominance.",
      "authors": [
        {
          "name": "Hugo Touvron",
          "born": null,
          "died": null,
          "nationality": "French",
          "role": "Research Scientist",
          "affiliation": "Meta FAIR",
          "background": "Lead author of LLaMA papers. Efficient transformer training research.",
          "contribution": "Lead researcher on LLaMA 1 and 2"
        }
      ],
      "work": {
        "type": "paper",
        "title": "LLaMA 2: Open Foundation and Fine-Tuned Chat Models",
        "year": 2023,
        "url": "https://arxiv.org/abs/2307.09288"
      },
      "learn": [
        {
          "title": "LLaMA 2 Paper",
          "url": "https://arxiv.org/abs/2307.09288",
          "type": "paper",
          "difficulty": "intermediate"
        },
        {
          "title": "LLaMA 2 - Meta",
          "url": "https://llama.meta.com/llama2/",
          "type": "website",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "LLaMA 2 - Meta",
          "url": "https://llama.meta.com/llama2/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "open-source",
        "meta",
        "commercial"
      ],
      "era": "genai"
    },
    {
      "id": "claude2-2023",
      "year": 2023,
      "month": 7,
      "type": "release",
      "title": "Claude 2 - 100K Context Window",
      "what": "Anthropic released Claude 2: 100K token context (could read entire books), improved coding, math, and reasoning. Available via API and claude.ai.",
      "why_it_matters": "100K context was revolutionary - process entire codebases or documents. Showed Anthropic keeping pace with GPT-4.",
      "impact": "Made Claude viable for enterprise document processing. Long context became expected feature.",
      "authors": [
        {
          "name": "Anthropic Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "Anthropic",
          "background": "Led by Dario Amodei. Team of ex-OpenAI researchers.",
          "contribution": "Developed Claude 2"
        }
      ],
      "work": {
        "type": "product",
        "title": "Claude 2",
        "year": 2023,
        "url": "https://www.anthropic.com/news/claude-2"
      },
      "learn": [
        {
          "title": "Claude 2 Announcement",
          "url": "https://www.anthropic.com/news/claude-2",
          "type": "blog",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Claude 2 - Anthropic",
          "url": "https://www.anthropic.com/news/claude-2",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "anthropic",
        "long-context",
        "product"
      ],
      "era": "genai"
    },
    {
      "id": "mistral-2023",
      "year": 2023,
      "month": 9,
      "type": "release",
      "title": "Mistral 7B - Efficient Open Source",
      "what": "Mistral AI (French startup) released Mistral 7B: only 7 billion parameters but matches much larger models. Uses grouped-query attention and sliding window attention for efficiency.",
      "why_it_matters": "Proves smaller models can be very capable. Can run locally on laptops. Mixtral (mixture of experts) followed. European AI leadership.",
      "impact": "Popular for local deployment. Mixtral 8x7B competitive with GPT-3.5. Mistral raised €600M at €6B valuation.",
      "authors": [
        {
          "name": "Arthur Mensch",
          "born": null,
          "died": null,
          "nationality": "French",
          "role": "CEO",
          "affiliation": "Mistral AI",
          "background": "Former DeepMind researcher. Co-founded Mistral AI.",
          "contribution": "Founded Mistral, leads research"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Mistral 7B",
        "year": 2023,
        "published_in": "arXiv",
        "url": "https://arxiv.org/abs/2310.06825"
      },
      "learn": [
        {
          "title": "Run Mistral with Ollama",
          "url": "https://ollama.com/library/mistral",
          "type": "tool",
          "difficulty": "beginner"
        },
        {
          "title": "Mistral API Documentation",
          "url": "https://docs.mistral.ai/",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Mistral Paper (arXiv)",
          "url": "https://arxiv.org/abs/2310.06825",
          "type": "paper"
        },
        {
          "title": "Mistral AI",
          "url": "https://mistral.ai/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "open-source",
        "efficient",
        "local"
      ],
      "era": "genai"
    },
    {
      "id": "dalle3-2023",
      "year": 2023,
      "month": 10,
      "type": "release",
      "title": "DALL-E 3 - Native ChatGPT Integration",
      "what": "OpenAI released DALL-E 3: dramatically improved prompt following, text rendering in images, integrated directly into ChatGPT. No more prompt engineering for images.",
      "why_it_matters": "Made AI image generation accessible via conversation. Actually follows complex prompts. Can render readable text.",
      "impact": "Integrated in ChatGPT Plus. Changed how people create images. Set new bar for prompt adherence.",
      "authors": [
        {
          "name": "Aditya Ramesh",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Research Scientist",
          "affiliation": "OpenAI",
          "background": "Lead researcher on DALL-E series. Pioneer in text-to-image generation.",
          "contribution": "Lead researcher on DALL-E 3"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Improving Image Generation with Better Captions",
        "year": 2023,
        "url": "https://cdn.openai.com/papers/dall-e-3.pdf"
      },
      "learn": [
        {
          "title": "DALL-E 3 in ChatGPT",
          "url": "https://openai.com/dall-e-3",
          "type": "website",
          "difficulty": "beginner"
        },
        {
          "title": "Image Generation API",
          "url": "https://platform.openai.com/docs/guides/images",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "DALL-E 3 - OpenAI",
          "url": "https://openai.com/dall-e-3",
          "type": "website"
        }
      ],
      "tags": [
        "image-generation",
        "openai",
        "multimodal",
        "product"
      ],
      "era": "genai"
    },
    {
      "id": "openai-devday-2023",
      "year": 2023,
      "month": 11,
      "type": "event",
      "title": "OpenAI DevDay - GPT-4 Turbo & GPTs",
      "what": "OpenAI's first developer conference. Announced GPT-4 Turbo (128K context, cheaper), custom GPTs (no-code AI agents), Assistants API (threads, retrieval, code interpreter), GPT Store.",
      "why_it_matters": "Democratized AI app building. Anyone could create custom GPTs without coding. Assistants API enabled RAG/agents easily.",
      "impact": "1M+ custom GPTs created. Assistants API became standard. Set template for AI developer platforms.",
      "authors": [
        {
          "name": "Sam Altman",
          "born": 1985,
          "died": null,
          "nationality": "American",
          "role": "CEO",
          "affiliation": "OpenAI",
          "background": "Former president of Y Combinator. CEO of OpenAI since 2019.",
          "contribution": "Presented DevDay, leads OpenAI"
        }
      ],
      "work": {
        "type": "event",
        "title": "OpenAI DevDay 2023",
        "year": 2023,
        "url": "https://openai.com/blog/new-models-and-developer-products-announced-at-devday"
      },
      "learn": [
        {
          "title": "DevDay Keynote",
          "url": "https://www.youtube.com/watch?v=U9mJuUkhUzk",
          "type": "video",
          "difficulty": "beginner"
        },
        {
          "title": "Assistants API Guide",
          "url": "https://platform.openai.com/docs/assistants/overview",
          "type": "documentation",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "DevDay Announcements",
          "url": "https://openai.com/blog/new-models-and-developer-products-announced-at-devday",
          "type": "website"
        }
      ],
      "tags": [
        "openai",
        "event",
        "gpt-4",
        "product",
        "developer"
      ],
      "era": "genai"
    },
    {
      "id": "grok-xai-2023",
      "year": 2023,
      "month": 11,
      "type": "product",
      "title": "Grok - xAI's Witty AI",
      "what": "Elon Musk's xAI released Grok, an AI assistant with real-time X (Twitter) access and a 'rebellious' personality. Later open-sourced Grok-1 weights (314B parameters).",
      "why_it_matters": "Showed new entrants could compete with established players. Real-time information access differentiated from ChatGPT.",
      "impact": "Added competition to LLM market. Open-sourcing Grok-1 advanced open models.",
      "authors": [
        {
          "name": "xAI",
          "role": "Company",
          "affiliation": "xAI",
          "contribution": "Developed Grok"
        }
      ],
      "work": {
        "type": "product",
        "title": "Grok",
        "year": 2023,
        "url": "https://x.ai/"
      },
      "learn": [
        {
          "title": "Grok Announcement",
          "url": "https://x.ai/blog/grok",
          "type": "blog",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "xAI Website",
          "url": "https://x.ai/",
          "type": "website"
        }
      ],
      "tags": [
        "xai",
        "elon-musk",
        "chatbot",
        "open-source"
      ],
      "era": "genai"
    },
    {
      "id": "gemini-1-2023",
      "year": 2023,
      "month": 12,
      "type": "release",
      "title": "Gemini 1.0 - Google's GPT-4 Competitor",
      "what": "Google DeepMind released Gemini 1.0: Ultra, Pro, Nano sizes. Native multimodal (trained on text, images, audio, video together). Claimed to beat GPT-4 on benchmarks.",
      "why_it_matters": "Google's answer to GPT-4. First native multimodal LLM. Showed Google AI was serious competition.",
      "impact": "Replaced Bard with Gemini. Gemini Pro free tier. Set up Google as frontier competitor.",
      "authors": [
        {
          "name": "Demis Hassabis",
          "born": 1976,
          "died": null,
          "nationality": "British",
          "role": "CEO",
          "affiliation": "Google DeepMind",
          "background": "Co-founder DeepMind (AlphaGo). Chess prodigy. Led merger of Google Brain and DeepMind.",
          "contribution": "Leads Google DeepMind, oversaw Gemini"
        },
        {
          "name": "Jeff Dean",
          "born": 1968,
          "died": null,
          "nationality": "American",
          "role": "Chief Scientist",
          "affiliation": "Google",
          "background": "Google Fellow, led AI research. Built MapReduce, TensorFlow. One of Google's most senior engineers.",
          "contribution": "Chief Scientist overseeing Gemini"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Gemini: A Family of Highly Capable Multimodal Models",
        "year": 2023,
        "url": "https://arxiv.org/abs/2312.11805"
      },
      "learn": [
        {
          "title": "Gemini Paper",
          "url": "https://arxiv.org/abs/2312.11805",
          "type": "paper",
          "difficulty": "advanced"
        },
        {
          "title": "Gemini API",
          "url": "https://ai.google.dev/",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Gemini - Google",
          "url": "https://deepmind.google/technologies/gemini/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "google",
        "multimodal",
        "frontier"
      ],
      "era": "genai"
    },
    {
      "id": "alphageometry-2024",
      "year": 2024,
      "month": 1,
      "type": "paper",
      "title": "AlphaGeometry - Math Olympiad Solving",
      "what": "DeepMind's AlphaGeometry solved International Mathematical Olympiad geometry problems at near-gold-medalist level. Combined neural language model with symbolic deduction engine.",
      "why_it_matters": "Mathematical reasoning is a key frontier for AI. AlphaGeometry demonstrated that AI could perform rigorous mathematical proofs, not just pattern match.",
      "impact": "Advanced the frontier of AI reasoning. Showed hybrid neuro-symbolic approaches could tackle mathematical proof.",
      "authors": [
        {
          "name": "DeepMind",
          "role": "Organization",
          "affiliation": "Google DeepMind",
          "contribution": "Developed AlphaGeometry"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Solving olympiad geometry without human demonstrations",
        "year": 2024,
        "url": "https://www.nature.com/articles/s41586-023-06747-5"
      },
      "learn": [
        {
          "title": "AlphaGeometry Explained - DeepMind",
          "url": "https://www.deepmind.com/blog/alphageometry-an-olympiad-level-ai-system-for-geometry",
          "type": "blog",
          "difficulty": "advanced"
        }
      ],
      "references": [
        {
          "title": "AlphaGeometry - Nature",
          "url": "https://www.nature.com/articles/s41586-023-06747-5",
          "type": "paper"
        }
      ],
      "tags": [
        "deepmind",
        "mathematics",
        "reasoning",
        "neuro-symbolic"
      ],
      "era": "genai"
    },
    {
      "id": "gemini-15-2024",
      "year": 2024,
      "month": 2,
      "type": "release",
      "title": "Gemini 1.5 Pro - 1M Token Context",
      "what": "Google released Gemini 1.5 Pro: 1 million token context window (experimental 10M). Can process entire codebases, hours of video, thousands of pages. Uses Mixture of Experts.",
      "why_it_matters": "1M context was 10x anything else. Could process entire books, full codebases. Changed what's possible with context.",
      "impact": "Raised expectations for context length. Enabled new use cases (full repo analysis). Pressured competitors.",
      "authors": [
        {
          "name": "Google DeepMind Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "Google DeepMind",
          "background": "Led by Demis Hassabis. Combined Google Brain and DeepMind expertise.",
          "contribution": "Developed Gemini 1.5"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
        "year": 2024,
        "url": "https://arxiv.org/abs/2403.05530"
      },
      "learn": [
        {
          "title": "Gemini 1.5 Paper",
          "url": "https://arxiv.org/abs/2403.05530",
          "type": "paper",
          "difficulty": "intermediate"
        },
        {
          "title": "Gemini API",
          "url": "https://ai.google.dev/",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Gemini 1.5 - Google",
          "url": "https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "google",
        "long-context",
        "multimodal"
      ],
      "era": "genai"
    },
    {
      "id": "claude3-opus-2024",
      "year": 2024,
      "month": 3,
      "type": "release",
      "title": "Claude 3 - Opus, Sonnet, Haiku",
      "what": "Anthropic released Claude 3 family: Opus (largest, smartest), Sonnet (balanced), Haiku (fastest, cheapest). Opus matched/beat GPT-4 on benchmarks. 200K context window.",
      "why_it_matters": "Showed Anthropic can compete at frontier. Different personality/style than GPT. Long context for document processing.",
      "impact": "Credible GPT-4 alternative. Many developers prefer Claude for coding. Enterprise adoption grew.",
      "authors": [
        {
          "name": "Dario Amodei",
          "born": 1983,
          "died": null,
          "nationality": "American",
          "role": "CEO",
          "affiliation": "Anthropic",
          "background": "Former VP of Research at OpenAI. Co-founded Anthropic with focus on AI safety.",
          "contribution": "Leads Anthropic, oversees Claude development"
        },
        {
          "name": "Anthropic Research Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "Anthropic",
          "background": "Team of researchers focused on interpretability and safety.",
          "contribution": "Developed Claude 3 family"
        }
      ],
      "work": {
        "type": "product",
        "title": "Claude 3",
        "year": 2024,
        "url": "https://www.anthropic.com/claude"
      },
      "learn": [
        {
          "title": "Claude 3 Model Card",
          "url": "https://www.anthropic.com/claude",
          "type": "documentation",
          "difficulty": "beginner"
        },
        {
          "title": "Claude API Quickstart",
          "url": "https://docs.anthropic.com/en/docs/quickstart",
          "type": "tutorial",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Claude 3 - Anthropic",
          "url": "https://www.anthropic.com/claude",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "anthropic",
        "product",
        "milestone"
      ],
      "era": "genai"
    },
    {
      "id": "nvidia-blackwell-2024",
      "year": 2024,
      "month": 3,
      "type": "technology",
      "title": "NVIDIA Blackwell B200 - Next-Gen AI Chip",
      "what": "NVIDIA announced Blackwell B200 GPU with 208 billion transistors. 4x faster AI training than H100. Designed for trillion-parameter models.",
      "why_it_matters": "Next generation of AI hardware enabling even larger models. Continued NVIDIA's AI dominance.",
      "impact": "Set roadmap for 2025+ AI infrastructure. Pre-orders from all major cloud providers.",
      "authors": [
        {
          "name": "NVIDIA",
          "role": "Company",
          "affiliation": "NVIDIA Corporation",
          "contribution": "Blackwell architecture"
        }
      ],
      "work": {
        "type": "technology",
        "title": "NVIDIA B200 GPU",
        "year": 2024,
        "url": "https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/"
      },
      "learn": [
        {
          "title": "Blackwell Explained",
          "url": "https://www.youtube.com/watch?v=kR4DheNjlcE",
          "type": "video",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Blackwell Architecture",
          "url": "https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/",
          "type": "product"
        }
      ],
      "tags": [
        "nvidia",
        "hardware",
        "gpu",
        "infrastructure"
      ],
      "era": "genai"
    },
    {
      "id": "llama3-2024",
      "year": 2024,
      "month": 4,
      "type": "release",
      "title": "LLaMA 3 - Open Source Maturity",
      "what": "Meta released LLaMA 3 (8B, 70B, later 405B): competitive with GPT-4 class models. Truly open (can use commercially). 128K context. Trained on 15T tokens.",
      "why_it_matters": "Open source now matches closed models. You can run GPT-4 level locally. Enables private, on-premise AI deployments.",
      "impact": "Shifted AI landscape toward open source. Companies can now avoid API lock-in. Community building on LLaMA 3.",
      "authors": [
        {
          "name": "Meta AI Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "Meta",
          "background": "Meta FAIR team.",
          "contribution": "Built LLaMA 3 series"
        }
      ],
      "work": {
        "type": "paper",
        "title": "LLaMA 3 Technical Report",
        "year": 2024,
        "url": "https://llama.meta.com/"
      },
      "learn": [
        {
          "title": "LLaMA 3 - Meta",
          "url": "https://llama.meta.com/",
          "type": "website",
          "difficulty": "beginner"
        },
        {
          "title": "Run LLaMA 3 with Ollama",
          "url": "https://ollama.com/library/llama3",
          "type": "tool",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "LLaMA 3 - Meta",
          "url": "https://llama.meta.com/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "open-source",
        "meta",
        "milestone"
      ],
      "era": "genai"
    },
    {
      "id": "gpt4o-2024",
      "year": 2024,
      "month": 5,
      "type": "release",
      "title": "GPT-4o - Omni Multimodal",
      "what": "OpenAI released GPT-4o ('o' for omni): natively multimodal (text, image, audio in one model). Real-time voice conversation. Faster and cheaper than GPT-4.",
      "why_it_matters": "Shows future of AI is multimodal. Voice interfaces become practical. Real-time AI assistants possible.",
      "impact": "Voice AI took off. Changed expectations for AI interfaces. Free tier made GPT-4 accessible to all.",
      "authors": [
        {
          "name": "OpenAI Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "OpenAI",
          "background": "Led by Sam Altman. Research team building GPT series.",
          "contribution": "Developed GPT-4o"
        },
        {
          "name": "Mira Murati",
          "born": 1988,
          "died": null,
          "nationality": "Albanian-American",
          "role": "CTO (former)",
          "affiliation": "OpenAI",
          "background": "Former CTO of OpenAI. Presented GPT-4o demo. Left in late 2024.",
          "contribution": "Led GPT-4o development and launch"
        }
      ],
      "work": {
        "type": "product",
        "title": "GPT-4o",
        "year": 2024,
        "url": "https://openai.com/index/hello-gpt-4o/"
      },
      "learn": [
        {
          "title": "GPT-4o Announcement",
          "url": "https://openai.com/index/hello-gpt-4o/",
          "type": "blog",
          "difficulty": "beginner"
        },
        {
          "title": "GPT-4o Voice Demo",
          "url": "https://www.youtube.com/watch?v=DQacCB9tDaw",
          "type": "video",
          "duration": "10 min",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "GPT-4o - OpenAI",
          "url": "https://openai.com/index/hello-gpt-4o/",
          "type": "website"
        }
      ],
      "tags": [
        "multimodal",
        "openai",
        "voice",
        "product"
      ],
      "era": "genai"
    },
    {
      "id": "alphafold3-2024",
      "year": 2024,
      "month": 5,
      "type": "paper",
      "title": "AlphaFold 3 - Molecular Interactions",
      "what": "AlphaFold 3 expanded beyond single proteins to predict structures of complexes including proteins, DNA, RNA, and small molecules. Critical for drug discovery and understanding molecular biology.",
      "why_it_matters": "Drugs work by binding to proteins. Understanding how molecules interact is essential for designing new medicines. AlphaFold 3 predicts these interactions.",
      "impact": "Major advance for pharmaceutical industry. Enables faster, cheaper drug discovery.",
      "authors": [
        {
          "name": "Google DeepMind",
          "role": "Organization",
          "affiliation": "Google DeepMind",
          "contribution": "Developed AlphaFold 3"
        }
      ],
      "work": {
        "type": "paper",
        "title": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
        "year": 2024,
        "url": "https://www.nature.com/articles/s41586-024-07487-w"
      },
      "learn": [
        {
          "title": "AlphaFold 3 - DeepMind Blog",
          "url": "https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/",
          "type": "blog",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "AlphaFold 3 - Nature",
          "url": "https://www.nature.com/articles/s41586-024-07487-w",
          "type": "paper"
        }
      ],
      "tags": [
        "deepmind",
        "biology",
        "drug-discovery",
        "molecular-modeling"
      ],
      "era": "genai"
    },
    {
      "id": "claude-35-sonnet-2024",
      "year": 2024,
      "month": 6,
      "type": "release",
      "title": "Claude 3.5 Sonnet - Best-in-Class Coding",
      "what": "Anthropic released Claude 3.5 Sonnet: faster than Claude 3 Opus, better at coding than any model at time of release. Introduced Artifacts (interactive code/doc generation) and later Computer Use (controlling desktop).",
      "why_it_matters": "Showed smaller, faster models can beat larger ones. Computer Use pioneered AI agents that interact with real software. Artifacts changed how people interact with AI output.",
      "impact": "Became preferred model for developers. Computer Use sparked agentic AI wave. Set new bar for instruction following.",
      "authors": [
        {
          "name": "Anthropic Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "Anthropic",
          "background": "Research team at Anthropic.",
          "contribution": "Developed Claude 3.5 Sonnet with best-in-class coding"
        }
      ],
      "work": {
        "type": "product",
        "title": "Claude 3.5 Sonnet",
        "year": 2024,
        "url": "https://www.anthropic.com/news/claude-3-5-sonnet"
      },
      "learn": [
        {
          "title": "Claude 3.5 Sonnet Announcement",
          "url": "https://www.anthropic.com/news/claude-3-5-sonnet",
          "type": "blog",
          "difficulty": "beginner"
        },
        {
          "title": "Computer Use Documentation",
          "url": "https://docs.anthropic.com/en/docs/computer-use",
          "type": "documentation",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Claude 3.5 Sonnet - Anthropic",
          "url": "https://www.anthropic.com/news/claude-3-5-sonnet",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "anthropic",
        "coding",
        "agents",
        "milestone"
      ],
      "era": "genai"
    },
    {
      "id": "llama31-405b-2024",
      "year": 2024,
      "month": 7,
      "type": "release",
      "title": "LLaMA 3.1 405B - Largest Open Source",
      "what": "Meta released LLaMA 3.1 with 405 billion parameters, the largest open-weights model ever. Competitive with GPT-4 and Claude on many benchmarks. Available for commercial use.",
      "why_it_matters": "First open model at GPT-4 scale. Democratized access to frontier AI capabilities. Changed competitive dynamics.",
      "impact": "Enabled companies to run GPT-4-class models internally. Pushed open source frontier forward.",
      "authors": [
        {
          "name": "Meta AI",
          "role": "Organization",
          "affiliation": "Meta",
          "contribution": "Developed LLaMA 3.1"
        }
      ],
      "work": {
        "type": "model",
        "title": "LLaMA 3.1",
        "year": 2024,
        "url": "https://ai.meta.com/blog/meta-llama-3-1/"
      },
      "learn": [
        {
          "title": "LLaMA 3.1 Announcement",
          "url": "https://ai.meta.com/blog/meta-llama-3-1/",
          "type": "blog",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "LLaMA 3.1 Paper",
          "url": "https://arxiv.org/abs/2407.21783",
          "type": "paper"
        }
      ],
      "tags": [
        "meta",
        "llama",
        "open-source",
        "frontier-model"
      ],
      "era": "genai"
    },
    {
      "id": "cline-2024",
      "year": 2024,
      "month": 8,
      "type": "tool",
      "title": "Cline - VS Code AI Agent",
      "what": "Cline (formerly Claude Dev): VS Code extension that acts as autonomous coding agent. Can browse files, run commands, use browser, make changes. Works with Claude, GPT, local models.",
      "why_it_matters": "Brought agentic coding to VS Code. Can handle complex multi-step tasks autonomously. Open source.",
      "impact": "Most downloaded AI coding extension. Proved agents can work in IDE. Template for agentic tools.",
      "authors": [
        {
          "name": "Saoud Rizwan",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Creator",
          "affiliation": "Independent",
          "background": "Independent developer. Created Claude Dev, rebranded to Cline.",
          "contribution": "Created Cline"
        }
      ],
      "work": {
        "type": "tool",
        "title": "Cline",
        "year": 2024,
        "url": "https://github.com/cline/cline"
      },
      "learn": [
        {
          "title": "Cline Documentation",
          "url": "https://github.com/cline/cline",
          "type": "code",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Cline GitHub",
          "url": "https://github.com/cline/cline",
          "type": "website"
        }
      ],
      "tags": [
        "coding",
        "agents",
        "vscode",
        "open-source",
        "developer-tools"
      ],
      "era": "genai"
    },
    {
      "id": "openai-o1-2024",
      "year": 2024,
      "month": 9,
      "type": "release",
      "title": "OpenAI o1 - Reasoning Models",
      "what": "OpenAI released o1 ('strawberry'): model trained with reinforcement learning to reason step-by-step before answering. Chain-of-thought internally. Excels at math, coding, logic.",
      "why_it_matters": "New paradigm: test-time compute (thinking longer = better answers). Shows path beyond just scaling training. System 2 thinking.",
      "impact": "PhD-level math performance. Changed how we think about model capabilities. Inspired similar reasoning approaches.",
      "authors": [
        {
          "name": "OpenAI Research Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "OpenAI",
          "background": "Research team focused on reasoning capabilities.",
          "contribution": "Developed o1 reasoning models"
        }
      ],
      "work": {
        "type": "product",
        "title": "OpenAI o1",
        "year": 2024,
        "url": "https://openai.com/o1/"
      },
      "learn": [
        {
          "title": "o1 Announcement",
          "url": "https://openai.com/o1/",
          "type": "blog",
          "difficulty": "beginner"
        },
        {
          "title": "How o1 Reasoning Works",
          "url": "https://www.youtube.com/watch?v=50W4YeQdnSg",
          "type": "video",
          "duration": "20 min",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "o1 - OpenAI",
          "url": "https://openai.com/o1/",
          "type": "website"
        }
      ],
      "tags": [
        "reasoning",
        "openai",
        "milestone",
        "new-paradigm"
      ],
      "era": "genai"
    },
    {
      "id": "qwen25-2024",
      "year": 2024,
      "month": 9,
      "type": "release",
      "title": "Qwen 2.5 - Alibaba's Frontier Model",
      "what": "Alibaba released Qwen 2.5 series with models up to 72B parameters. Strong performance especially in coding and math. Open weights with commercial license.",
      "why_it_matters": "Showed China's AI labs reaching parity with Western models. Strong open alternative to LLaMA.",
      "impact": "Diversified global AI ecosystem. Popular in Asia and for specific use cases.",
      "authors": [
        {
          "name": "Alibaba Cloud",
          "role": "Organization",
          "affiliation": "Alibaba",
          "contribution": "Developed Qwen series"
        }
      ],
      "work": {
        "type": "model",
        "title": "Qwen 2.5",
        "year": 2024,
        "url": "https://qwenlm.github.io/"
      },
      "learn": [
        {
          "title": "Qwen 2.5 Blog",
          "url": "https://qwenlm.github.io/blog/qwen2.5/",
          "type": "blog",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Qwen GitHub",
          "url": "https://github.com/QwenLM/Qwen2.5",
          "type": "code"
        }
      ],
      "tags": [
        "alibaba",
        "china",
        "open-source",
        "frontier-model"
      ],
      "era": "genai"
    },
    {
      "id": "notebooklm-2024",
      "year": 2024,
      "month": 9,
      "type": "product",
      "title": "NotebookLM - AI Podcast Generation",
      "what": "Google's NotebookLM added 'Audio Overview' feature that generates podcast-style conversations from uploaded documents. Two AI hosts discuss your content naturally.",
      "why_it_matters": "Novel AI UX paradigm - turning documents into engaging audio content. Showed creative applications of multimodal AI.",
      "impact": "Went viral. Demonstrated AI could create compelling content, not just answer questions.",
      "authors": [
        {
          "name": "Google Labs",
          "role": "Organization",
          "affiliation": "Google",
          "contribution": "Developed NotebookLM"
        }
      ],
      "work": {
        "type": "product",
        "title": "NotebookLM",
        "year": 2024,
        "url": "https://notebooklm.google.com/"
      },
      "learn": [
        {
          "title": "NotebookLM Tutorial",
          "url": "https://notebooklm.google.com/",
          "type": "product",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "NotebookLM Blog",
          "url": "https://blog.google/technology/ai/notebooklm-audio-overviews/",
          "type": "blog"
        }
      ],
      "tags": [
        "google",
        "audio",
        "podcast",
        "multimodal",
        "productivity"
      ],
      "era": "genai"
    },
    {
      "id": "anthropic-claude-computer-use-2024",
      "year": 2024,
      "month": 10,
      "type": "feature",
      "title": "Claude Computer Use - AI Controls Desktop",
      "what": "Anthropic released Claude's ability to control computers - move mouse, click, type, and navigate software. First major LLM with direct computer control API.",
      "why_it_matters": "Major step toward AI agents. Enables automation of any computer task. Opens new application categories.",
      "impact": "Sparked discussion about AI safety and job automation. Enabled new agentic applications.",
      "authors": [
        {
          "name": "Anthropic",
          "role": "Company",
          "affiliation": "Anthropic",
          "contribution": "Developed computer use capability"
        }
      ],
      "work": {
        "type": "feature",
        "title": "Computer Use",
        "year": 2024,
        "url": "https://www.anthropic.com/news/3-5-models-and-computer-use"
      },
      "learn": [
        {
          "title": "Computer Use Demo",
          "url": "https://www.anthropic.com/news/3-5-models-and-computer-use",
          "type": "blog",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Anthropic Blog",
          "url": "https://www.anthropic.com/news/3-5-models-and-computer-use",
          "type": "blog"
        }
      ],
      "tags": [
        "anthropic",
        "claude",
        "agents",
        "automation",
        "computer-use"
      ],
      "era": "genai"
    },
    {
      "id": "mcp-2024",
      "year": 2024,
      "month": 11,
      "type": "standard",
      "title": "Model Context Protocol (MCP)",
      "what": "Anthropic released MCP: open standard for connecting AI models to data sources and tools. Defines how models request context, call tools, and interact with external systems.",
      "why_it_matters": "Standardizes how RAG and tool use work. Instead of custom integrations, use MCP. Could become the 'HTTP of AI context'.",
      "impact": "Early days but growing adoption. IDEs (Cursor), browsers, and applications implementing MCP support.",
      "authors": [
        {
          "name": "David Soria",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Engineer",
          "affiliation": "Anthropic",
          "background": "Lead engineer on MCP protocol design.",
          "contribution": "Designed and implemented MCP"
        },
        {
          "name": "Anthropic Tools Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "Anthropic",
          "background": "Team focused on tool use and integration.",
          "contribution": "Built MCP specification and SDKs"
        }
      ],
      "work": {
        "type": "standard",
        "title": "Model Context Protocol",
        "year": 2024,
        "url": "https://modelcontextprotocol.io/"
      },
      "learn": [
        {
          "title": "MCP Documentation",
          "url": "https://modelcontextprotocol.io/docs",
          "type": "documentation",
          "difficulty": "intermediate"
        },
        {
          "title": "Building MCP Servers",
          "url": "https://modelcontextprotocol.io/docs/first-server",
          "type": "tutorial",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "MCP - Official Site",
          "url": "https://modelcontextprotocol.io/",
          "type": "website"
        }
      ],
      "tags": [
        "standard",
        "protocol",
        "tools",
        "anthropic",
        "emerging"
      ],
      "era": "genai"
    },
    {
      "id": "windsurf-2024",
      "year": 2024,
      "month": 11,
      "type": "product",
      "title": "Windsurf - Codeium's AI IDE",
      "what": "Codeium launched Windsurf: AI-native IDE with 'Flows' (autonomous multi-file editing), Cascade (AI agent for coding), and deep codebase understanding. Free tier available.",
      "why_it_matters": "Raised bar for AI coding tools. Autonomous agents that can implement features across files. Strong Cursor competitor.",
      "impact": "Pushed AI coding forward. Free tier democratized access. Showed agentic coding is viable.",
      "authors": [
        {
          "name": "Varun Mohan",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Co-founder & CEO",
          "affiliation": "Codeium",
          "background": "Stanford. Previously at Facebook AI. Built Codeium as free Copilot alternative.",
          "contribution": "Founded Codeium, launched Windsurf"
        }
      ],
      "work": {
        "type": "product",
        "title": "Windsurf",
        "year": 2024,
        "url": "https://codeium.com/windsurf"
      },
      "learn": [
        {
          "title": "Windsurf Documentation",
          "url": "https://codeium.com/windsurf",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Windsurf - Codeium",
          "url": "https://codeium.com/windsurf",
          "type": "website"
        }
      ],
      "tags": [
        "coding",
        "ide",
        "product",
        "agents",
        "developer-tools"
      ],
      "era": "genai"
    },
    {
      "id": "gemini-2-2024",
      "year": 2024,
      "month": 12,
      "type": "release",
      "title": "Gemini 2.0 - Google's Agentic Push",
      "what": "Google released Gemini 2.0: native multimodal (text, image, audio, video), 1M token context, built for agentic AI. Introduced Project Astra (real-time AI assistant) and Mariner (browser agent).",
      "why_it_matters": "Google's bet on agentic AI. Native multimodality vs bolted-on. Shows direction: AI that acts, not just responds.",
      "impact": "Competitive with GPT-4o. Free tier made frontier AI accessible. Pushed industry toward agents.",
      "authors": [
        {
          "name": "Demis Hassabis",
          "born": 1976,
          "died": null,
          "nationality": "British",
          "role": "CEO",
          "affiliation": "Google DeepMind",
          "background": "Co-founder DeepMind, Nobel Prize winner 2024 for AlphaFold.",
          "contribution": "Leads Google DeepMind, oversees Gemini"
        },
        {
          "name": "Oriol Vinyals",
          "born": 1981,
          "died": null,
          "nationality": "Spanish",
          "role": "VP of Research",
          "affiliation": "Google DeepMind",
          "background": "Key researcher on sequence-to-sequence, StarCraft AI, Gemini.",
          "contribution": "Led Gemini research"
        }
      ],
      "work": {
        "type": "product",
        "title": "Gemini 2.0",
        "year": 2024,
        "url": "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/"
      },
      "learn": [
        {
          "title": "Gemini 2.0 Announcement",
          "url": "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/",
          "type": "blog",
          "difficulty": "beginner"
        },
        {
          "title": "Gemini API",
          "url": "https://ai.google.dev/",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "Gemini 2.0 - Google",
          "url": "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "google",
        "multimodal",
        "agents"
      ],
      "era": "genai"
    },
    {
      "id": "sora-2024",
      "year": 2024,
      "month": 12,
      "type": "release",
      "title": "Sora - Text-to-Video Generation",
      "what": "OpenAI released Sora publicly: generates realistic videos up to 1 minute from text prompts. Uses diffusion transformer architecture. Understands physics, motion, and cinematography.",
      "why_it_matters": "Video was the last major modality to crack. Implications for film, advertising, education. Raises deepfake concerns to new level.",
      "impact": "Launched video generation race. Hollywood took notice. Questions about creative jobs intensified.",
      "authors": [
        {
          "name": "OpenAI Sora Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "OpenAI",
          "background": "Research team working on video generation.",
          "contribution": "Developed Sora video model"
        },
        {
          "name": "Tim Brooks",
          "born": null,
          "died": null,
          "nationality": "American",
          "role": "Research Scientist",
          "affiliation": "OpenAI",
          "background": "Previously at Google. Computer vision and video research.",
          "contribution": "Co-lead of Sora project"
        }
      ],
      "work": {
        "type": "product",
        "title": "Sora",
        "year": 2024,
        "url": "https://openai.com/sora"
      },
      "learn": [
        {
          "title": "Sora - OpenAI",
          "url": "https://openai.com/sora",
          "type": "website",
          "difficulty": "beginner"
        },
        {
          "title": "Sora Technical Report",
          "url": "https://openai.com/research/video-generation-models-as-world-simulators",
          "type": "paper",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Sora - OpenAI",
          "url": "https://openai.com/sora",
          "type": "website"
        }
      ],
      "tags": [
        "video",
        "generative",
        "openai",
        "multimodal",
        "milestone"
      ],
      "era": "genai"
    },
    {
      "id": "ai-agents-2025",
      "year": 2025,
      "month": null,
      "type": "trend",
      "title": "Year of AI Agents",
      "what": "2025 saw explosion of AI agents: systems that plan, use tools, browse web, write code, and take actions autonomously. Computer Use, Devin, OpenAI Operator, and many others launched.",
      "why_it_matters": "Shift from AI as 'assistant you ask' to AI as 'worker you delegate to'. Changes how developers build with AI. Raises stakes for safety and alignment.",
      "impact": "New category of AI products. Changed software development practices. Amplified both excitement and concerns.",
      "authors": [
        {
          "name": "Industry-wide",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Trend",
          "affiliation": "Various",
          "background": "Convergence of work from OpenAI, Anthropic, Google, and startups.",
          "contribution": "Collective push toward agentic AI"
        }
      ],
      "work": {
        "type": "trend",
        "title": "AI Agents",
        "year": 2025,
        "url": "https://www.anthropic.com/research/building-effective-agents"
      },
      "learn": [
        {
          "title": "Building Effective Agents - Anthropic",
          "url": "https://www.anthropic.com/research/building-effective-agents",
          "type": "article",
          "difficulty": "intermediate"
        },
        {
          "title": "AI Agents - LangChain",
          "url": "https://python.langchain.com/docs/concepts/agents/",
          "type": "documentation",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Anthropic Agents Guide",
          "url": "https://www.anthropic.com/research/building-effective-agents",
          "type": "article"
        }
      ],
      "tags": [
        "agents",
        "tools",
        "automation",
        "trend",
        "course-core"
      ],
      "era": "genai"
    },
    {
      "id": "deepseek-r1-2025",
      "year": 2025,
      "month": 1,
      "type": "release",
      "title": "DeepSeek R1 - China Catches Up",
      "what": "Chinese company DeepSeek released R1: reasoning model matching o1 performance, trained at fraction of cost (reportedly $5.6M vs billions). Open weights. Showed efficient training methods.",
      "why_it_matters": "Proved you don't need billions to train frontier models. Open weights enable inspection. Shifted AI geopolitics.",
      "impact": "Stock market impact (Nvidia dropped). Showed compute efficiency gains. Changed assumptions about AI costs.",
      "authors": [
        {
          "name": "DeepSeek AI Team",
          "born": null,
          "died": null,
          "nationality": "Chinese",
          "role": "Team",
          "affiliation": "DeepSeek",
          "background": "Chinese AI lab founded by High-Flyer hedge fund. Focus on reasoning models.",
          "contribution": "Developed DeepSeek R1"
        },
        {
          "name": "Liang Wenfeng",
          "born": null,
          "died": null,
          "nationality": "Chinese",
          "role": "Founder",
          "affiliation": "DeepSeek / High-Flyer",
          "background": "Founder of High-Flyer quant hedge fund. Founded DeepSeek AI lab.",
          "contribution": "Founded DeepSeek"
        }
      ],
      "work": {
        "type": "paper",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "year": 2025,
        "url": "https://github.com/deepseek-ai/DeepSeek-R1"
      },
      "learn": [
        {
          "title": "DeepSeek R1 Paper",
          "url": "https://arxiv.org/abs/2501.12948",
          "type": "paper",
          "difficulty": "intermediate"
        },
        {
          "title": "Run DeepSeek with Ollama",
          "url": "https://ollama.com/library/deepseek-r1",
          "type": "tool",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "DeepSeek GitHub",
          "url": "https://github.com/deepseek-ai/DeepSeek-R1",
          "type": "code"
        },
        {
          "title": "DeepSeek",
          "url": "https://www.deepseek.com/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "reasoning",
        "open-source",
        "china",
        "milestone"
      ],
      "era": "genai"
    },
    {
      "id": "eu-ai-act-2025",
      "year": 2025,
      "month": 2,
      "type": "regulation",
      "title": "EU AI Act Enforcement Begins",
      "what": "First major provisions of EU AI Act took effect. Bans certain AI uses (social scoring, emotion recognition in workplaces). Requires transparency for high-risk AI. Sets global regulatory precedent.",
      "why_it_matters": "First comprehensive AI law. 'Brussels Effect' means global companies must comply. Shapes how AI is developed and deployed worldwide.",
      "impact": "Companies scrambled to comply. Set template for other countries. Debate: helps safety vs stifles innovation.",
      "authors": [
        {
          "name": "European Commission",
          "born": null,
          "died": null,
          "nationality": "European Union",
          "role": "Regulatory Body",
          "affiliation": "EU",
          "background": "Executive branch of the EU responsible for proposing and implementing legislation.",
          "contribution": "Drafted and enacted AI Act"
        },
        {
          "name": "Margrethe Vestager",
          "born": 1968,
          "died": null,
          "nationality": "Danish",
          "role": "Executive VP",
          "affiliation": "European Commission",
          "background": "EVP for A Europe Fit for the Digital Age. Led tech regulation efforts.",
          "contribution": "Championed AI Act"
        }
      ],
      "work": {
        "type": "regulation",
        "title": "EU AI Act",
        "year": 2024,
        "url": "https://artificialintelligenceact.eu/"
      },
      "learn": [
        {
          "title": "EU AI Act Explained",
          "url": "https://artificialintelligenceact.eu/",
          "type": "website",
          "difficulty": "beginner"
        },
        {
          "title": "AI Act Compliance Guide",
          "url": "https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai",
          "type": "documentation",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "EU AI Act - Official",
          "url": "https://artificialintelligenceact.eu/",
          "type": "website"
        }
      ],
      "tags": [
        "regulation",
        "policy",
        "europe",
        "safety",
        "milestone"
      ],
      "era": "genai"
    },
    {
      "id": "claude-4-opus-2025",
      "year": 2025,
      "month": 3,
      "type": "release",
      "title": "Claude Opus 4 - Extended Thinking",
      "what": "Anthropic released Claude Opus 4 and Sonnet 4: new frontier models with extended thinking (visible chain-of-thought reasoning). Opus 4 showed strong agentic capabilities for multi-step tasks.",
      "why_it_matters": "Visible reasoning increases trust and debuggability. Strong agentic performance enables autonomous coding, research tasks.",
      "impact": "Raised bar for reasoning transparency. Anthropic solidified position as OpenAI alternative.",
      "authors": [
        {
          "name": "Dario Amodei",
          "born": 1983,
          "died": null,
          "nationality": "American",
          "role": "CEO",
          "affiliation": "Anthropic",
          "background": "Co-founder and CEO of Anthropic.",
          "contribution": "Leads Anthropic"
        },
        {
          "name": "Anthropic Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "Anthropic",
          "background": "Research team building Claude.",
          "contribution": "Developed Claude Opus 4 with extended thinking"
        }
      ],
      "work": {
        "type": "product",
        "title": "Claude 4",
        "year": 2025,
        "url": "https://www.anthropic.com/claude"
      },
      "learn": [
        {
          "title": "Claude 4 - Anthropic",
          "url": "https://www.anthropic.com/claude",
          "type": "website",
          "difficulty": "beginner"
        },
        {
          "title": "Extended Thinking Guide",
          "url": "https://docs.anthropic.com/",
          "type": "documentation",
          "difficulty": "intermediate"
        }
      ],
      "references": [
        {
          "title": "Claude - Anthropic",
          "url": "https://www.anthropic.com/claude",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "anthropic",
        "reasoning",
        "agents",
        "milestone"
      ],
      "era": "genai"
    },
    {
      "id": "gpt5-2025",
      "year": 2025,
      "month": 5,
      "type": "release",
      "title": "GPT-5 / Orion",
      "what": "OpenAI released GPT-5 (internally 'Orion'): significant jump in reasoning, reliability, and multimodal capabilities. Native integration with reasoning (o-series style) and improved instruction following.",
      "why_it_matters": "Closed gap between base models and reasoning models. More reliable for production use. Raised ceiling for what AI can do.",
      "impact": "Set new frontier benchmark. Renewed OpenAI's lead. Enterprise adoption accelerated.",
      "authors": [
        {
          "name": "Sam Altman",
          "born": 1985,
          "died": null,
          "nationality": "American",
          "role": "CEO",
          "affiliation": "OpenAI",
          "background": "CEO of OpenAI since 2019.",
          "contribution": "Leads OpenAI"
        },
        {
          "name": "OpenAI Research Team",
          "born": null,
          "died": null,
          "nationality": null,
          "role": "Team",
          "affiliation": "OpenAI",
          "background": "Research team building GPT series.",
          "contribution": "Developed GPT-5"
        }
      ],
      "work": {
        "type": "product",
        "title": "GPT-5",
        "year": 2025,
        "url": "https://openai.com/"
      },
      "learn": [
        {
          "title": "OpenAI Models",
          "url": "https://platform.openai.com/docs/models",
          "type": "documentation",
          "difficulty": "beginner"
        }
      ],
      "references": [
        {
          "title": "OpenAI",
          "url": "https://openai.com/",
          "type": "website"
        }
      ],
      "tags": [
        "llm",
        "openai",
        "frontier",
        "milestone"
      ],
      "era": "genai"
    }
  ],
  "eras": [
    {
      "id": "prerequisites",
      "name": "Mathematical Prerequisites",
      "start_year": 1843,
      "end_year": 1939,
      "color": "#a855f7",
      "description": "Mathematical foundations: logic, computability, information theory"
    },
    {
      "id": "foundations",
      "name": "Foundations",
      "start_year": 1940,
      "end_year": 1956,
      "color": "#6366f1",
      "description": "Birth of computing, first neuron models, AI term coined"
    },
    {
      "id": "early-ai",
      "name": "Early AI",
      "start_year": 1956,
      "end_year": 1974,
      "color": "#8b5cf6",
      "description": "First AI programs, perceptrons, LISP, high optimism"
    },
    {
      "id": "first-winter",
      "name": "First AI Winter",
      "start_year": 1974,
      "end_year": 1980,
      "color": "#64748b",
      "description": "Funding cuts, criticism of perceptrons"
    },
    {
      "id": "expert-systems",
      "name": "Expert Systems Era",
      "start_year": 1980,
      "end_year": 1987,
      "color": "#f59e0b",
      "description": "Rule-based systems, knowledge engineering"
    },
    {
      "id": "second-winter",
      "name": "Second AI Winter",
      "start_year": 1987,
      "end_year": 1993,
      "color": "#64748b",
      "description": "Expert systems fail to scale"
    },
    {
      "id": "ml-rise",
      "name": "Machine Learning Rise",
      "start_year": 1993,
      "end_year": 2011,
      "color": "#10b981",
      "description": "Statistical ML, SVMs, early deep learning"
    },
    {
      "id": "deep-learning",
      "name": "Deep Learning Revolution",
      "start_year": 2012,
      "end_year": 2017,
      "color": "#06b6d4",
      "description": "AlexNet, GPU training, breakthrough results"
    },
    {
      "id": "transformers",
      "name": "Transformer Era",
      "start_year": 2017,
      "end_year": 2022,
      "color": "#ec4899",
      "description": "Attention mechanism transforms NLP"
    },
    {
      "id": "genai",
      "name": "Generative AI Era",
      "start_year": 2022,
      "end_year": 2025,
      "color": "#ef4444",
      "description": "ChatGPT, widespread adoption, multimodal AI, agentic systems"
    }
  ],
  "connections": [
    {
      "from": "boolean-algebra-1847",
      "to": "mcculloch-pitts-neuron-1943",
      "relationship": "enabled",
      "description": "Boolean logic provided mathematical foundation for modeling neurons as logic gates"
    },
    {
      "from": "predicate-logic-1879",
      "to": "gps-1959",
      "relationship": "enabled",
      "description": "Formal logic enabled symbolic AI and problem solving as logical deduction"
    },
    {
      "from": "godel-incompleteness-1931",
      "to": "turing-machine-1936",
      "relationship": "inspired",
      "description": "Gödel's work on undecidability inspired Turing's formalization of computation"
    },
    {
      "from": "turing-machine-1936",
      "to": "von-neumann-architecture-1945",
      "relationship": "enabled",
      "description": "Turing's theoretical machine led to practical stored-program computer design"
    },
    {
      "from": "mcculloch-pitts-neuron-1943",
      "to": "perceptron-1957",
      "relationship": "evolved-into",
      "description": "McCulloch-Pitts model evolved into Rosenblatt's trainable perceptron"
    },
    {
      "from": "hebbian-learning-1949",
      "to": "perceptron-1957",
      "relationship": "inspired",
      "description": "Hebb's learning rule inspired perceptron weight adjustment"
    },
    {
      "from": "turing-test-1950",
      "to": "dartmouth-conference-1956",
      "relationship": "inspired",
      "description": "Turing's vision of machine intelligence inspired the AI research program"
    },
    {
      "from": "turing-test-1950",
      "to": "eliza-1966",
      "relationship": "inspired",
      "description": "Turing Test directly inspired ELIZA as conversational AI"
    },
    {
      "from": "dartmouth-conference-1956",
      "to": "lisp-1958",
      "relationship": "led-to",
      "description": "McCarthy created LISP to implement AI ideas from Dartmouth"
    },
    {
      "from": "dartmouth-conference-1956",
      "to": "gps-1959",
      "relationship": "led-to",
      "description": "Dartmouth participants Newell and Simon created GPS"
    },
    {
      "from": "perceptron-1957",
      "to": "perceptrons-book-1969",
      "relationship": "criticized-by",
      "description": "Minsky & Papert's book criticized perceptron limitations"
    },
    {
      "from": "perceptrons-book-1969",
      "to": "lighthill-report-1973",
      "relationship": "led-to",
      "description": "Perceptron criticism contributed to broader AI skepticism"
    },
    {
      "from": "perceptrons-book-1969",
      "to": "backpropagation-1986",
      "relationship": "solved-by",
      "description": "Backpropagation solved the multi-layer training problem"
    },
    {
      "from": "shannon-information-theory-1948",
      "to": "backpropagation-1986",
      "relationship": "enabled",
      "description": "Cross-entropy loss comes from information theory"
    },
    {
      "from": "backpropagation-1986",
      "to": "cnn-lenet-1989",
      "relationship": "enabled",
      "description": "Backprop enabled training of convolutional networks"
    },
    {
      "from": "backpropagation-1986",
      "to": "lstm-1997",
      "relationship": "enabled",
      "description": "Backprop through time enabled RNN/LSTM training"
    },
    {
      "from": "cnn-lenet-1989",
      "to": "imagenet-2009",
      "relationship": "motivated",
      "description": "CNN progress motivated creation of larger benchmarks"
    },
    {
      "from": "cnn-lenet-1989",
      "to": "alexnet-2012",
      "relationship": "evolved-into",
      "description": "AlexNet scaled up and improved LeCun's CNN architecture"
    },
    {
      "from": "imagenet-2009",
      "to": "alexnet-2012",
      "relationship": "enabled",
      "description": "ImageNet dataset enabled training and benchmarking AlexNet"
    },
    {
      "from": "deep-belief-nets-2006",
      "to": "alexnet-2012",
      "relationship": "inspired",
      "description": "Hinton's work reignited deep learning before AlexNet"
    },
    {
      "from": "alexnet-2012",
      "to": "word2vec-2013",
      "relationship": "inspired",
      "description": "AlexNet success inspired neural approaches to NLP"
    },
    {
      "from": "word2vec-2013",
      "to": "seq2seq-attention-2014",
      "relationship": "enabled",
      "description": "Word embeddings enabled neural machine translation"
    },
    {
      "from": "alexnet-2012",
      "to": "resnet-2015",
      "relationship": "evolved-into",
      "description": "ResNet built on AlexNet with skip connections"
    },
    {
      "from": "seq2seq-attention-2014",
      "to": "transformer-2017",
      "relationship": "evolved-into",
      "description": "Attention mechanism became core of Transformer"
    },
    {
      "from": "lstm-1997",
      "to": "transformer-2017",
      "relationship": "replaced-by",
      "description": "Transformers replaced LSTMs for most sequence tasks"
    },
    {
      "from": "transformer-2017",
      "to": "bert-2018",
      "relationship": "enabled",
      "description": "BERT uses Transformer encoder architecture"
    },
    {
      "from": "transformer-2017",
      "to": "gpt1-2018",
      "relationship": "enabled",
      "description": "GPT uses Transformer decoder architecture"
    },
    {
      "from": "gpt1-2018",
      "to": "gpt2-2019",
      "relationship": "evolved-into",
      "description": "GPT-2 scaled up GPT-1"
    },
    {
      "from": "gpt2-2019",
      "to": "gpt3-2020",
      "relationship": "evolved-into",
      "description": "GPT-3 massively scaled GPT-2"
    },
    {
      "from": "bert-2018",
      "to": "rag-original-2020",
      "relationship": "enabled",
      "description": "BERT-style encoders used for retrieval in RAG"
    },
    {
      "from": "word2vec-2013",
      "to": "rag-original-2020",
      "relationship": "enabled",
      "description": "Embedding similarity enables document retrieval"
    },
    {
      "from": "gpt3-2020",
      "to": "instructgpt-2022",
      "relationship": "evolved-into",
      "description": "InstructGPT fine-tuned GPT-3 with RLHF"
    },
    {
      "from": "instructgpt-2022",
      "to": "chatgpt-2022",
      "relationship": "evolved-into",
      "description": "ChatGPT applied InstructGPT techniques"
    },
    {
      "from": "gpt3-2020",
      "to": "github-copilot-2021",
      "relationship": "enabled",
      "description": "Copilot built on Codex (GPT-3 fine-tuned on code)"
    },
    {
      "from": "gan-2014",
      "to": "dalle-2021",
      "relationship": "inspired",
      "description": "GANs pioneered image generation, inspiring DALL·E"
    },
    {
      "from": "dalle-2021",
      "to": "stable-diffusion-2022",
      "relationship": "inspired",
      "description": "DALL·E sparked open-source image generation efforts"
    },
    {
      "from": "chatgpt-2022",
      "to": "gpt4-2023",
      "relationship": "led-to",
      "description": "ChatGPT success led to GPT-4 investment"
    },
    {
      "from": "chatgpt-2022",
      "to": "claude-2023",
      "relationship": "competed-with",
      "description": "Claude launched to compete with ChatGPT"
    },
    {
      "from": "llama-2023",
      "to": "mistral-2023",
      "relationship": "inspired",
      "description": "LLaMA open-source approach inspired Mistral"
    },
    {
      "from": "llama-2023",
      "to": "llama3-2024",
      "relationship": "evolved-into",
      "description": "LLaMA 3 continued the series with improvements"
    },
    {
      "from": "rag-original-2020",
      "to": "langchain-rise-2023",
      "relationship": "enabled",
      "description": "RAG pattern was core to LangChain's design"
    },
    {
      "from": "rag-original-2020",
      "to": "vector-databases-2023",
      "relationship": "drove-demand",
      "description": "RAG created demand for vector storage solutions"
    },
    {
      "from": "gpt4-2023",
      "to": "function-calling-2023",
      "relationship": "enabled",
      "description": "GPT-4 added structured function calling"
    },
    {
      "from": "function-calling-2023",
      "to": "mcp-2024",
      "relationship": "standardized-by",
      "description": "MCP standardizes tool/context patterns"
    },
    {
      "from": "gpt4-2023",
      "to": "openai-o1-2024",
      "relationship": "evolved-into",
      "description": "o1 built on GPT-4 with reasoning training"
    },
    {
      "from": "openai-o1-2024",
      "to": "deepseek-r1-2025",
      "relationship": "inspired",
      "description": "DeepSeek R1 reproduced o1's reasoning approach"
    },
    {
      "from": "gpt4-2023",
      "to": "gpt4o-2024",
      "relationship": "evolved-into",
      "description": "GPT-4o added native multimodal capabilities"
    },
    {
      "from": "claude-2023",
      "to": "claude3-opus-2024",
      "relationship": "evolved-into",
      "description": "Claude 3 series improved on Claude 2"
    },
    {
      "from": "claude3-opus-2024",
      "to": "claude-35-sonnet-2024",
      "relationship": "evolved-into",
      "description": "Claude 3.5 Sonnet surpassed Opus in many tasks"
    },
    {
      "from": "claude-35-sonnet-2024",
      "to": "claude-4-opus-2025",
      "relationship": "evolved-into",
      "description": "Claude 4 added extended thinking and stronger agentic capabilities"
    },
    {
      "from": "function-calling-2023",
      "to": "ai-agents-2025",
      "relationship": "enabled",
      "description": "Tool use capabilities enabled agentic AI systems"
    },
    {
      "from": "mcp-2024",
      "to": "ai-agents-2025",
      "relationship": "enabled",
      "description": "MCP standardized how agents interact with tools and context"
    },
    {
      "from": "openai-o1-2024",
      "to": "gpt5-2025",
      "relationship": "merged-into",
      "description": "GPT-5 integrated reasoning capabilities from o1 series"
    },
    {
      "from": "dalle-2021",
      "to": "sora-2024",
      "relationship": "led-to",
      "description": "DALL-E's generative approach extended to video generation"
    },
    {
      "from": "gpt4o-2024",
      "to": "gemini-2-2024",
      "relationship": "competed-with",
      "description": "Google responded to GPT-4o with native multimodal Gemini 2.0"
    },
    {
      "from": "deepmind-founded-2010",
      "to": "deepmind-acquired-2014",
      "relationship": "led-to",
      "description": "DeepMind's success led to Google acquisition"
    },
    {
      "from": "deepmind-acquired-2014",
      "to": "alphago-2016",
      "relationship": "enabled",
      "description": "Google resources enabled AlphaGo development"
    },
    {
      "from": "alphago-2016",
      "to": "alphazero-2017",
      "relationship": "evolved-into",
      "description": "AlphaGo principles generalized to AlphaZero"
    },
    {
      "from": "alphazero-2017",
      "to": "alphafold1-2018",
      "relationship": "inspired",
      "description": "Deep RL techniques from AlphaZero applied to biology"
    },
    {
      "from": "alphafold1-2018",
      "to": "alphafold2-2020",
      "relationship": "evolved-into",
      "description": "AlphaFold 2 dramatically improved on original"
    },
    {
      "from": "alphafold2-2020",
      "to": "alphafold-db-2021",
      "relationship": "led-to",
      "description": "AlphaFold 2 success led to public database release"
    },
    {
      "from": "alphafold2-2020",
      "to": "alphafold3-2024",
      "relationship": "evolved-into",
      "description": "AlphaFold 3 expanded to molecular complexes"
    },
    {
      "from": "deepmind-acquired-2014",
      "to": "alphacode-2022",
      "relationship": "enabled",
      "description": "DeepMind's resources enabled AlphaCode research"
    },
    {
      "from": "deepmind-acquired-2014",
      "to": "alphageometry-2024",
      "relationship": "enabled",
      "description": "DeepMind continued pushing AI reasoning boundaries"
    },
    {
      "from": "openai-founded-2015",
      "to": "gpt1-2018",
      "relationship": "led-to",
      "description": "OpenAI created GPT series"
    },
    {
      "from": "openai-founded-2015",
      "to": "dalle-2021",
      "relationship": "led-to",
      "description": "OpenAI created DALL-E image generation"
    },
    {
      "from": "deepmind-founded-2010",
      "to": "openai-founded-2015",
      "relationship": "inspired",
      "description": "DeepMind's approach influenced OpenAI's founding mission"
    },
    {
      "from": "resnet-2015",
      "to": "openai-founded-2015",
      "relationship": "context",
      "description": "Deep learning breakthroughs motivated OpenAI founding"
    },
    {
      "from": "alphacode-2022",
      "to": "copilot-2021",
      "relationship": "competed-with",
      "description": "Different approaches to AI code generation"
    },
    {
      "from": "ada-lovelace-1843",
      "to": "boolean-algebra-1847",
      "relationship": "contemporary",
      "description": "Contemporaries laying groundwork for computing"
    },
    {
      "from": "hilbert-problems-1900",
      "to": "godel-incompleteness-1931",
      "relationship": "inspired",
      "description": "Gödel addressed Hilbert's questions about mathematical completeness"
    },
    {
      "from": "hilbert-problems-1900",
      "to": "turing-machine-1936",
      "relationship": "inspired",
      "description": "Turing solved Hilbert's Entscheidungsproblem (negatively)"
    },
    {
      "from": "eliza-1966",
      "to": "shakey-robot-1969",
      "relationship": "contemporary",
      "description": "Both showed early AI promise"
    },
    {
      "from": "predicate-logic-1879",
      "to": "prolog-1972",
      "relationship": "implemented-by",
      "description": "Prolog implemented predicate logic for programming"
    },
    {
      "from": "prolog-1972",
      "to": "japan-5th-gen-1982",
      "relationship": "enabled",
      "description": "Japan's 5th Gen was built on Prolog"
    },
    {
      "from": "japan-5th-gen-1982",
      "to": "ai-winter-collapse-1987",
      "relationship": "contributed-to",
      "description": "5th Gen failure contributed to AI winter"
    },
    {
      "from": "perceptrons-book-1969",
      "to": "ai-winter-collapse-1987",
      "relationship": "contributed-to",
      "description": "Perceptron limitations contributed to skepticism"
    },
    {
      "from": "google-founded-1998",
      "to": "deepmind-founded-2010",
      "relationship": "acquired",
      "description": "Google later acquired DeepMind"
    },
    {
      "from": "random-forests-2001",
      "to": "netflix-prize-2006",
      "relationship": "used-in",
      "description": "Ensemble methods like RF were key to Netflix Prize"
    },
    {
      "from": "netflix-prize-2006",
      "to": "deep-belief-networks-2006",
      "relationship": "context",
      "description": "Both 2006 - ML gaining momentum"
    },
    {
      "from": "nvidia-cuda-2006",
      "to": "alexnet-2012",
      "relationship": "enabled",
      "description": "CUDA made GPU training of AlexNet possible"
    },
    {
      "from": "nvidia-cuda-2006",
      "to": "deep-belief-networks-2006",
      "relationship": "enabled",
      "description": "GPU computing enabled larger neural network experiments"
    },
    {
      "from": "google-tpu-2016",
      "to": "alphago-2016",
      "relationship": "powered",
      "description": "TPUs powered AlphaGo's match against Lee Sedol"
    },
    {
      "from": "nvidia-h100-2022",
      "to": "chatgpt-2022",
      "relationship": "powered",
      "description": "H100s used for training large models"
    },
    {
      "from": "nvidia-h100-2022",
      "to": "nvidia-blackwell-2024",
      "relationship": "evolved-into",
      "description": "Blackwell succeeded Hopper architecture"
    },
    {
      "from": "dqn-atari-2013",
      "to": "deepmind-acquired-2014",
      "relationship": "led-to",
      "description": "DQN success led to Google acquisition"
    },
    {
      "from": "dqn-atari-2013",
      "to": "alphago-2016",
      "relationship": "enabled",
      "description": "DQN techniques evolved into AlphaGo"
    },
    {
      "from": "openai-gym-2016",
      "to": "openai-five-2019",
      "relationship": "enabled",
      "description": "Gym environment framework supported RL research"
    },
    {
      "from": "alphago-2016",
      "to": "openai-five-2019",
      "relationship": "inspired",
      "description": "AlphaGo success inspired similar RL projects"
    },
    {
      "from": "neural-language-model-2003",
      "to": "word2vec-2013",
      "relationship": "evolved-into",
      "description": "Bengio's work led to Word2Vec embeddings"
    },
    {
      "from": "word2vec-2013",
      "to": "glove-2014",
      "relationship": "inspired",
      "description": "GloVe was alternative approach to word vectors"
    },
    {
      "from": "glove-2014",
      "to": "seq2seq-attention-2014",
      "relationship": "context",
      "description": "Both 2014 NLP advances"
    },
    {
      "from": "glove-2014",
      "to": "bert-2018",
      "relationship": "preceded",
      "description": "BERT's contextual embeddings replaced static GloVe"
    },
    {
      "from": "llama-2023",
      "to": "llama31-405b-2024",
      "relationship": "evolved-into",
      "description": "LLaMA 3.1 continued the series at massive scale"
    },
    {
      "from": "openai-founded-2015",
      "to": "grok-xai-2023",
      "relationship": "competed-with",
      "description": "xAI competes with OpenAI"
    },
    {
      "from": "gpt4-2023",
      "to": "qwen25-2024",
      "relationship": "competed-with",
      "description": "Qwen competes with GPT-4 class models"
    },
    {
      "from": "chatgpt-2022",
      "to": "perplexity-2022",
      "relationship": "inspired",
      "description": "LLM success enabled answer engines"
    },
    {
      "from": "rag-original-2020",
      "to": "perplexity-2022",
      "relationship": "used-by",
      "description": "Perplexity uses RAG architecture"
    },
    {
      "from": "gemini-1-2023",
      "to": "notebooklm-2024",
      "relationship": "powers",
      "description": "Gemini powers NotebookLM features"
    },
    {
      "from": "claude-35-sonnet-2024",
      "to": "anthropic-claude-computer-use-2024",
      "relationship": "enabled",
      "description": "Claude 3.5 introduced computer use capability"
    }
  ],
  "key_people": []
}