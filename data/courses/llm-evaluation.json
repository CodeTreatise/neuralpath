{
  "id": "llm-evaluation",
  "title": "LLM Evaluation & Testing",
  "description": "Master comprehensive techniques for evaluating and testing LLM-powered applications. Learn to measure accuracy, reliability, and safety of AI systems using automated metrics, LLM-as-judge approaches, and human evaluation frameworks to ensure production-ready AI solutions.",
  "icon": "ðŸ“",
  "level": "intermediate",
  "duration": "3 weeks",
  "validationSources": [
    "https://www.ibm.com/topics/ai-model-evaluation",
    "https://www.ibm.com/topics/ai-benchmarks",
    "https://www.ibm.com/topics/ai-hallucinations"
  ],
  "prerequisites": [
    "LLM fundamentals",
    "RAG basics",
    "Python programming"
  ],
  "learningOutcomes": [
    "Understand why LLM evaluation is challenging and essential",
    "Use OpenAI Evals API for automated prompt testing",
    "Implement RAG evaluation with RAGAS metrics",
    "Build evaluation pipelines with LangSmith",
    "Design effective human evaluation workflows"
  ],
  "modules": [
    {
      "id": "eval-fundamentals",
      "title": "Evaluation Fundamentals",
      "lessons": [
        {
          "id": "why-eval-matters",
          "title": "Why LLM Evaluation Matters",
          "duration": "35 min",
          "content": {
            "overview": "Unlike traditional software, LLM outputs are non-deterministic and subjective. Evaluation is critical for building reliable AI applications but requires new approaches.",
            "sections": [
              {
                "title": "The Evaluation Challenge",
                "content": "LLM evaluation is hard because:\n\n**1. Non-determinism**: Same input can produce different outputs\n**2. Subjectivity**: 'Good' depends on context and use case\n**3. Multi-dimensional**: Accuracy, fluency, helpfulness, safety all matter\n**4. Scale**: Manual evaluation doesn't scale\n**5. Distribution shift**: Models behave differently on new data\n\n**What can go wrong without evaluation**:\n- Hallucinations reach production\n- Regression after prompt changes\n- Poor user experience\n- Safety issues\n- Wasted costs on bad responses",
                "code": "# Example: Same prompt, different outputs\nimport openai\n\nprompt = \"Explain quantum computing in one sentence.\"\n\nresponses = []\nfor _ in range(3):\n    response = openai.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.7\n    )\n    responses.append(response.choices[0].message.content)\n\n# All valid, but different:\n# 1. \"Quantum computing uses quantum mechanics to process information...\"\n# 2. \"It's computing that leverages superposition and entanglement...\"\n# 3. \"Quantum computers use qubits instead of classical bits...\"\n\n# How do we know which is 'best'?"
              },
              {
                "title": "Evaluation Dimensions",
                "content": "Different dimensions matter for different use cases:\n\n**Factual Accuracy**: Is the information correct?\n- Critical for: search, Q&A, medical, legal\n- Measured by: fact verification, comparison to ground truth\n\n**Relevance**: Does it answer the question?\n- Critical for: RAG, customer support\n- Measured by: semantic similarity, answer extraction\n\n**Faithfulness**: Is it grounded in provided context?\n- Critical for: RAG, summarization\n- Measured by: claim verification, citation accuracy\n\n**Fluency**: Is it well-written?\n- Critical for: content generation, translation\n- Measured by: perplexity, grammar checks\n\n**Helpfulness**: Does it solve the user's problem?\n- Critical for: assistants, chatbots\n- Measured by: user ratings, task completion\n\n**Safety**: Is it harmful or inappropriate?\n- Critical for: all public-facing applications\n- Measured by: toxicity classifiers, red-teaming",
                "code": "# Define evaluation dimensions for your use case\nclass EvaluationConfig:\n    def __init__(self, use_case):\n        self.dimensions = self._get_dimensions(use_case)\n    \n    def _get_dimensions(self, use_case):\n        configs = {\n            'rag_qa': {\n                'faithfulness': {'weight': 0.3, 'required': True},\n                'relevance': {'weight': 0.3, 'required': True},\n                'accuracy': {'weight': 0.3, 'required': True},\n                'fluency': {'weight': 0.1, 'required': False}\n            },\n            'chatbot': {\n                'helpfulness': {'weight': 0.4, 'required': True},\n                'safety': {'weight': 0.3, 'required': True},\n                'fluency': {'weight': 0.2, 'required': True},\n                'engagement': {'weight': 0.1, 'required': False}\n            },\n            'summarization': {\n                'faithfulness': {'weight': 0.4, 'required': True},\n                'conciseness': {'weight': 0.3, 'required': True},\n                'coverage': {'weight': 0.2, 'required': True},\n                'fluency': {'weight': 0.1, 'required': False}\n            }\n        }\n        return configs.get(use_case, configs['rag_qa'])"
              },
              {
                "title": "Evaluation Pipeline Design",
                "content": "A robust evaluation pipeline includes:\n\n**1. Test Set Creation**\n- Representative samples\n- Edge cases\n- Expected outputs (where possible)\n\n**2. Automated Metrics**\n- Fast, repeatable\n- Run on every change\n\n**3. LLM-as-Judge**\n- More nuanced than metrics\n- Cheaper than humans\n\n**4. Human Evaluation**\n- Ground truth for important decisions\n- Periodic calibration\n\n**5. Production Monitoring**\n- Real user feedback\n- Drift detection",
                "diagram": {
                  "title": "LLM Evaluation Pipeline",
                  "code": "flowchart LR\n    subgraph Input\n        TS[Test Set]\n        PR[Production Queries]\n    end\n    \n    subgraph Automated[\"Automated Eval\"]\n        AM[Metrics]\n        LJ[LLM Judge]\n    end\n    \n    subgraph Human[\"Human Eval\"]\n        HE[Expert Review]\n        UF[User Feedback]\n    end\n    \n    TS --> AM\n    AM --> LJ\n    LJ --> |\"Sample\"| HE\n    PR --> UF\n    \n    AM --> D{Pass?}\n    LJ --> D\n    D -->|Yes| DEPLOY[Deploy]\n    D -->|No| FIX[Fix & Retry]"
                },
                "code": "class EvaluationPipeline:\n    def __init__(self, config):\n        self.config = config\n        self.test_set = self._load_test_set()\n        \n    def run_evaluation(self, model_fn, run_name):\n        results = {\n            'automated_metrics': self._run_automated(model_fn),\n            'llm_judge': self._run_llm_judge(model_fn),\n            'aggregate_score': None\n        }\n        \n        # Calculate aggregate\n        results['aggregate_score'] = self._aggregate(results)\n        \n        # Save results\n        self._save_results(run_name, results)\n        \n        # Check thresholds\n        if not self._passes_thresholds(results):\n            raise EvaluationFailed(results)\n        \n        return results\n    \n    def _run_automated(self, model_fn):\n        metrics = {}\n        for sample in self.test_set:\n            output = model_fn(sample['input'])\n            metrics[sample['id']] = {\n                'similarity': self._semantic_similarity(output, sample['expected']),\n                'length_ok': self._check_length(output),\n                'format_ok': self._check_format(output)\n            }\n        return metrics\n    \n    def _run_llm_judge(self, model_fn):\n        # Sample subset for LLM evaluation (cost control)\n        sample = random.sample(self.test_set, min(50, len(self.test_set)))\n        \n        scores = []\n        for s in sample:\n            output = model_fn(s['input'])\n            score = self._judge_with_llm(s['input'], output, s.get('context'))\n            scores.append(score)\n        \n        return {'mean': np.mean(scores), 'std': np.std(scores)}"
              }
            ],
            "keyTakeaways": [
              "LLM evaluation is challenging due to non-determinism and subjectivity",
              "Different use cases prioritize different evaluation dimensions",
              "Robust evaluation combines automated metrics, LLM judges, and human review"
            ],
            "exercises": [
              {
                "title": "Define Your Evaluation Dimensions",
                "description": "For a RAG-based customer support bot, identify the 3 most important evaluation dimensions and explain why"
              }
            ],
            "sources": [
              {
                "title": "Evaluating LLM Applications - Hamel Husain",
                "url": "https://hamel.dev/blog/posts/evals/"
              },
              {
                "title": "LLM Evaluation Metrics - Weights & Biases",
                "url": "https://wandb.ai/site/articles/llm-evaluation-metrics"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "automated-eval",
      "title": "Automated Evaluation",
      "lessons": [
        {
          "id": "openai-evals-api",
          "title": "OpenAI Evals API",
          "duration": "35 min",
          "content": {
            "overview": "OpenAI provides an official Evals API for testing model outputs against your criteria - think of it as 'unit tests for your prompts'. You define what success looks like, provide test data, and the platform runs your prompts against multiple test cases automatically (Source: OpenAI Evals Guide).",
            "sections": [
              {
                "title": "How OpenAI Evals Work",
                "content": "The Evals workflow follows three steps - similar to behavior-driven development (BDD):\n\n**Step 1: Create an Eval** - Define the task and success criteria\n**Step 2: Run with Test Data** - Execute your prompt against test cases  \n**Step 3: Analyze Results** - See what passed/failed and iterate\n\n**Key Components:**\n- `data_source_config`: Schema for your test data (inputs + expected outputs)\n- `testing_criteria`: How to check if outputs are correct (graders)\n- `data_source`: Your test data file (JSONL format)",
                "diagram": {
                  "title": "OpenAI Evals Workflow",
                  "code": "flowchart LR\n    subgraph Define\n        E[Create Eval] --> S[Set Schema]\n        S --> G[Add Graders]\n    end\n    \n    subgraph Run\n        D[Upload Test Data] --> R[Create Run]\n        R --> P[Generate Outputs]\n    end\n    \n    subgraph Analyze\n        P --> C[Check Criteria]\n        C --> RES[Pass/Fail Results]\n    end"
                }
              },
              {
                "title": "Creating an Eval",
                "content": "First, define what you're testing and how to grade it:",
                "code": "import requests\nimport os\n\n# Create an eval for IT ticket classification\neval_config = {\n    \"name\": \"IT Ticket Categorization\",\n    \"data_source_config\": {\n        \"type\": \"custom\",\n        \"item_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"ticket_text\": {\"type\": \"string\"},\n                \"correct_label\": {\"type\": \"string\"}\n            },\n            \"required\": [\"ticket_text\", \"correct_label\"]\n        },\n        \"include_sample_schema\": True\n    },\n    # Testing criteria - how to grade outputs\n    \"testing_criteria\": [\n        {\n            \"type\": \"string_check\",  # Exact match grader\n            \"name\": \"Match output to human label\",\n            \"input\": \"{{ sample.output_text }}\",\n            \"operation\": \"eq\",  # equals\n            \"reference\": \"{{ item.correct_label }}\"\n        }\n    ]\n}\n\n# Create via API\nresponse = requests.post(\n    \"https://api.openai.com/v1/evals\",\n    headers={\n        \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\",\n        \"Content-Type\": \"application/json\"\n    },\n    json=eval_config\n)\n\neval_id = response.json()[\"id\"]\nprint(f\"Created eval: {eval_id}\")"
              },
              {
                "title": "Grader Types",
                "content": "OpenAI provides several grader types for different testing needs:\n\n**`string_check`** - Simple string operations\n- `eq`: Exact match\n- `contains`: Output contains reference\n- `starts_with`, `ends_with`: Prefix/suffix matching\n- `regex`: Regular expression match\n\n**`model_graded_criteria`** - LLM-as-judge for subjective criteria\n- Define criteria in natural language\n- Model scores the output (typically 1-5 or pass/fail)\n\n**`model_graded_factual`** - Check factual accuracy against context\n\n**`code_execution`** - Run code to verify outputs\n- Great for testing structured outputs, JSON parsing, etc.",
                "code": "# Different grader examples\n\n# 1. String check - exact match\nstring_grader = {\n    \"type\": \"string_check\",\n    \"name\": \"Category match\",\n    \"input\": \"{{ sample.output_text }}\",\n    \"operation\": \"eq\",\n    \"reference\": \"{{ item.expected }}\"\n}\n\n# 2. String check - contains\ncontains_grader = {\n    \"type\": \"string_check\",\n    \"name\": \"Contains key phrase\",\n    \"input\": \"{{ sample.output_text }}\",\n    \"operation\": \"contains\",\n    \"reference\": \"{{ item.required_phrase }}\"\n}\n\n# 3. Model-graded - subjective quality\nmodel_grader = {\n    \"type\": \"model_graded_criteria\",\n    \"name\": \"Helpfulness\",\n    \"model\": \"gpt-4o\",\n    \"criteria\": \"Is the response helpful and actionable for the user's question?\",\n    \"input\": {\n        \"question\": \"{{ item.question }}\",\n        \"answer\": \"{{ sample.output_text }}\"\n    }\n}\n\n# 4. Regex for format checking\nformat_grader = {\n    \"type\": \"string_check\",\n    \"name\": \"Valid JSON format\",\n    \"input\": \"{{ sample.output_text }}\",\n    \"operation\": \"regex\",\n    \"reference\": r'^\\{.*\\}$'  # Starts and ends with braces\n}"
              },
              {
                "title": "Running an Eval",
                "content": "Upload test data and run your eval:",
                "code": "from openai import OpenAI\nimport json\n\nclient = OpenAI()\n\n# 1. Create test data file (JSONL format)\ntest_data = [\n    {\"item\": {\"ticket_text\": \"My monitor won't turn on!\", \"correct_label\": \"Hardware\"}},\n    {\"item\": {\"ticket_text\": \"I'm in vim and I can't quit!\", \"correct_label\": \"Software\"}},\n    {\"item\": {\"ticket_text\": \"Best restaurants near office?\", \"correct_label\": \"Other\"}}\n]\n\nwith open(\"test_tickets.jsonl\", \"w\") as f:\n    for item in test_data:\n        f.write(json.dumps(item) + \"\\n\")\n\n# 2. Upload the file\nwith open(\"test_tickets.jsonl\", \"rb\") as f:\n    file = client.files.create(file=f, purpose=\"evals\")\n\nprint(f\"Uploaded file: {file.id}\")\n\n# 3. Create an eval run\nimport requests\n\nrun_config = {\n    \"name\": \"Classification test v1.0\",\n    \"data_source\": {\n        \"type\": \"responses\",\n        \"model\": \"gpt-4.1\",\n        \"input_messages\": {\n            \"type\": \"template\",\n            \"template\": [\n                {\n                    \"role\": \"developer\",\n                    \"content\": \"\"\"Categorize the IT support ticket into one of: \nHardware, Software, or Other. Respond with only one word.\"\"\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"{{ item.ticket_text }}\"  # Template variable!\n                }\n            ]\n        },\n        \"source\": {\"type\": \"file_id\", \"id\": file.id}\n    }\n}\n\nresponse = requests.post(\n    f\"https://api.openai.com/v1/evals/{eval_id}/runs\",\n    headers={\n        \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\",\n        \"Content-Type\": \"application/json\"\n    },\n    json=run_config\n)\n\nrun = response.json()\nprint(f\"Run started: {run['id']}\")\nprint(f\"View results: {run['report_url']}\")"
              },
              {
                "title": "Analyzing Results",
                "content": "Check your eval run results programmatically or in the dashboard:",
                "code": "import time\n\ndef wait_for_eval_run(eval_id, run_id):\n    \"\"\"Poll until eval run completes\"\"\"\n    while True:\n        response = requests.get(\n            f\"https://api.openai.com/v1/evals/{eval_id}/runs/{run_id}\",\n            headers={\"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"}\n        )\n        run = response.json()\n        \n        if run[\"status\"] == \"completed\":\n            return run\n        elif run[\"status\"] in [\"failed\", \"canceled\"]:\n            raise Exception(f\"Run failed: {run['error']}\")\n        \n        print(f\"Status: {run['status']}...\")\n        time.sleep(5)\n\n# Wait for completion\nresults = wait_for_eval_run(eval_id, run[\"id\"])\n\n# Check results\nprint(f\"\\n=== Eval Results ===\")\nprint(f\"Total tests: {results['result_counts']['total']}\")\nprint(f\"Passed: {results['result_counts']['passed']}\")\nprint(f\"Failed: {results['result_counts']['failed']}\")\nprint(f\"Errors: {results['result_counts']['errored']}\")\n\n# Per-criteria breakdown\nfor criteria in results['per_testing_criteria_results']:\n    print(f\"\\n{criteria['testing_criteria']}:\")\n    print(f\"  Passed: {criteria['passed']}, Failed: {criteria['failed']}\")\n\n# Token usage\nfor usage in results['per_model_usage']:\n    print(f\"\\nModel: {usage['model_name']}\")\n    print(f\"  Invocations: {usage['invocation_count']}\")\n    print(f\"  Tokens: {usage['total_tokens']}\")\n\n# View detailed results in dashboard\nprint(f\"\\nðŸ“Š Full report: {results['report_url']}\")"
              }
            ],
            "keyTakeaways": [
              "OpenAI Evals API lets you create 'unit tests' for your prompts",
              "Define success criteria with graders: string_check, model_graded, regex",
              "Upload test data as JSONL with inputs and expected outputs",
              "Run evals to compare prompts, models, and track regressions"
            ],
            "sources": [
              {
                "title": "OpenAI Evals Guide",
                "url": "https://platform.openai.com/docs/guides/evals"
              },
              {
                "title": "OpenAI Graders Reference",
                "url": "https://platform.openai.com/docs/guides/graders"
              }
            ]
          }
        },
        {
          "id": "ragas-eval",
          "title": "Evaluating RAG with RAGAS",
          "duration": "50 min",
          "content": {
            "overview": "RAGAS (Retrieval Augmented Generation Assessment) provides specialized metrics for evaluating RAG pipelines, measuring both retrieval quality and generation faithfulness.",
            "sections": [
              {
                "title": "RAGAS Metrics Overview",
                "content": "RAGAS provides 4 core metrics:\n\n**1. Faithfulness** (0-1)\nMeasures if the answer is grounded in the retrieved context. Prevents hallucination.\n\n**2. Answer Relevancy** (0-1)\nMeasures if the answer addresses the question. Prevents off-topic responses.\n\n**3. Context Precision** (0-1)\nMeasures if retrieved chunks are relevant to the question. Evaluates retrieval quality.\n\n**4. Context Recall** (0-1)\nMeasures if all relevant information was retrieved. Requires ground truth answer.",
                "diagram": {
                  "title": "RAGAS Metrics Flow",
                  "code": "flowchart TB\n    Q[Question] --> R[Retriever]\n    R --> C[Contexts]\n    C --> G[Generator]\n    Q --> G\n    G --> A[Answer]\n    \n    subgraph Metrics\n        C --> CP[Context Precision]\n        C --> CR[Context Recall]\n        C --> F[Faithfulness]\n        A --> F\n        A --> AR[Answer Relevancy]\n        Q --> AR\n        Q --> CP\n        GT[Ground Truth] --> CR\n    end"
                },
                "code": "from ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall\n)\nfrom datasets import Dataset\n\n# Prepare your evaluation data\neval_data = {\n    'question': [\n        'What is the capital of France?',\n        'How does photosynthesis work?'\n    ],\n    'answer': [\n        'The capital of France is Paris.',\n        'Photosynthesis converts sunlight into chemical energy.'\n    ],\n    'contexts': [\n        ['Paris is the capital and largest city of France.'],\n        ['Photosynthesis is a process used by plants to convert light energy.']\n    ],\n    'ground_truth': [\n        'Paris is the capital of France.',\n        'Photosynthesis converts light energy into chemical energy in plants.'\n    ]\n}\n\ndataset = Dataset.from_dict(eval_data)\n\n# Run evaluation\nresults = evaluate(\n    dataset,\n    metrics=[\n        faithfulness,\n        answer_relevancy,\n        context_precision,\n        context_recall\n    ]\n)\n\nprint(results)\n# {'faithfulness': 0.95, 'answer_relevancy': 0.88, \n#  'context_precision': 0.92, 'context_recall': 0.85}"
              },
              {
                "title": "Understanding Each Metric",
                "content": "**Faithfulness** breaks down the answer into claims and verifies each against context:\n\n**Context Precision** uses LLM to judge if each chunk is relevant:\n\n**Answer Relevancy** generates questions from the answer and compares to original:",
                "code": "# Deep dive: How Faithfulness works internally\ndef faithfulness_score(answer, contexts):\n    # Step 1: Extract claims from the answer\n    claims = extract_claims(answer)\n    # e.g., ['Paris is the capital', 'Paris is in France']\n    \n    # Step 2: Verify each claim against context\n    verified = []\n    for claim in claims:\n        is_supported = llm_verify(\n            f\"Is this claim supported by the context?\\n\"\n            f\"Claim: {claim}\\n\"\n            f\"Context: {contexts}\"\n        )\n        verified.append(is_supported)\n    \n    # Step 3: Score = verified claims / total claims\n    return sum(verified) / len(claims)\n\n# Deep dive: How Context Precision works\ndef context_precision_score(question, contexts):\n    # For each chunk, judge relevance\n    relevance_scores = []\n    for i, chunk in enumerate(contexts):\n        is_relevant = llm_judge(\n            f\"Is this context relevant to answer the question?\\n\"\n            f\"Question: {question}\\n\"\n            f\"Context: {chunk}\"\n        )\n        relevance_scores.append(is_relevant)\n    \n    # Precision@k with position weighting\n    # Earlier relevant chunks are better\n    weighted_score = 0\n    for i, rel in enumerate(relevance_scores):\n        if rel:\n            precision_at_i = sum(relevance_scores[:i+1]) / (i+1)\n            weighted_score += precision_at_i\n    \n    return weighted_score / sum(relevance_scores) if sum(relevance_scores) > 0 else 0"
              },
              {
                "title": "Building an Evaluation Test Set",
                "content": "Good evaluation requires good test data:\n\n**Approaches to create test sets**:\n1. **Manual curation**: High quality, expensive\n2. **LLM generation**: Fast, needs validation\n3. **Production sampling**: Real queries, may lack ground truth\n4. **Synthetic variations**: Test robustness",
                "code": "from ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.document_loaders import DirectoryLoader\n\n# Load your documents\nloader = DirectoryLoader('./docs', glob='**/*.md')\ndocuments = loader.load()\n\n# Initialize generator\ngenerator_llm = ChatOpenAI(model='gpt-4o')\ncritic_llm = ChatOpenAI(model='gpt-4o')\nembeddings = OpenAIEmbeddings()\n\ngenerator = TestsetGenerator.from_langchain(\n    generator_llm, critic_llm, embeddings\n)\n\n# Generate test set with different question types\ntestset = generator.generate_with_langchain_docs(\n    documents,\n    test_size=50,\n    distributions={\n        simple: 0.4,        # Simple factual questions\n        reasoning: 0.3,     # Require reasoning\n        multi_context: 0.3  # Need multiple chunks\n    }\n)\n\n# Convert to pandas for review\ndf = testset.to_pandas()\nprint(df[['question', 'ground_truth', 'evolution_type']].head())\n\n# Manual review and curation\ndf.to_csv('test_set_for_review.csv')"
              },
              {
                "title": "Continuous Evaluation Pipeline",
                "content": "Run evaluations automatically on every change:",
                "code": "import json\nfrom datetime import datetime\nfrom pathlib import Path\n\nclass RAGEvaluator:\n    def __init__(self, test_set_path, results_dir='./eval_results'):\n        self.test_set = self._load_test_set(test_set_path)\n        self.results_dir = Path(results_dir)\n        self.results_dir.mkdir(exist_ok=True)\n        \n        # Thresholds for passing\n        self.thresholds = {\n            'faithfulness': 0.8,\n            'answer_relevancy': 0.75,\n            'context_precision': 0.7\n        }\n    \n    def evaluate_rag_pipeline(self, rag_fn, run_name):\n        \"\"\"Evaluate a RAG pipeline function\"\"\"\n        \n        # Generate answers\n        answers = []\n        contexts = []\n        for q in self.test_set['questions']:\n            result = rag_fn(q)\n            answers.append(result['answer'])\n            contexts.append(result['contexts'])\n        \n        # Prepare dataset\n        eval_dataset = Dataset.from_dict({\n            'question': self.test_set['questions'],\n            'answer': answers,\n            'contexts': contexts,\n            'ground_truth': self.test_set['ground_truths']\n        })\n        \n        # Run RAGAS\n        results = evaluate(\n            eval_dataset,\n            metrics=[faithfulness, answer_relevancy, context_precision]\n        )\n        \n        # Save results\n        result_file = self.results_dir / f\"{run_name}_{datetime.now().isoformat()}.json\"\n        with open(result_file, 'w') as f:\n            json.dump({\n                'run_name': run_name,\n                'timestamp': datetime.now().isoformat(),\n                'scores': dict(results),\n                'passed': self._check_thresholds(results)\n            }, f, indent=2)\n        \n        return results\n    \n    def _check_thresholds(self, results):\n        for metric, threshold in self.thresholds.items():\n            if results.get(metric, 0) < threshold:\n                return False\n        return True\n\n# Usage in CI/CD\nevaluator = RAGEvaluator('test_set.json')\nresults = evaluator.evaluate_rag_pipeline(my_rag_function, 'v1.2.0')\n\nif not results['passed']:\n    raise Exception(f\"Evaluation failed: {results}\")"
              }
            ],
            "keyTakeaways": [
              "RAGAS provides specialized metrics for RAG: faithfulness, relevancy, precision, recall",
              "Generate synthetic test sets from your documents for evaluation",
              "Run evaluations automatically in CI/CD with pass/fail thresholds"
            ],
            "exercises": [
              {
                "title": "Evaluate Your RAG",
                "description": "Set up RAGAS evaluation for an existing RAG pipeline with at least 20 test questions"
              },
              {
                "title": "Metric Analysis",
                "description": "Run evaluation and identify which metric is lowest. Propose improvements to address it."
              }
            ],
            "sources": [
              {
                "title": "RAGAS Documentation",
                "url": "https://docs.ragas.io/"
              },
              {
                "title": "RAGAS Paper",
                "url": "https://arxiv.org/abs/2309.15217"
              }
            ]
          }
        },
        {
          "id": "langsmith-eval",
          "title": "Evaluation with LangSmith",
          "duration": "45 min",
          "content": {
            "overview": "LangSmith provides a platform for tracing, evaluating, and monitoring LLM applications. It integrates tightly with LangChain but works with any LLM framework.",
            "sections": [
              {
                "title": "LangSmith Setup",
                "content": "LangSmith provides:\n- **Tracing**: See every LLM call, its inputs, outputs, and latency\n- **Datasets**: Manage evaluation test sets\n- **Evaluators**: Built-in and custom evaluation functions\n- **Monitoring**: Track production performance",
                "code": "# Setup\nimport os\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_API_KEY'] = 'your-api-key'\nos.environ['LANGCHAIN_PROJECT'] = 'my-rag-app'\n\nfrom langsmith import Client\n\nclient = Client()\n\n# Create a dataset\ndataset = client.create_dataset(\n    'rag-eval-set',\n    description='Evaluation dataset for RAG Q&A'\n)\n\n# Add examples\nexamples = [\n    {\n        'input': {'question': 'What is the return policy?'},\n        'output': {'answer': 'You can return items within 30 days.'}\n    },\n    {\n        'input': {'question': 'How do I track my order?'},\n        'output': {'answer': 'Use the tracking link in your confirmation email.'}\n    }\n]\n\nfor ex in examples:\n    client.create_example(\n        dataset_id=dataset.id,\n        inputs=ex['input'],\n        outputs=ex['output']\n    )"
              },
              {
                "title": "Custom Evaluators",
                "content": "Build evaluators tailored to your use case:",
                "code": "from langsmith.evaluation import evaluate, LangChainStringEvaluator\nfrom langsmith.schemas import Run, Example\n\n# Built-in evaluator: exact match\nexact_match = LangChainStringEvaluator('exact_match')\n\n# Built-in evaluator: LLM-based\ncorrectness = LangChainStringEvaluator(\n    'labeled_score_string',\n    config={\n        'criteria': {\n            'correctness': 'Is the answer factually correct based on the reference?'\n        },\n        'normalize_by': 10  # Score 0-10\n    }\n)\n\n# Custom evaluator function\ndef answer_length_evaluator(run: Run, example: Example) -> dict:\n    \"\"\"Check if answer is appropriate length\"\"\"\n    answer = run.outputs.get('answer', '')\n    length = len(answer.split())\n    \n    return {\n        'key': 'answer_length',\n        'score': 1 if 10 <= length <= 200 else 0,\n        'comment': f'Answer has {length} words'\n    }\n\ndef mentions_source_evaluator(run: Run, example: Example) -> dict:\n    \"\"\"Check if answer cites sources\"\"\"\n    answer = run.outputs.get('answer', '')\n    has_citation = '[' in answer or 'source' in answer.lower()\n    \n    return {\n        'key': 'has_citation',\n        'score': 1 if has_citation else 0\n    }\n\n# Combine evaluators\nevaluators = [\n    correctness,\n    answer_length_evaluator,\n    mentions_source_evaluator\n]"
              },
              {
                "title": "Running Evaluations",
                "content": "Evaluate your application against a dataset:",
                "code": "from langsmith.evaluation import evaluate\n\n# Your RAG function to evaluate\ndef my_rag_app(inputs: dict) -> dict:\n    question = inputs['question']\n    \n    # Your RAG logic here\n    contexts = retrieve(question)\n    answer = generate(question, contexts)\n    \n    return {\n        'answer': answer,\n        'contexts': contexts\n    }\n\n# Run evaluation\nresults = evaluate(\n    my_rag_app,\n    data='rag-eval-set',  # Dataset name\n    evaluators=[\n        correctness,\n        answer_length_evaluator,\n        mentions_source_evaluator\n    ],\n    experiment_prefix='rag-v1.0',\n    max_concurrency=4\n)\n\n# Results are automatically logged to LangSmith UI\nprint(f\"Experiment URL: {results.experiment_url}\")\n\n# Access scores programmatically\nfor result in results:\n    print(f\"Input: {result.example.inputs}\")\n    print(f\"Output: {result.run.outputs}\")\n    for eval_result in result.evaluation_results:\n        print(f\"  {eval_result.key}: {eval_result.score}\")"
              },
              {
                "title": "Comparing Experiments",
                "content": "Track improvements across versions:",
                "code": "# Run evaluation for different versions\nversions = [\n    ('baseline', baseline_rag),\n    ('improved-retrieval', improved_rag),\n    ('with-reranking', reranked_rag)\n]\n\nfor version_name, rag_fn in versions:\n    evaluate(\n        rag_fn,\n        data='rag-eval-set',\n        evaluators=evaluators,\n        experiment_prefix=f'rag-{version_name}'\n    )\n\n# In LangSmith UI, compare experiments side-by-side\n# See which version performs best on each metric\n\n# Programmatic comparison\nfrom langsmith import Client\n\nclient = Client()\n\n# Get all experiments\nexperiments = client.list_projects(name_contains='rag-')\n\nfor exp in experiments:\n    runs = list(client.list_runs(project_name=exp.name))\n    \n    # Calculate average scores\n    scores = defaultdict(list)\n    for run in runs:\n        for feedback in run.feedback:\n            scores[feedback.key].append(feedback.score)\n    \n    print(f\"\\n{exp.name}:\")\n    for metric, values in scores.items():\n        print(f\"  {metric}: {np.mean(values):.3f}\")"
              }
            ],
            "keyTakeaways": [
              "LangSmith provides end-to-end tracing and evaluation for LLM apps",
              "Create datasets and custom evaluators for your specific use case",
              "Compare experiments to track improvements across versions"
            ],
            "exercises": [
              {
                "title": "Set Up LangSmith",
                "description": "Create a LangSmith dataset with 10 examples and run an evaluation"
              },
              {
                "title": "Custom Evaluator",
                "description": "Build a custom evaluator that checks for specific domain requirements"
              }
            ],
            "sources": [
              {
                "title": "LangSmith Documentation",
                "url": "https://docs.smith.langchain.com/"
              },
              {
                "title": "LangSmith Evaluation Guide",
                "url": "https://docs.smith.langchain.com/evaluation"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "benchmarks-leaderboards",
      "title": "Benchmarks & Leaderboards",
      "lessons": [
        {
          "id": "standard-benchmarks",
          "title": "Standard LLM Benchmarks",
          "duration": "45 min",
          "content": {
            "overview": "LLM benchmarks are standardized frameworks for assessing model performance. Understanding major benchmarks helps you interpret model capabilities and choose the right model for your use case. However, benchmark scores don't always predict real-world performance.",
            "sections": [
              {
                "title": "Core Benchmark Categories",
                "content": "Benchmarks are classified by what they measure:\n\n**Knowledge & Reasoning**\n- MMLU: 15,000+ questions across 57 subjects\n- HellaSwag: Commonsense reasoning completion\n- ARC: Grade-school science questions\n- Winogrande: Pronoun resolution reasoning\n\n**Mathematics**\n- GSM8K: 8,500 grade-school math word problems\n- MATH: Competition-level math problems\n\n**Code Generation**\n- HumanEval: 164 programming problems with unit tests\n- MBPP: 900+ basic Python problems\n- SWE-bench: Real GitHub issues to fix\n\n**Truthfulness & Safety**\n- TruthfulQA: Measures hallucination tendency\n- ToxiGen: Detects toxic content generation\n\n**Multi-turn & Preference**\n- MT-Bench: 80 multi-turn dialogue questions\n- Chatbot Arena: Human preference ELO ratings",
                "diagram": {
                  "title": "Benchmark Categories",
                  "code": "flowchart TB\n    subgraph Knowledge\n        MMLU[MMLU\\n57 subjects]\n        HS[HellaSwag\\nCommonsense]\n        ARC[ARC\\nScience QA]\n    end\n    \n    subgraph Code\n        HE[HumanEval\\npass@k]\n        SWE[SWE-bench\\nGitHub fixes]\n    end\n    \n    subgraph Math\n        GSM[GSM8K\\nWord problems]\n        MATH[MATH\\nCompetition]\n    end\n    \n    subgraph Safety\n        TQA[TruthfulQA\\nHallucinations]\n        TOX[ToxiGen\\nToxicity]\n    end"
                }
              },
              {
                "title": "Key Benchmarks Explained",
                "content": "**MMLU (Massive Multitask Language Understanding)**\nTests breadth of knowledge across 57 academic subjects from STEM to humanities. Multiple-choice format, scored by accuracy.\n\n**HumanEval**\nMeasures code generation with **pass@k** metric:\n- Model generates k code samples per problem\n- pass@k = probability that at least 1 sample passes unit tests\n- Higher k gives more chances but reflects consistency\n\n**TruthfulQA**\nMeasures tendency to hallucinate with 817 questions designed to elicit false answers. Models trained on web data often confidently give wrong answers to these adversarial questions.\n\n**GSM8K**\nTests multi-step mathematical reasoning with grade-school word problems. Requires breaking down problems into steps.\n\n**SWE-bench**\nTests real software engineering ability by presenting GitHub issues from popular Python repos. Models must generate patches that pass existing test suites.",
                "code": "# Example: Running HumanEval locally\nimport human_eval\nfrom human_eval.data import write_jsonl, read_problems\nfrom human_eval.evaluation import evaluate_functional_correctness\n\n# Get problems\nproblems = read_problems()\n\n# Generate solutions with your model\ndef generate_solution(prompt):\n    # Your model inference here\n    response = model.generate(prompt)\n    return response\n\n# Generate samples\nsamples = []\nfor task_id, problem in problems.items():\n    completion = generate_solution(problem['prompt'])\n    samples.append({\n        'task_id': task_id,\n        'completion': completion\n    })\n\n# Save and evaluate\nwrite_jsonl('samples.jsonl', samples)\n\n# Evaluate pass@k\nresults = evaluate_functional_correctness(\n    'samples.jsonl',\n    k=[1, 10, 100]  # Calculate pass@1, pass@10, pass@100\n)\n\nprint(f\"pass@1: {results['pass@1']:.2%}\")\nprint(f\"pass@10: {results['pass@10']:.2%}\")"
              },
              {
                "title": "Zero-Shot vs Few-Shot Evaluation",
                "content": "Benchmarks test models in different settings:\n\n**Zero-Shot**: Model performs task with no examples\n- Tests generalization ability\n- More challenging, realistic scenario\n\n**Few-Shot**: Model given 1-5 examples before task\n- Tests in-context learning ability\n- Usually higher scores than zero-shot\n\n**Chain-of-Thought (CoT)**: Model prompted to 'think step by step'\n- Dramatically improves reasoning tasks\n- GSM8K: ~20% zero-shot â†’ ~80% with CoT",
                "code": "# Zero-shot evaluation\nzero_shot_prompt = \"\"\"Q: What is the capital of France?\nA:\"\"\"\n\n# Few-shot evaluation (2 examples)\nfew_shot_prompt = \"\"\"Q: What is the capital of Germany?\nA: Berlin\n\nQ: What is the capital of Japan?\nA: Tokyo\n\nQ: What is the capital of France?\nA:\"\"\"\n\n# Chain-of-thought for reasoning\ncot_prompt = \"\"\"Q: A store had 25 apples. They sold 8 in the morning \nand 12 in the afternoon. How many are left?\n\nA: Let me solve this step by step:\n1. Started with: 25 apples\n2. Sold in morning: 8 apples\n3. After morning: 25 - 8 = 17 apples\n4. Sold in afternoon: 12 apples\n5. Remaining: 17 - 12 = 5 apples\n\nThe answer is 5 apples.\n\nQ: [Your actual question]\nA: Let me solve this step by step:\"\"\""
              },
              {
                "title": "Benchmark Limitations",
                "content": "**Why Benchmark Scores Don't Tell the Whole Story**\n\n**1. Benchmark Saturation**\nTop models score 90%+ on older benchmarks like MMLU. At ceiling, benchmarks lose discriminative power. New harder benchmarks keep emerging.\n\n**2. Overfitting Risk**\nIf training data overlaps with benchmark data, scores inflate. Some model providers may inadvertently or intentionally train on benchmark data.\n\n**3. Transfer Gap**\nHigh benchmark scores don't guarantee good performance on your specific use case. Your domain may have unique requirements not captured by general benchmarks.\n\n**4. Gaming Metrics**\nModels can be optimized to game specific metrics without genuine capability improvement.\n\n**Best Practice**: Use benchmarks for initial filtering, then run your own domain-specific evaluations.",
                "code": "# Why you need custom evals beyond benchmarks\nclass BenchmarkVsRealWorld:\n    \"\"\"\n    Benchmark: MMLU score 85%\n    Real-world: Legal contract analysis for your company\n    \n    Problems:\n    1. MMLU tests general knowledge, not legal domain depth\n    2. MMLU is multiple choice, your task is free-form analysis\n    3. MMLU doesn't test your specific contract format\n    4. MMLU doesn't measure safety for your use case\n    \"\"\"\n    \n    def evaluation_strategy(self):\n        return [\n            \"1. Filter by benchmark (MMLU > 80%, TruthfulQA > 60%)\",\n            \"2. Run on 100 representative legal documents\",\n            \"3. Have lawyers grade accuracy on 5-point scale\",\n            \"4. Test adversarial inputs (edge cases, trick questions)\",\n            \"5. Measure latency and cost at production scale\"\n        ]"
              }
            ],
            "keyTakeaways": [
              "Major benchmarks test knowledge (MMLU), code (HumanEval), math (GSM8K), and truthfulness (TruthfulQA)",
              "pass@k measures code generation: probability that at least 1 of k samples passes tests",
              "Few-shot and Chain-of-Thought prompting significantly improve benchmark scores",
              "Benchmark scores are useful for filtering but don't guarantee real-world performance"
            ],
            "sources": [
              {
                "title": "IBM - LLM Benchmarks",
                "url": "https://www.ibm.com/think/topics/llm-benchmarks"
              },
              {
                "title": "HumanEval Paper",
                "url": "https://arxiv.org/abs/2107.03374"
              }
            ]
          }
        },
        {
          "id": "leaderboards-model-selection",
          "title": "Leaderboards & Model Selection",
          "duration": "35 min",
          "content": {
            "overview": "LLM leaderboards rank models across multiple benchmarks. They're valuable for model selection but require careful interpretation. Learn to use leaderboards effectively and understand their limitations.",
            "sections": [
              {
                "title": "Major LLM Leaderboards",
                "content": "**Hugging Face Open LLM Leaderboard**\n- Ranks open-source models\n- Composite score from: ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, GSM8K\n- Transparent methodology, reproducible results\n\n**Chatbot Arena (LMSYS)**\n- Human preference-based ELO ratings\n- Crowdsourced: users chat with 2 anonymous models, vote for winner\n- Most realistic measure of user preference\n- Covers both open and closed models\n\n**MT-Bench**\n- GPT-4 as judge for multi-turn conversations\n- 80 questions across 8 categories\n- Tests instruction-following and dialogue\n\n**HELM (Holistic Evaluation)**\n- Stanford's comprehensive evaluation\n- Tests 42 scenarios with 7 metric types\n- Focuses on holistic assessment beyond accuracy",
                "code": "# Fetching leaderboard data programmatically\nimport requests\nimport pandas as pd\n\n# Hugging Face Open LLM Leaderboard API\nHF_LEADERBOARD_URL = \"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\"\n\n# Example: Model selection based on leaderboard + requirements\nclass ModelSelector:\n    def __init__(self, requirements):\n        self.requirements = requirements\n        # Example requirements:\n        # {'min_mmlu': 70, 'max_params': 13, 'open_source': True}\n    \n    def filter_models(self, leaderboard_df):\n        filtered = leaderboard_df.copy()\n        \n        if 'min_mmlu' in self.requirements:\n            filtered = filtered[filtered['MMLU'] >= self.requirements['min_mmlu']]\n        \n        if 'max_params' in self.requirements:\n            filtered = filtered[filtered['params_b'] <= self.requirements['max_params']]\n        \n        if self.requirements.get('open_source'):\n            filtered = filtered[filtered['license'].isin(['MIT', 'Apache-2.0', 'Llama'])]\n        \n        return filtered.sort_values('average', ascending=False)\n    \n    def recommend(self, leaderboard_df):\n        candidates = self.filter_models(leaderboard_df)\n        \n        print(\"Top candidates for your requirements:\")\n        for _, model in candidates.head(5).iterrows():\n            print(f\"  {model['model_name']}: {model['average']:.1f}%\")\n        \n        return candidates"
              },
              {
                "title": "Interpreting Leaderboard Scores",
                "content": "**What High Scores Mean**\n- Model has strong general capabilities in tested areas\n- Useful baseline for shortlisting candidates\n\n**What High Scores DON'T Mean**\n- Will work well for your specific use case\n- Safe for production deployment\n- Cost-effective at scale\n- Good at following your specific instructions\n\n**Red Flags to Watch For**\n- Suspiciously high scores on all benchmarks (possible data contamination)\n- High variance between runs (unreliable model)\n- Missing benchmarks (may hide weaknesses)\n- No reproduction details",
                "code": "# Decision framework for model selection\ndef select_model_for_production(use_case):\n    selection_criteria = {\n        'rag_qa': {\n            'priority_benchmarks': ['TruthfulQA', 'MMLU'],\n            'reasoning': 'Need low hallucination + broad knowledge',\n            'test_focus': 'Faithfulness to retrieved context'\n        },\n        'code_assistant': {\n            'priority_benchmarks': ['HumanEval', 'MBPP', 'SWE-bench'],\n            'reasoning': 'Need code correctness + real-world fixing ability',\n            'test_focus': 'pass@1 on your language/framework'\n        },\n        'math_tutor': {\n            'priority_benchmarks': ['GSM8K', 'MATH'],\n            'reasoning': 'Need step-by-step reasoning ability',\n            'test_focus': 'Explanation quality, not just answer'\n        },\n        'creative_writing': {\n            'priority_benchmarks': ['MT-Bench', 'Chatbot Arena'],\n            'reasoning': 'Need user preference + style variety',\n            'test_focus': 'Tone matching, originality'\n        },\n        'customer_support': {\n            'priority_benchmarks': ['MT-Bench', 'HellaSwag'],\n            'reasoning': 'Need instruction following + commonsense',\n            'test_focus': 'Policy compliance, empathy'\n        }\n    }\n    \n    config = selection_criteria.get(use_case, selection_criteria['rag_qa'])\n    \n    return {\n        'step_1': f\"Filter by {config['priority_benchmarks']} scores\",\n        'step_2': f\"Shortlist top 3-5 models\",\n        'step_3': f\"Run custom eval focused on: {config['test_focus']}\",\n        'step_4': 'Test latency and cost at expected scale',\n        'step_5': 'Red-team for safety in your domain'\n    }"
              },
              {
                "title": "Model vs System Evaluation",
                "content": "**Model Evaluation**\n- Tests the LLM in isolation\n- Measures raw capabilities\n- What benchmarks typically assess\n\n**System Evaluation**\n- Tests your complete application\n- Includes: retrieval, prompts, guardrails, post-processing\n- What actually matters for production\n\n**Why the Distinction Matters**\nA weaker model in a well-designed system can outperform a stronger model with poor prompts. Always evaluate your full stack, not just the model.",
                "code": "class EvaluationLevels:\n    \"\"\"\n    Three levels of LLM evaluation\n    \"\"\"\n    \n    def model_level(self, model):\n        \"\"\"Level 1: Raw model capabilities\"\"\"\n        return {\n            'what': 'Model in isolation',\n            'how': 'Standard benchmarks (MMLU, HumanEval)',\n            'when': 'Choosing between base models',\n            'example': 'GPT-4 vs Claude vs Llama'\n        }\n    \n    def prompt_level(self, model, prompts):\n        \"\"\"Level 2: Model + your prompts\"\"\"\n        return {\n            'what': 'Model with your system/user prompts',\n            'how': 'Custom test set, A/B testing prompts',\n            'when': 'Optimizing prompt design',\n            'example': 'Testing 5 different system prompts'\n        }\n    \n    def system_level(self, full_app):\n        \"\"\"Level 3: Complete application\"\"\"\n        return {\n            'what': 'End-to-end: retrieval + model + post-processing',\n            'how': 'RAGAS, user studies, production monitoring',\n            'when': 'Pre-launch validation, ongoing monitoring',\n            'example': 'Full RAG pipeline with guardrails'\n        }\n\n# Production evaluation should be at system level\n# but informed by model-level benchmarks for selection"
              }
            ],
            "keyTakeaways": [
              "Hugging Face Leaderboard ranks open-source models; Chatbot Arena uses human preference ELO",
              "Use leaderboards for initial shortlisting, then run domain-specific evaluations",
              "High benchmark scores don't guarantee production success - test your full system",
              "Match benchmark priorities to your use case (code â†’ HumanEval, QA â†’ TruthfulQA)"
            ],
            "sources": [
              {
                "title": "Hugging Face Open LLM Leaderboard",
                "url": "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"
              },
              {
                "title": "Chatbot Arena Paper",
                "url": "https://arxiv.org/abs/2403.04132"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "human-eval",
      "title": "Human Evaluation",
      "lessons": [
        {
          "id": "human-eval-design",
          "title": "Designing Human Evaluation",
          "duration": "40 min",
          "content": {
            "overview": "Automated metrics can't capture everything. Human evaluation provides ground truth for subjective qualities and helps calibrate automated systems.",
            "sections": [
              {
                "title": "When Human Evaluation is Essential",
                "content": "Use human evaluation for:\n\n**1. Subjective qualities**\n- Tone and style\n- Helpfulness\n- Creativity\n- User satisfaction\n\n**2. Edge cases**\n- Novel queries\n- Ambiguous situations\n- Safety concerns\n\n**3. Ground truth calibration**\n- Validating automated metrics\n- Training LLM judges\n- Benchmark creation\n\n**4. Pre-launch validation**\n- Final quality check\n- Stakeholder approval\n- User acceptance testing",
                "code": "# Decision framework for evaluation type\ndef choose_evaluation_method(task_type, volume, urgency):\n    if task_type in ['safety', 'legal', 'medical']:\n        return 'human_required'  # Always human review\n    \n    if volume < 100 and not urgency:\n        return 'human_preferred'  # Human is feasible\n    \n    if volume > 1000:\n        return 'automated_with_sampling'  # Auto + human sample\n    \n    return 'llm_judge_with_calibration'  # LLM judge, human calibrate"
              },
              {
                "title": "Designing Evaluation Rubrics",
                "content": "Clear rubrics ensure consistent ratings across evaluators:\n\n**Principles**:\n1. Define each score level with examples\n2. Use observable criteria, not interpretations\n3. Limit scale to 3-5 points\n4. Include 'N/A' option when applicable\n5. Pilot test with multiple evaluators",
                "code": "# Example: Helpfulness rubric for customer support\nHELPFULNESS_RUBRIC = {\n    'metric': 'helpfulness',\n    'question': 'How helpful is this response for resolving the user\\'s issue?',\n    'scale': {\n        1: {\n            'label': 'Not Helpful',\n            'description': 'Response does not address the issue or provides wrong information',\n            'examples': [\n                'User asks about refund, response discusses unrelated product features',\n                'Response says \"I don\\'t know\" without offering alternatives'\n            ]\n        },\n        2: {\n            'label': 'Partially Helpful',\n            'description': 'Response addresses the issue but is incomplete or unclear',\n            'examples': [\n                'Provides refund policy but doesn\\'t explain how to initiate',\n                'Gives correct info but in confusing language'\n            ]\n        },\n        3: {\n            'label': 'Helpful',\n            'description': 'Response fully addresses the issue with clear next steps',\n            'examples': [\n                'Explains refund policy AND provides link to start return',\n                'Solves the problem and offers to help with related questions'\n            ]\n        }\n    }\n}\n\n# Example: Faithfulness rubric for RAG\nFAITHFULNESS_RUBRIC = {\n    'metric': 'faithfulness',\n    'question': 'Is the response fully supported by the provided context?',\n    'scale': {\n        1: {'label': 'Hallucinated', 'description': 'Contains claims not in context'},\n        2: {'label': 'Partially Faithful', 'description': 'Some claims not supported'},\n        3: {'label': 'Fully Faithful', 'description': 'All claims supported by context'}\n    }\n}"
              },
              {
                "title": "Inter-Rater Reliability",
                "content": "Measure agreement between evaluators to ensure quality:\n\n**Cohen's Kappa**: Agreement between 2 raters\n**Fleiss' Kappa**: Agreement among 3+ raters\n**ICC**: For continuous scales",
                "code": "from sklearn.metrics import cohen_kappa_score\nimport numpy as np\n\ndef calculate_inter_rater_reliability(ratings_df):\n    \"\"\"\n    ratings_df: DataFrame with columns [item_id, rater_id, score]\n    \"\"\"\n    # Pivot to get rater x item matrix\n    pivot = ratings_df.pivot(index='item_id', columns='rater_id', values='score')\n    \n    # Calculate pairwise kappa\n    raters = pivot.columns.tolist()\n    kappas = []\n    \n    for i, r1 in enumerate(raters):\n        for r2 in raters[i+1:]:\n            # Get items rated by both\n            mask = pivot[[r1, r2]].notna().all(axis=1)\n            if mask.sum() > 10:  # Need enough samples\n                k = cohen_kappa_score(\n                    pivot.loc[mask, r1],\n                    pivot.loc[mask, r2]\n                )\n                kappas.append({'rater1': r1, 'rater2': r2, 'kappa': k})\n    \n    avg_kappa = np.mean([k['kappa'] for k in kappas])\n    \n    # Interpretation\n    if avg_kappa > 0.8:\n        interpretation = 'Excellent agreement'\n    elif avg_kappa > 0.6:\n        interpretation = 'Good agreement'\n    elif avg_kappa > 0.4:\n        interpretation = 'Moderate agreement - review rubric'\n    else:\n        interpretation = 'Poor agreement - retrain raters'\n    \n    return {\n        'average_kappa': avg_kappa,\n        'interpretation': interpretation,\n        'pairwise_kappas': kappas\n    }"
              },
              {
                "title": "Efficient Human Evaluation Workflows",
                "content": "Balance quality with cost and time:",
                "code": "class HumanEvaluationWorkflow:\n    def __init__(self, rubrics, target_reliability=0.7):\n        self.rubrics = rubrics\n        self.target_reliability = target_reliability\n        \n    def setup_evaluation(self, samples, num_raters=3):\n        \"\"\"Set up efficient evaluation with overlap for reliability\"\"\"\n        \n        # Strategy: Each item rated by 2 raters minimum\n        # 20% overlap for reliability calculation\n        \n        n_samples = len(samples)\n        n_overlap = int(n_samples * 0.2)\n        n_unique = n_samples - n_overlap\n        \n        assignments = []\n        \n        # Overlap items: all raters\n        overlap_samples = samples[:n_overlap]\n        for sample in overlap_samples:\n            for rater_id in range(num_raters):\n                assignments.append({\n                    'sample_id': sample['id'],\n                    'rater_id': rater_id,\n                    'is_overlap': True\n                })\n        \n        # Unique items: 2 raters each\n        unique_samples = samples[n_overlap:]\n        for i, sample in enumerate(unique_samples):\n            raters = [(i * 2) % num_raters, (i * 2 + 1) % num_raters]\n            for rater_id in raters:\n                assignments.append({\n                    'sample_id': sample['id'],\n                    'rater_id': rater_id,\n                    'is_overlap': False\n                })\n        \n        return assignments\n    \n    def resolve_disagreements(self, ratings):\n        \"\"\"Resolve cases where raters disagree\"\"\"\n        \n        disagreements = []\n        for item_id, item_ratings in ratings.groupby('item_id'):\n            scores = item_ratings['score'].tolist()\n            \n            if max(scores) - min(scores) > 1:  # Disagreement threshold\n                disagreements.append({\n                    'item_id': item_id,\n                    'scores': scores,\n                    'needs_review': True\n                })\n        \n        # Options:\n        # 1. Take majority vote\n        # 2. Send to expert reviewer\n        # 3. Discuss and reach consensus\n        \n        return disagreements"
              }
            ],
            "keyTakeaways": [
              "Human evaluation is essential for subjective qualities and edge cases",
              "Clear rubrics with examples ensure consistent ratings",
              "Measure inter-rater reliability to validate evaluation quality",
              "Use sampling and overlap strategies for efficiency"
            ],
            "exercises": [
              {
                "title": "Design a Rubric",
                "description": "Create a 5-point rubric for evaluating chatbot response quality in your domain"
              },
              {
                "title": "Calculate Agreement",
                "description": "Have 3 people rate 20 responses and calculate inter-rater reliability"
              }
            ],
            "sources": [
              {
                "title": "Human Evaluation of NLG - ACL",
                "url": "https://aclanthology.org/2020.inlg-1.23/"
              },
              {
                "title": "Rating Scale Design",
                "url": "https://www.sciencedirect.com/science/article/pii/S1364661318302456"
              }
            ]
          }
        },
        {
          "id": "llm-evaluation-quiz",
          "title": "LLM Evaluation Quiz",
          "type": "quiz",
          "duration": "15 min",
          "questions": [
            {
              "id": "mmlu-benchmark",
              "question": "What does the MMLU benchmark primarily measure?",
              "options": [
                "Code generation ability",
                "Breadth of knowledge across 57 academic subjects",
                "Speed of inference",
                "Image recognition accuracy"
              ],
              "explanation": "MMLU (Massive Multitask Language Understanding) tests breadth of knowledge across 57 academic subjects from STEM to humanities using multiple-choice questions.",
              "correct": 1
            },
            {
              "id": "pass-at-k",
              "question": "In the HumanEval benchmark, what does 'pass@k' measure?",
              "options": [
                "The percentage of code that runs without errors",
                "The probability that at least 1 of k generated samples passes unit tests",
                "How many seconds the code takes to execute",
                "The number of lines of code generated"
              ],
              "explanation": "pass@k is the probability that at least one of k code samples generated for a problem passes the corresponding unit tests. Higher k gives more chances but reflects model consistency.",
              "correct": 1
            },
            {
              "id": "truthfulqa-purpose",
              "question": "What is the primary purpose of the TruthfulQA benchmark?",
              "options": [
                "Testing mathematical reasoning",
                "Measuring hallucination tendency with adversarial questions",
                "Evaluating translation quality",
                "Assessing code debugging ability"
              ],
              "explanation": "TruthfulQA measures a model's tendency to generate false but plausible-sounding information (hallucinations) using questions specifically designed to elicit incorrect answers.",
              "correct": 1
            },
            {
              "id": "chatbot-arena",
              "question": "How does Chatbot Arena rank LLMs?",
              "options": [
                "By measuring response latency",
                "Using crowdsourced human preference votes in pairwise comparisons",
                "By counting the number of parameters",
                "Through automated grammar checking"
              ],
              "explanation": "Chatbot Arena uses crowdsourced human preference: users chat with two anonymous models and vote for their preferred response, generating ELO ratings similar to chess rankings.",
              "correct": 1
            },
            {
              "id": "benchmark-limitation",
              "question": "What is a major limitation of relying solely on benchmark leaderboards for model selection?",
              "options": [
                "Benchmarks are always too easy",
                "Leaderboards don't show model names",
                "High benchmark scores don't guarantee good performance on your specific use case",
                "Benchmarks can only test closed-source models"
              ],
              "explanation": "Benchmarks test general capabilities, but your production application has specific requirements, domains, and edge cases that require custom evaluation tailored to your use case.",
              "correct": 2
            },
            {
              "id": "eval-challenges",
              "question": "Why is evaluating LLM outputs significantly harder than traditional software testing?",
              "options": [
                "LLMs are too slow to test",
                "LLM outputs are non-deterministic and subjective",
                "There are no tools available for LLM testing",
                "LLMs cannot output text"
              ],
              "explanation": "LLMs can produce different outputs for the same input (non-determinism), and 'quality' often depends on context and human preference (subjectivity), making exact-match testing insufficient.",
              "correct": 1
            },
            {
              "id": "ragas-faithfulness",
              "question": "In the RAGAS framework, what does the 'Faithfulness' metric measure?",
              "options": [
                "How polite the model is",
                "Whether the answer is grounded in the retrieved context",
                "Whether the answer is grammatically correct",
                "How fast the model responds"
              ],
              "explanation": "Faithfulness measures if the claims in the generated answer can be inferred from the retrieved context, helping to detect hallucinations.",
              "correct": 1
            },
            {
              "id": "llm-as-judge",
              "question": "What is the 'LLM-as-Judge' approach?",
              "options": [
                "Using a lawyer to review AI outputs",
                "Using a strong LLM (like GPT-4) to evaluate the outputs of another model",
                "Asking the model to judge its own training data",
                "A legal framework for AI regulation"
              ],
              "explanation": "LLM-as-Judge involves using a capable LLM to score or critique the outputs of another system based on defined criteria, offering a scalable alternative to human review.",
              "correct": 1
            },
            {
              "id": "inter-rater-reliability",
              "question": "Why is 'Inter-Rater Reliability' important in human evaluation?",
              "options": [
                "It measures how fast humans can read",
                "It ensures that different evaluators consistently agree on ratings",
                "It calculates the cost of evaluation",
                "It checks if the model likes the humans"
              ],
              "explanation": "Inter-rater reliability metrics (like Cohen's Kappa) quantify the level of agreement between different human evaluators, ensuring that the evaluation criteria are clear and the scores are trustworthy.",
              "correct": 1
            },
            {
              "id": "eval-dimensions",
              "question": "Which evaluation dimension is most critical for a medical Q&A system?",
              "options": [
                "Fluency (how well it writes)",
                "Factual Accuracy (is the information correct)",
                "Creativity (how novel the answer is)",
                "Latency (how fast it answers)"
              ],
              "explanation": "In high-stakes domains like medicine, factual accuracy is paramount. A fluent but incorrect answer could be dangerous.",
              "correct": 1
            },
            {
              "id": "hallucination-detection",
              "question": "What is an AI hallucination in the context of LLM evaluation?",
              "options": [
                "When the model runs too slowly",
                "When the model generates confident but factually incorrect or fabricated information",
                "When the model refuses to answer",
                "When the model outputs are too short"
              ],
              "explanation": "AI hallucinations occur when LLMs generate plausible-sounding but false or unsupported information, presenting it as fact. Detecting and preventing hallucinations is a key evaluation concern.",
              "correct": 1
            },
            {
              "id": "context-relevance",
              "question": "In RAG evaluation, what does 'context relevance' measure?",
              "options": [
                "How fast the retrieval system responds",
                "Whether the retrieved documents are relevant to the user's query",
                "The total number of documents in the database",
                "The formatting of the context window"
              ],
              "explanation": "Context relevance measures whether the retrieval system finds documents that are actually relevant to answering the user's question, which directly impacts answer quality.",
              "correct": 1
            },
            {
              "id": "answer-correctness",
              "question": "What does 'answer correctness' measure in LLM evaluation?",
              "options": [
                "Whether the answer uses correct grammar",
                "Whether the answer accurately addresses the question and aligns with ground truth",
                "Whether the answer is short enough",
                "Whether the answer contains code"
              ],
              "explanation": "Answer correctness evaluates whether the generated response accurately and completely answers the question, often compared against ground truth or verified information.",
              "correct": 1
            },
            {
              "id": "benchmark-limitations",
              "question": "Why can't we rely solely on standard benchmarks like MMLU for evaluating production LLM applications?",
              "options": [
                "Benchmarks are too expensive to run",
                "Standard benchmarks may not reflect your specific use case, domain, or quality requirements",
                "Benchmarks only work for small models",
                "Benchmarks require special hardware"
              ],
              "explanation": "Standard benchmarks test general capabilities, but production applications have specific requirements, domains, and edge cases that require custom evaluation tailored to your use case.",
              "correct": 1
            },
            {
              "id": "evaluation-frequency",
              "question": "When should you run LLM evaluations in a production system?",
              "options": [
                "Only before the initial launch",
                "Continuously - before deployment, after prompt changes, and regularly in production",
                "Only when users complain",
                "Once per year during audits"
              ],
              "explanation": "Continuous evaluation catches regressions from prompt changes, model updates, and distribution shift. Evaluation should be part of your CI/CD pipeline and ongoing monitoring.",
              "correct": 1
            }
          ],
          "references": {
            "lessonRefs": [
              "intro-to-llm-evaluation",
              "automated-metrics",
              "human-evaluation"
            ],
            "externalRefs": [
              {
                "title": "RAGAS Framework",
                "url": "https://docs.ragas.io/"
              },
              {
                "title": "OpenAI Evals",
                "url": "https://github.com/openai/evals"
              }
            ]
          }
        }
      ]
    }
  ]
}