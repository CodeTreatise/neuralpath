{
  "id": "llm-finetuning",
  "title": "LLM Fine-tuning Mastery",
  "description": "Master the art of fine-tuning large language models to customize them for your specific use cases. Fine-tuning is the process of taking a pre-trained foundation model and adapting it for specialized tasks, domains, or organizational requirements through continued training on curated datasets.",
  "icon": "ðŸŽ¯",
  "level": "advanced",
  "duration": "6 weeks",
  "validationSources": [
    "https://www.ibm.com/topics/fine-tuning",
    "https://www.ibm.com/think/topics/fine-tuning-large-language-models",
    "https://www.ibm.com/topics/foundation-models"
  ],
  "keyThinkers": [
    {
      "name": "Edward J. Hu",
      "contribution": "Lead author of LoRA paper at Microsoft Research (2021)",
      "keyWork": "LoRA: Low-Rank Adaptation of Large Language Models"
    },
    {
      "name": "Tim Dettmers",
      "contribution": "Creator of QLoRA - 4-bit quantized fine-tuning at University of Washington (2023)",
      "keyWork": "QLoRA: Efficient Finetuning of Quantized LLMs"
    },
    {
      "name": "Rafael Rafailov",
      "contribution": "Lead author of DPO (Direct Preference Optimization) at Stanford (2023)",
      "keyWork": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
    },
    {
      "name": "Chelsea Finn",
      "contribution": "Senior author on DPO, pioneering research in meta-learning and RL",
      "keyWork": "Stanford AI Lab research on alignment and preference learning"
    },
    {
      "name": "Long Ouyang",
      "contribution": "Lead author of InstructGPT and RLHF at OpenAI (2022)",
      "keyWork": "Training Language Models to Follow Instructions with Human Feedback"
    },
    {
      "name": "Yuntao Bai",
      "contribution": "Lead author of Constitutional AI at Anthropic (2022)",
      "keyWork": "Constitutional AI: Harmlessness from AI Feedback"
    },
    {
      "name": "Yizhong Wang",
      "contribution": "Creator of Self-Instruct for synthetic instruction dataset generation",
      "keyWork": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
    },
    {
      "name": "Leandro von Werra",
      "contribution": "Creator and maintainer of TRL library at Hugging Face",
      "keyWork": "Transformer Reinforcement Learning library for RLHF/DPO"
    },
    {
      "name": "Sourab Mangrulkar",
      "contribution": "Lead maintainer of PEFT library at Hugging Face",
      "keyWork": "Parameter-Efficient Fine-Tuning library (LoRA, adapters, prefix tuning)"
    },
    {
      "name": "Daniel Han",
      "contribution": "Creator of Unsloth for 2x faster fine-tuning with 70% less VRAM",
      "keyWork": "Unsloth: Faster LLM fine-tuning with Triton kernels"
    }
  ],
  "prerequisites": [
    "Deep learning fundamentals",
    "PyTorch experience",
    "Transformer understanding"
  ],
  "whatYouNeed": [
    "GPU access (free tier: Google Colab, Kaggle)",
    "Python and PyTorch installed",
    "Completed the 'Math for AI' and 'Python for AI' courses, OR equivalent experience"
  ],
  "learningOutcomes": [
    "Understand when and why to fine-tune LLMs",
    "Implement LoRA and QLoRA for efficient fine-tuning",
    "Prepare high-quality instruction datasets",
    "Evaluate and deploy fine-tuned models"
  ],
  "modules": [
    {
      "id": "finetuning-intro",
      "title": "What is Fine-tuning? (Start Here!)",
      "lessons": [
        {
          "id": "finetuning-explained-simply",
          "title": "Fine-tuning in Plain English",
          "duration": "25 min",
          "content": {
            "overview": "Before we dive into code, let's understand what fine-tuning actually means and whether you even need it. Spoiler: most people don't need to fine-tune, but understanding it will make you a better AI practitioner.",
            "sections": [
              {
                "title": "The Restaurant Chef Analogy",
                "content": "Imagine you're a world-class chef (like GPT-4 or Claude). You can cook almost anything because you trained in the best culinary schools for years.\n\nBut now a company hires you to run their cafeteria. They want you to:\n- Always use their specific recipes\n- Follow their plating style exactly\n- Never suggest dishes they don't serve\n\n**You're still the same skilled chef, but now you've been 'fine-tuned' for their specific needs.**\n\nThat's fine-tuning:\n- **Pre-trained model** = The skilled chef (general knowledge)\n- **Fine-tuning** = Training on specific examples to behave differently\n- **Fine-tuned model** = Same chef, specialized for your use case"
              },
              {
                "title": "What Fine-tuning Actually Changes",
                "content": "An LLM is just billions of numbers (called 'weights' or 'parameters') that determine how it responds. When you fine-tune:\n\n**You adjust these numbers slightly** so the model:\n- Writes in a specific style\n- Uses domain-specific terminology correctly\n- Formats outputs consistently\n- Follows your specific instructions better\n\n**You are NOT:**\n- Teaching it new facts (use RAG for that)\n- Making it smarter (the knowledge is already there)\n- Creating a new model from scratch\n\n**Think of it as**: Fine-tuning changes the model's *habits*, not its *knowledge*."
              },
              {
                "title": "When Do You Actually Need This?",
                "content": "**Fine-tuning is probably OVERKILL if you:**\n- Just want better answers â†’ Use prompt engineering first\n- Need current information â†’ Use RAG (retrieval)\n- Have a small budget â†’ Fine-tuning costs money\n- Only have a few examples â†’ Not enough data\n\n**Fine-tuning makes sense when:**\n- You need consistent output format across thousands of calls\n- You want the model to 'sound' like your brand\n- You're doing high-volume production and want to save on token costs\n- You can't share data with OpenAI/Anthropic (privacy)\n\n**The 80/20 rule**: 80% of use cases are solved by good prompts + RAG. Only 20% truly need fine-tuning."
              },
              {
                "title": "Key Terms You'll Need",
                "content": "**Pre-trained model**: The base model (like Llama 3, Mistral) that has been trained on internet-scale data. It's the starting point.\n\n**Fine-tuning**: Continuing to train a pre-trained model on your specific data.\n\n**Full fine-tuning**: Updating ALL the model's parameters. Expensive, needs big GPUs.\n\n**LoRA (Low-Rank Adaptation)**: A clever trick that adds small trainable 'adapters' instead of changing all parameters. Much cheaper!\n\n**QLoRA**: LoRA + quantization. Runs on consumer GPUs (even free Colab!).\n\n**Instruction dataset**: Training examples in the format 'instruction â†’ response' that teach the model your desired behavior.\n\n**Catastrophic forgetting**: When fine-tuning makes the model forget its original capabilities. We avoid this with LoRA."
              },
              {
                "title": "Common Beginner Questions",
                "content": "**Q: Can I fine-tune ChatGPT?**\nA: You can fine-tune OpenAI's models through their API, but you can't access the actual weights. For full control, use open models like Llama or Mistral.\n\n**Q: How much data do I need?**\nA: For basic fine-tuning: 100-1,000 high-quality examples. For significant behavior change: 1,000-10,000+.\n\n**Q: What hardware do I need?**\nA: With QLoRA, you can fine-tune a 7B model on a free Google Colab GPU! We'll show you how.\n\n**Q: How long does it take?**\nA: A few hours for small datasets on consumer hardware. The data preparation takes longer than the training.\n\n**Q: Can I make the model smarter?**\nA: No. Fine-tuning doesn't add intelligence. It changes behavior and style. For adding knowledge, use RAG.\n\n**Q: Is this the same as 'training' a model?**\nA: Training from scratch takes millions of dollars. Fine-tuning is adjusting an existing model - much cheaper."
              }
            ],
            "keyTakeaways": [
              "Fine-tuning adjusts model behavior, not knowledge",
              "80% of use cases don't need fine-tuning - try prompting and RAG first",
              "LoRA/QLoRA makes fine-tuning accessible on consumer hardware",
              "You need 100+ high-quality examples to start"
            ],
            "exercises": [
              {
                "title": "Do You Need Fine-tuning?",
                "description": "Think of a project you want to build. Write down what you need the AI to do. Then decide: Is this a prompt engineering problem, a RAG problem, or a fine-tuning problem?"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "finetuning-fundamentals",
      "title": "Fine-tuning Fundamentals",
      "lessons": [
        {
          "id": "when-to-finetune",
          "title": "When and Why to Fine-tune",
          "author": "PEFT concepts by Sourab Mangrulkar et al. (Hugging Face); QLoRA by Tim Dettmers (UW)",
          "duration": "30 min",
          "content": {
            "overview": "Fine-tuning adapts a pre-trained LLM to your specific domain or task. But it's not always the right choiceâ€”understanding when to fine-tune versus when to use prompt engineering or RAG is crucial for efficient development.",
            "sections": [
              {
                "title": "The Fine-tuning Decision Framework",
                "content": "Consider fine-tuning when:\n\n**âœ… Good candidates for fine-tuning**:\n- Need consistent output format/style across many queries\n- Domain-specific terminology or jargon\n- Task requires behavior change (not just knowledge)\n- High-volume production with cost constraints\n- Privacy: Can't send data to external API\n\n**âŒ Consider alternatives first**:\n- Need up-to-date information â†’ **RAG**\n- Want to add knowledge â†’ **RAG**\n- One-off customization â†’ **Prompt engineering**\n- Complex multi-step reasoning â†’ **Agent systems**\n\n**Cost comparison** (rough estimates):\n| Approach | Setup Cost | Per-query Cost | Flexibility |\n|----------|------------|----------------|-------------|\n| Prompt Engineering | Low | Higher (more tokens) | High |\n| RAG | Medium | Medium | High |\n| Fine-tuning | High | Lower | Low |",
                "code": "# Decision tree implementation\ndef should_finetune(requirements: dict) -> str:\n    \"\"\"\n    Simple decision framework for fine-tuning.\n    \n    requirements = {\n        'need_current_info': bool,\n        'consistent_format': bool,\n        'domain_specific': bool,\n        'high_volume': bool,\n        'privacy_required': bool,\n        'behavior_change': bool\n    }\n    \"\"\"\n    # RAG is better for knowledge needs\n    if requirements.get('need_current_info'):\n        return \"Use RAG - fine-tuning can't provide current information\"\n    \n    # Strong indicators for fine-tuning\n    finetune_score = sum([\n        requirements.get('consistent_format', False) * 2,\n        requirements.get('domain_specific', False) * 1.5,\n        requirements.get('high_volume', False) * 2,\n        requirements.get('privacy_required', False) * 2,\n        requirements.get('behavior_change', False) * 2\n    ])\n    \n    if finetune_score >= 5:\n        return \"Fine-tuning recommended\"\n    elif finetune_score >= 3:\n        return \"Consider fine-tuning, but start with prompting\"\n    else:\n        return \"Start with prompt engineering or RAG\"\n\n# Example usage\nreqs = {\n    'need_current_info': False,\n    'consistent_format': True,\n    'domain_specific': True,\n    'high_volume': True,\n    'privacy_required': False,\n    'behavior_change': True\n}\n\nprint(should_finetune(reqs))  # \"Fine-tuning recommended\""
              },
              {
                "title": "Types of Fine-tuning",
                "content": "**Full Fine-tuning**: Update all model parameters\n- Most powerful but expensive\n- Requires significant compute (multiple GPUs)\n- Risk of catastrophic forgetting\n\n**Parameter-Efficient Fine-tuning (PEFT)**: Update only a small subset\n- LoRA, QLoRA, Adapters, Prefix Tuning\n- Can run on consumer hardware\n- Preserves base model capabilities\n\n**Instruction Fine-tuning**: Teach model to follow instructions\n- Most common for LLMs\n- Uses instruction-response pairs\n\n**RLHF/DPO**: Align with human preferences\n- Requires preference data (chosen vs rejected)\n- More complex pipeline",
                "diagram": {
                  "title": "Fine-tuning Methods Comparison",
                  "code": "flowchart LR\n    subgraph Full[\"Full Fine-tuning\"]\n        F1[All Parameters]\n        F2[High Memory]\n        F3[Most Powerful]\n    end\n    \n    subgraph PEFT[\"Parameter Efficient\"]\n        P1[LoRA]\n        P2[QLoRA]\n        P3[Adapters]\n    end\n    \n    subgraph Alignment[\"Alignment\"]\n        A1[RLHF]\n        A2[DPO]\n    end\n    \n    BASE[Base Model] --> Full\n    BASE --> PEFT\n    Full --> Alignment\n    PEFT --> Alignment"
                },
                "code": "# Overview of fine-tuning landscape\nFINETUNING_METHODS = {\n    \"full_finetuning\": {\n        \"description\": \"Update all parameters\",\n        \"memory\": \"Very High (8x model size)\",\n        \"examples\": [\"Standard PyTorch training\"],\n        \"when_to_use\": \"Maximum control, large compute budget\"\n    },\n    \"lora\": {\n        \"description\": \"Low-Rank Adaptation - add small trainable matrices\",\n        \"memory\": \"Low (1-2x model size)\",\n        \"examples\": [\"peft\", \"unsloth\"],\n        \"when_to_use\": \"Most common choice, good balance\"\n    },\n    \"qlora\": {\n        \"description\": \"LoRA with 4-bit quantized base model\",\n        \"memory\": \"Very Low (0.5x model size)\",\n        \"examples\": [\"bitsandbytes + peft\"],\n        \"when_to_use\": \"Consumer GPU, limited memory\"\n    },\n    \"prefix_tuning\": {\n        \"description\": \"Learn virtual prefix tokens\",\n        \"memory\": \"Very Low\",\n        \"examples\": [\"peft PrefixTuning\"],\n        \"when_to_use\": \"Very parameter-efficient, less powerful\"\n    },\n    \"dpo\": {\n        \"description\": \"Direct Preference Optimization\",\n        \"memory\": \"Medium-High\",\n        \"examples\": [\"trl DPOTrainer\"],\n        \"when_to_use\": \"Alignment, preference learning\"\n    }\n}\n\n# Memory requirements for 7B model\ndef estimate_memory(model_size_b: float, method: str) -> str:\n    base_fp16 = model_size_b * 2  # 2 bytes per param in FP16\n    \n    multipliers = {\n        \"full_finetuning\": 8,     # Model + gradients + optimizer states\n        \"lora\": 1.1,              # Base + small adapters\n        \"qlora\": 0.5,             # 4-bit quantized\n        \"inference\": 1.0          # Just model weights\n    }\n    \n    memory_gb = base_fp16 * multipliers.get(method, 1.0)\n    return f\"{method}: ~{memory_gb:.1f} GB VRAM needed\"\n\nfor method in [\"full_finetuning\", \"lora\", \"qlora\"]:\n    print(estimate_memory(7, method))\n# full_finetuning: ~112.0 GB VRAM needed\n# lora: ~15.4 GB VRAM needed  \n# qlora: ~7.0 GB VRAM needed"
              }
            ],
            "keyTakeaways": [
              "Fine-tune for consistent style/format, not for adding knowledge",
              "RAG is better when you need current or extensive factual information",
              "LoRA/QLoRA makes fine-tuning accessible on consumer hardware"
            ],
            "exercises": [
              {
                "title": "Use Case Analysis",
                "description": "Analyze 5 real-world scenarios and recommend fine-tuning vs RAG vs prompting"
              }
            ],
            "sources": [
              {
                "title": "LoRA Paper",
                "author": "Hu et al.",
                "url": "https://arxiv.org/abs/2106.09685"
              },
              {
                "title": "QLoRA Paper",
                "author": "Dettmers et al.",
                "url": "https://arxiv.org/abs/2305.14314"
              },
              {
                "title": "OpenAI Model Optimization Guide",
                "url": "https://platform.openai.com/docs/guides/model-optimization"
              },
              {
                "title": "Hugging Face PEFT Documentation",
                "url": "https://huggingface.co/docs/peft/index"
              }
            ]
          }
        },
        {
          "id": "openai-finetuning",
          "title": "OpenAI API Fine-tuning",
          "duration": "30 min",
          "content": {
            "overview": "If you're using OpenAI's API, you don't need GPUs or PyTorch - OpenAI handles the training infrastructure. This lesson covers the four fine-tuning methods available through their platform.",
            "sections": [
              {
                "title": "The Four OpenAI Fine-tuning Methods",
                "content": "OpenAI offers four distinct approaches, each optimized for different use cases:\n\n| Method | How it Works | Best For | Models |\n|--------|-------------|----------|--------|\n| **SFT** (Supervised) | Provide example prompts with ideal responses | Classification, formatting, style | gpt-4.1, gpt-4.1-mini |\n| **Vision** | Include images in training data | Image classification, visual instructions | gpt-4o |\n| **DPO** (Direct Preference) | Show correct AND incorrect responses | Summarization, tone adjustment | gpt-4.1, gpt-4.1-mini |\n| **RFT** (Reinforcement) | Grade model responses, reinforce good chains-of-thought | Complex domain reasoning, expert tasks | o4-mini |\n\n**The key insight:** OpenAI's fine-tuning doesn't require you to manage GPU memory, quantization, or LoRA - that's all handled on their servers."
              },
              {
                "title": "SFT: Supervised Fine-tuning",
                "content": "The most common approach. You provide examples of what you want the model to say:",
                "code": "from openai import OpenAI\nimport json\n\nclient = OpenAI()\n\n# Step 1: Prepare training data (JSONL format)\ntraining_examples = [\n    {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a customer support assistant for Acme Inc.\"},\n            {\"role\": \"user\", \"content\": \"I can't log in to my account\"},\n            {\"role\": \"assistant\", \"content\": \"I'm sorry to hear you're having trouble logging in! Let me help you with that. First, could you tell me what error message you're seeing? This will help me identify the exact issue.\"}\n        ]\n    },\n    {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a customer support assistant for Acme Inc.\"},\n            {\"role\": \"user\", \"content\": \"How do I cancel my subscription?\"},\n            {\"role\": \"assistant\", \"content\": \"I'd be happy to help you with cancellation. Before we proceed, I want to make sure you're aware that your subscription includes several benefits. Would you like me to explain what you'd be giving up, or should I walk you through the cancellation steps right away?\"}\n        ]\n    }\n]\n\n# Save as JSONL\nwith open('training_data.jsonl', 'w') as f:\n    for example in training_examples:\n        f.write(json.dumps(example) + '\\n')\n\n# Step 2: Upload training file\nfile = client.files.create(\n    file=open('training_data.jsonl', 'rb'),\n    purpose='fine-tune'\n)\nprint(f\"File uploaded: {file.id}\")\n\n# Step 3: Create fine-tuning job\njob = client.fine_tuning.jobs.create(\n    training_file=file.id,\n    model=\"gpt-4.1-2025-04-14\"  # Base model\n)\nprint(f\"Fine-tuning job started: {job.id}\")\n\n# Step 4: Monitor progress\nimport time\nwhile True:\n    status = client.fine_tuning.jobs.retrieve(job.id)\n    print(f\"Status: {status.status}\")\n    if status.status in ['succeeded', 'failed']:\n        break\n    time.sleep(60)\n\n# Step 5: Use the fine-tuned model\nif status.status == 'succeeded':\n    fine_tuned_model = status.fine_tuned_model\n    \n    response = client.chat.completions.create(\n        model=fine_tuned_model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a customer support assistant for Acme Inc.\"},\n            {\"role\": \"user\", \"content\": \"My order hasn't arrived yet\"}\n        ]\n    )\n    print(response.choices[0].message.content)"
              },
              {
                "title": "DPO: Teaching Preferences",
                "content": "Direct Preference Optimization shows the model what you prefer AND what you don't:\n\n```json\n{\n  \"input\": {\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Summarize this article about climate change...\"}\n    ]\n  },\n  \"preferred_output\": [\n    {\"role\": \"assistant\", \"content\": \"Key findings: 1) Global temperatures rose 1.2Â°C... [factual, concise]\"}\n  ],\n  \"non_preferred_output\": [\n    {\"role\": \"assistant\", \"content\": \"Climate change is a very important topic that everyone should care about... [vague, editorialized]\"}\n  ]\n}\n```\n\n**When to use DPO over SFT:**\n- You want to steer *away* from certain behaviors\n- Style/tone is more important than content\n- You have examples of both good AND bad outputs"
              },
              {
                "title": "The OpenAI Optimization Workflow",
                "content": "OpenAI recommends this order:\n\n1. **Write evals first** - Measure current performance\n2. **Try prompt engineering** - Often sufficient\n3. **If prompting fails â†’ Fine-tune** - Use SFT for most cases\n4. **Evaluate again** - Measure improvement\n5. **Iterate** - Improve dataset based on failures\n\n**Key insight from OpenAI docs:** Fine-tuning is for changing *behavior*, not adding *knowledge*. For new facts, use RAG or function calling."
              }
            ],
            "keyTakeaways": [
              "OpenAI handles all the GPU/infrastructure complexity",
              "SFT is best for consistent formatting and style",
              "DPO shows the model what NOT to do",
              "Always try prompting before fine-tuning"
            ],
            "sources": [
              {
                "title": "OpenAI Model Optimization Guide",
                "url": "https://platform.openai.com/docs/guides/model-optimization"
              },
              {
                "title": "OpenAI Supervised Fine-tuning",
                "url": "https://platform.openai.com/docs/guides/supervised-fine-tuning"
              },
              {
                "title": "OpenAI Direct Preference Optimization",
                "url": "https://platform.openai.com/docs/guides/direct-preference-optimization"
              }
            ]
          }
        },
        {
          "id": "rlhf-alignment",
          "title": "RLHF and Alignment Methods",
          "author": "RLHF by Long Ouyang (OpenAI); DPO by Rafael Rafailov et al. (Stanford); Constitutional AI by Yuntao Bai (Anthropic)",
          "duration": "50 min",
          "content": {
            "overview": "Understanding how models are aligned with human preferences is essential for advanced fine-tuning. This lesson covers RLHF (the original alignment technique), its successor DPO, and other modern alignment methods used in production.",
            "sections": [
              {
                "title": "RLHF: The Original Alignment Technique",
                "content": "**RLHF (Reinforcement Learning from Human Feedback)** is how ChatGPT was made helpful and safe. It's a 3-step process:\n\n**Step 1: Supervised Fine-Tuning (SFT)**\n- Train on human-written responses\n- Model learns basic instruction following\n\n**Step 2: Reward Model Training**\n- Collect human comparisons (which response is better?)\n- Train a reward model to predict human preferences\n\n**Step 3: PPO Optimization**\n- Use reinforcement learning (PPO algorithm)\n- Optimize policy to maximize reward model scores\n- Add KL penalty to prevent straying from SFT model\n\n**Why it's complex:**\n- Requires training 3 separate models\n- RL is unstable and hyperparameter-sensitive\n- Expensive in compute and human annotation\n\nThis is why DPO was developed as a simpler alternative.",
                "diagram": {
                  "title": "RLHF Pipeline",
                  "code": "flowchart LR\n    subgraph Step1[\"Step 1: SFT\"]\n        D1[Demonstration Data] --> SFT[SFT Model]\n    end\n    \n    subgraph Step2[\"Step 2: Reward Model\"]\n        P1[Preference Data] --> RM[Reward Model]\n    end\n    \n    subgraph Step3[\"Step 3: RL\"]\n        SFT --> PPO[PPO Training]\n        RM --> PPO\n        PPO --> ALIGNED[Aligned Model]\n    end"
                }
              },
              {
                "title": "DPO: Direct Preference Optimization",
                "content": "**DPO** eliminates the reward model and RL entirely:\n\n**Key insight**: The optimal policy can be derived directly from preferences without explicitly training a reward model.\n\n**The DPO loss:**\n$$\\mathcal{L}_{DPO} = -\\log\\sigma\\left(\\beta \\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)$$\n\nWhere:\n- $y_w$ = preferred (winning) response\n- $y_l$ = rejected (losing) response\n- $\\pi_\\theta$ = model being trained\n- $\\pi_{ref}$ = reference model (frozen SFT)\n- $\\beta$ = temperature controlling deviation from reference\n\n**Benefits over RLHF:**\n- No reward model needed\n- No RL instability\n- Simple classification loss\n- Faster, cheaper training\n- Often equal or better results",
                "code": "from trl import DPOConfig, DPOTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\n\n# Load SFT model as base\nmodel = AutoModelForCausalLM.from_pretrained(\"your-sft-model\")\ntokenizer = AutoTokenizer.from_pretrained(\"your-sft-model\")\n\n# Preference dataset: each example has chosen and rejected\n# Format: {\"prompt\": \"...\", \"chosen\": \"...\", \"rejected\": \"...\"}\ndataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n\n# DPO training\ntraining_args = DPOConfig(\n    output_dir=\"./dpo-model\",\n    beta=0.1,                    # KL penalty strength\n    loss_type=\"sigmoid\",         # Standard DPO loss\n    learning_rate=5e-7,          # Lower than SFT\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    bf16=True,\n    logging_steps=10,\n)\n\ntrainer = DPOTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\n\ntrainer.train()"
              },
              {
                "title": "Beyond DPO: Modern Alignment Variants",
                "content": "The TRL library supports many DPO variants:\n\n**IPO (Identity Preference Optimization)**\n- Addresses DPO overfitting issues\n- Averages log-likelihoods instead of summing\n- Better theoretical grounding\n\n**KTO (Kahneman-Tversky Optimization)**\n- Works with binary labels (good/bad) instead of pairs\n- More efficient data usage\n- Based on prospect theory\n\n**ORPO (Odds Ratio Preference Optimization)**\n- Combines SFT and preference alignment in one step\n- Simpler pipeline\n- No reference model needed\n\n**Which to choose?**\n- DPO: Default, well-tested, great results\n- IPO: If DPO overfits\n- KTO: If you only have thumbs up/down data\n- ORPO: If you want single-stage training",
                "code": "from trl import DPOConfig\n\n# Different loss types in TRL\n\n# Standard DPO\ndpo_config = DPOConfig(\n    loss_type=\"sigmoid\",\n    beta=0.1\n)\n\n# IPO - often more stable\nipo_config = DPOConfig(\n    loss_type=\"ipo\",\n    beta=0.1  # Acts as regularization parameter Ï„\n)\n\n# Robust DPO - handles noisy labels\nrobust_config = DPOConfig(\n    loss_type=\"robust\",\n    label_smoothing=0.1  # Assume 10% label noise\n)\n\n# For KTO, use dedicated KTOTrainer\nfrom trl import KTOConfig, KTOTrainer\n\nkto_config = KTOConfig(\n    output_dir=\"./kto-model\",\n    desirable_weight=1.0,\n    undesirable_weight=1.0,\n)\n\n# ORPO - single-stage alignment\nfrom trl import ORPOConfig, ORPOTrainer\n\norpo_config = ORPOConfig(\n    output_dir=\"./orpo-model\",\n    beta=0.1,  # Controls preference vs SFT balance\n)"
              },
              {
                "title": "The Alignment Tax and Constitutional AI",
                "content": "**Alignment Tax**: The observation that RLHF/alignment can sometimes reduce raw capability.\n\n- Aligned models may be more conservative\n- May refuse valid requests (over-refusal)\n- Trade-off between safety and helpfulness\n\n**Mitigations:**\n- Careful reward model training\n- Balance safety and helpfulness in preferences\n- Use DPO with appropriate Î² (lower = more deviation allowed)\n\n**Constitutional AI (Anthropic)**\nInstead of human feedback, use AI feedback:\n1. Generate responses\n2. Ask AI to critique based on principles (constitution)\n3. Have AI revise responses\n4. Train on AI-improved responses\n\n**Benefits:**\n- Scales better than human annotation\n- Consistent application of principles\n- Can encode complex rules",
                "code": "# Constitutional AI-style self-improvement\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nCONSTITUTION = \"\"\"\nPrinciples:\n1. Be helpful, harmless, and honest\n2. Avoid giving advice that could cause harm\n3. Respect user privacy\n4. Acknowledge uncertainty\n5. Be concise but complete\n\"\"\"\n\ndef generate_and_critique(prompt: str) -> dict:\n    \"\"\"Constitutional AI: Generate, critique, revise.\"\"\"\n    \n    # Step 1: Initial generation\n    initial = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    ).choices[0].message.content\n    \n    # Step 2: Critique based on constitution\n    critique_prompt = f\"\"\"\n    Review this response against these principles:\n    {CONSTITUTION}\n    \n    Response: {initial}\n    \n    List any violations and suggest improvements.\n    \"\"\"\n    \n    critique = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": critique_prompt}]\n    ).choices[0].message.content\n    \n    # Step 3: Revise based on critique\n    revise_prompt = f\"\"\"\n    Original response: {initial}\n    Critique: {critique}\n    \n    Provide an improved response that addresses the critique.\n    \"\"\"\n    \n    revised = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": revise_prompt}]\n    ).choices[0].message.content\n    \n    return {\n        \"prompt\": prompt,\n        \"initial\": initial,\n        \"critique\": critique,\n        \"revised\": revised  # Use as training data\n    }"
              },
              {
                "title": "RFT: Reinforcement Fine-tuning",
                "content": "**OpenAI's RFT** is designed for reasoning models (o1, o3, o4-mini):\n\n**How it works:**\n1. Model generates response with chain-of-thought\n2. Expert grader evaluates the response\n3. Good reasoning chains are reinforced\n4. Bad reasoning chains are discouraged\n\n**When to use RFT:**\n- Complex domain-specific reasoning\n- Tasks requiring multi-step thinking\n- When you have expert graders available\n\n**Key difference from DPO:**\n- DPO works on final outputs\n- RFT reinforces the thinking process\n- Better for tasks where reasoning matters",
                "code": "# RFT conceptual example (OpenAI API handles this internally)\n# This shows the grading flow you'd set up\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n# Training data format for RFT\nrft_training_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Given patient symptoms X, Y, Z and history A, B, what is the most likely diagnosis?\"\n        }\n    ],\n    \"grader\": {\n        # Grader evaluates the reasoning chain and final answer\n        # This requires domain expert graders\n        \"model\": \"your-expert-grader\",\n        \"criteria\": [\n            \"Correct final diagnosis\",\n            \"Sound reasoning steps\",\n            \"Consideration of differential diagnoses\",\n            \"Appropriate follow-up recommendations\"\n        ]\n    }\n}\n\n# RFT is available via OpenAI's fine-tuning API\n# for reasoning models like o4-mini\njob = client.fine_tuning.jobs.create(\n    training_file=\"file-xxx\",\n    model=\"o4-mini-2025-04-16\",\n    method={\n        \"type\": \"reinforcement\",\n        \"grader\": {\n            \"type\": \"model\",\n            \"model\": \"gpt-4o\",\n            \"prompt\": \"Grade this medical diagnosis reasoning...\"\n        }\n    }\n)"
              }
            ],
            "keyTakeaways": [
              "RLHF is the original 3-step alignment (SFT â†’ Reward Model â†’ RL) - complex but powerful",
              "DPO simplifies RLHF to a single classification loss - no reward model or RL needed",
              "Modern variants (IPO, KTO, ORPO) address specific DPO limitations",
              "Constitutional AI uses AI feedback instead of human feedback for scalable alignment",
              "RFT reinforces reasoning chains, not just final outputs"
            ],
            "exercises": [
              {
                "title": "Preference Dataset Creation",
                "description": "Create a preference dataset with 50 examples for a customer support use case, including chosen and rejected responses."
              },
              {
                "title": "DPO vs IPO Comparison",
                "description": "Train the same model with DPO and IPO loss, compare training curves and final evaluation metrics."
              }
            ],
            "sources": [
              {
                "title": "DPO Paper - Direct Preference Optimization",
                "url": "https://arxiv.org/abs/2305.18290"
              },
              {
                "title": "TRL DPO Trainer Documentation",
                "url": "https://huggingface.co/docs/trl/main/en/dpo_trainer"
              },
              {
                "title": "Constitutional AI Paper",
                "url": "https://arxiv.org/abs/2212.08073"
              },
              {
                "title": "OpenAI Reinforcement Fine-tuning",
                "url": "https://platform.openai.com/docs/guides/reinforcement-fine-tuning"
              },
              {
                "title": "KTO Paper - Kahneman-Tversky Optimization",
                "url": "https://arxiv.org/abs/2402.01306"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "lora-qlora",
      "title": "LoRA and QLoRA",
      "lessons": [
        {
          "id": "lora-explained",
          "title": "Understanding LoRA",
          "author": "LoRA paper by Edward J. Hu et al. (Microsoft, 2021)",
          "duration": "45 min",
          "content": {
            "overview": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method that adds small trainable matrices to frozen model weights. Instead of updating millions of parameters, LoRA updates thousandsâ€”dramatically reducing memory and compute requirements.",
            "sections": [
              {
                "title": "The LoRA Intuition",
                "content": "**Key insight**: Weight updates during fine-tuning have low intrinsic rank.\n\nInstead of updating $W$ directly:\n$$W' = W + \\Delta W$$\n\nLoRA decomposes the update as:\n$$W' = W + BA$$\n\nWhere:\n- $W$ is frozen (original weights): $d \\times k$\n- $B$ is trainable: $d \\times r$\n- $A$ is trainable: $r \\times k$\n- $r$ is the rank (typically 8-64)\n\n**Parameter reduction**:\n- Full update: $d \\times k$ parameters\n- LoRA: $r \\times (d + k)$ parameters\n- For $d=k=4096$, $r=16$: 16M â†’ 131K (99.2% reduction!)",
                "code": "import torch\nimport torch.nn as nn\nimport math\n\nclass LoRALayer(nn.Module):\n    \"\"\"LoRA adapter for a linear layer.\"\"\"\n    \n    def __init__(\n        self, \n        in_features: int, \n        out_features: int, \n        rank: int = 8,\n        alpha: float = 16,\n        dropout: float = 0.0\n    ):\n        super().__init__()\n        \n        self.rank = rank\n        self.alpha = alpha\n        self.scaling = alpha / rank\n        \n        # LoRA matrices\n        self.lora_A = nn.Parameter(torch.zeros(in_features, rank))\n        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n        \n        # Initialize A with kaiming, B with zeros\n        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n        nn.init.zeros_(self.lora_B)\n        \n        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x @ A @ B * scaling\n        return self.dropout(x) @ self.lora_A @ self.lora_B * self.scaling\n\nclass LinearWithLoRA(nn.Module):\n    \"\"\"Linear layer with LoRA adapter.\"\"\"\n    \n    def __init__(self, linear: nn.Linear, rank: int = 8, alpha: float = 16):\n        super().__init__()\n        \n        self.linear = linear\n        self.linear.weight.requires_grad = False  # Freeze original\n        if linear.bias is not None:\n            self.linear.bias.requires_grad = False\n        \n        self.lora = LoRALayer(\n            linear.in_features,\n            linear.out_features,\n            rank=rank,\n            alpha=alpha\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Original output + LoRA output\n        return self.linear(x) + self.lora(x)\n\n# Example usage\noriginal_linear = nn.Linear(4096, 4096)\nlora_linear = LinearWithLoRA(original_linear, rank=16)\n\n# Count parameters\ntotal = sum(p.numel() for p in lora_linear.parameters())\ntrainable = sum(p.numel() for p in lora_linear.parameters() if p.requires_grad)\nprint(f\"Total: {total:,}, Trainable: {trainable:,}\")  \n# Total: 16,908,288, Trainable: 131,072"
              },
              {
                "title": "LoRA Hyperparameters",
                "content": "**Rank (r)**: Controls capacity of adaptation\n- Higher rank = more parameters, more expressive\n- Common values: 8, 16, 32, 64\n- Start with 8-16, increase if underfitting\n\n**Alpha (Î±)**: Scaling factor\n- Controls strength of LoRA updates\n- Effective learning rate: Î±/r\n- Common: Î± = 2Ã—r or Î± = r\n\n**Target modules**: Which layers to adapt\n- Attention: q_proj, k_proj, v_proj, o_proj\n- MLP: gate_proj, up_proj, down_proj\n- Start with attention, add MLP if needed",
                "code": "from peft import LoraConfig, get_peft_model, TaskType\nfrom transformers import AutoModelForCausalLM\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,                          # Rank\n    lora_alpha=32,                  # Alpha (scaling = alpha/r = 2)\n    target_modules=[               # Which modules to adapt\n        \"q_proj\",\n        \"k_proj\", \n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\"\n    ],\n    lora_dropout=0.05,             # Dropout for regularization\n    bias=\"none\",                   # Don't train biases\n    task_type=TaskType.CAUSAL_LM   # Task type for model\n)\n\n# Apply LoRA to model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n# trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622"
              },
              {
                "title": "QLoRA: Quantized LoRA",
                "content": "QLoRA combines LoRA with 4-bit quantization:\n\n1. **4-bit NormalFloat (NF4)**: Information-theoretically optimal quantization\n2. **Double Quantization**: Quantize the quantization constants too\n3. **Paged Optimizers**: Handle memory spikes gracefully\n\nThis enables fine-tuning a 65B model on a single 48GB GPU!",
                "code": "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                    # Use 4-bit\n    bnb_4bit_quant_type=\"nf4\",            # NormalFloat4\n    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bf16\n    bnb_4bit_use_double_quant=True         # Double quantization\n)\n\n# Load quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Add LoRA adapters\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Memory usage: ~7GB for 7B model!\nprint(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
              }
            ],
            "keyTakeaways": [
              "LoRA adds small trainable matrices instead of updating all weights",
              "Rank 8-16 is usually sufficient; increase if model underperforms",
              "QLoRA enables fine-tuning large models on consumer GPUs"
            ],
            "exercises": [
              {
                "title": "LoRA from Scratch",
                "description": "Implement LoRA without using the peft library"
              },
              {
                "title": "Rank Ablation",
                "description": "Fine-tune with r=4, 8, 16, 32, 64 and compare performance"
              }
            ],
            "sources": [
              {
                "title": "LoRA: Low-Rank Adaptation",
                "url": "https://arxiv.org/abs/2106.09685"
              },
              {
                "title": "QLoRA Paper",
                "url": "https://arxiv.org/abs/2305.14314"
              },
              {
                "title": "Hugging Face PEFT Library",
                "url": "https://github.com/huggingface/peft"
              },
              {
                "title": "PEFT Adapters Guide (LoRA, AdaLoRA, LoHa)",
                "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter"
              },
              {
                "title": "PEFT + Transformers Integration",
                "url": "https://huggingface.co/docs/transformers/main/en/peft"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "dataset-preparation",
      "title": "Dataset Preparation",
      "lessons": [
        {
          "id": "instruction-datasets",
          "title": "Building Instruction Datasets",
          "author": "Self-Instruct by Yizhong Wang et al. (UW); Alpaca by Stanford NLP",
          "duration": "60 min",
          "content": {
            "overview": "The quality of your fine-tuning dataset is the most important factor for success. A small, high-quality dataset often outperforms a large, noisy one. This lesson covers how to create, format, and validate instruction datasets.",
            "sections": [
              {
                "title": "Instruction Dataset Formats",
                "content": "**Alpaca Format** (most common):\n```json\n{\n  \"instruction\": \"What to do\",\n  \"input\": \"Optional context\",\n  \"output\": \"Expected response\"\n}\n```\n\n**ChatML Format** (for chat models):\n```json\n{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are...\"},\n    {\"role\": \"user\", \"content\": \"Question\"},\n    {\"role\": \"assistant\", \"content\": \"Response\"}\n  ]\n}\n```\n\n**Completion Format** (simple):\n```json\n{\n  \"prompt\": \"Question:\",\n  \"completion\": \"Answer\"\n}\n```",
                "code": "from datasets import Dataset\nimport json\n\n# Alpaca format dataset\nalpaca_data = [\n    {\n        \"instruction\": \"Write a Python function to calculate factorial.\",\n        \"input\": \"\",\n        \"output\": \"\"\"def factorial(n):\n    if n <= 1:\n        return 1\n    return n * factorial(n - 1)\"\"\"\n    },\n    {\n        \"instruction\": \"Explain the concept in simple terms.\",\n        \"input\": \"Recursion in programming\",\n        \"output\": \"Recursion is when a function calls itself to solve a smaller version of the same problem. Like looking up a word in a dictionary and finding it defined using other words you also need to look up.\"\n    }\n]\n\n# Convert to prompt format\ndef format_alpaca(example):\n    if example[\"input\"]:\n        prompt = f\"\"\"### Instruction:\n{example['instruction']}\n\n### Input:\n{example['input']}\n\n### Response:\n{example['output']}\"\"\"\n    else:\n        prompt = f\"\"\"### Instruction:\n{example['instruction']}\n\n### Response:\n{example['output']}\"\"\"\n    return {\"text\": prompt}\n\n# ChatML format\nchat_data = [\n    {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n            {\"role\": \"user\", \"content\": \"Write a factorial function\"},\n            {\"role\": \"assistant\", \"content\": \"def factorial(n):\\n    return 1 if n <= 1 else n * factorial(n-1)\"}\n        ]\n    }\n]\n\ndef format_chatml(example):\n    text = \"\"\n    for msg in example[\"messages\"]:\n        if msg[\"role\"] == \"system\":\n            text += f\"<|im_start|>system\\n{msg['content']}<|im_end|>\\n\"\n        elif msg[\"role\"] == \"user\":\n            text += f\"<|im_start|>user\\n{msg['content']}<|im_end|>\\n\"\n        else:\n            text += f\"<|im_start|>assistant\\n{msg['content']}<|im_end|>\\n\"\n    return {\"text\": text}"
              },
              {
                "title": "Data Quality Guidelines",
                "content": "**Quality > Quantity**: 1000 excellent examples beat 100,000 mediocre ones.\n\n**Guidelines**:\n1. **Clear instructions**: Unambiguous, specific\n2. **High-quality outputs**: Well-formatted, correct, comprehensive\n3. **Diverse tasks**: Cover range of expected use cases\n4. **Consistent format**: Same structure throughout\n5. **No contradictions**: Don't train conflicting behaviors\n\n**Common mistakes**:\n- Mixing styles (formal/casual) without labels\n- Truncated or incomplete responses\n- Incorrect information in outputs\n- Too narrow task distribution",
                "code": "from typing import List, Dict\nimport re\n\ndef validate_dataset(examples: List[Dict]) -> Dict:\n    \"\"\"Validate instruction dataset quality.\"\"\"\n    issues = {\n        \"empty_instruction\": [],\n        \"empty_output\": [],\n        \"too_short\": [],\n        \"too_long\": [],\n        \"duplicates\": [],\n        \"format_issues\": []\n    }\n    \n    seen_instructions = set()\n    \n    for i, ex in enumerate(examples):\n        instruction = ex.get(\"instruction\", \"\")\n        output = ex.get(\"output\", \"\")\n        \n        # Check for empty fields\n        if not instruction.strip():\n            issues[\"empty_instruction\"].append(i)\n        if not output.strip():\n            issues[\"empty_output\"].append(i)\n        \n        # Check length\n        if len(output.split()) < 10:\n            issues[\"too_short\"].append(i)\n        if len(output.split()) > 2000:\n            issues[\"too_long\"].append(i)\n        \n        # Check duplicates\n        instr_norm = instruction.lower().strip()\n        if instr_norm in seen_instructions:\n            issues[\"duplicates\"].append(i)\n        seen_instructions.add(instr_norm)\n        \n        # Check format (example: code blocks for code tasks)\n        if \"code\" in instruction.lower() or \"function\" in instruction.lower():\n            if \"def \" not in output and \"class \" not in output:\n                issues[\"format_issues\"].append(i)\n    \n    # Summary\n    total_issues = sum(len(v) for v in issues.values())\n    return {\n        \"total_examples\": len(examples),\n        \"issues_found\": total_issues,\n        \"issue_details\": issues,\n        \"quality_score\": 1 - (total_issues / len(examples))\n    }\n\n# Example\nresult = validate_dataset(alpaca_data)\nprint(f\"Quality Score: {result['quality_score']:.2%}\")"
              },
              {
                "title": "Synthetic Data Generation",
                "content": "When you don't have enough real data, generate synthetic examples using LLMs:",
                "code": "from openai import OpenAI\nimport json\n\nclient = OpenAI()\n\ndef generate_instruction_pairs(\n    domain: str,\n    task_types: list,\n    n_examples: int = 10\n) -> list:\n    \"\"\"Generate synthetic instruction-response pairs.\"\"\"\n    \n    prompt = f\"\"\"Generate {n_examples} instruction-response pairs for fine-tuning an LLM.\n\nDomain: {domain}\nTask types: {', '.join(task_types)}\n\nRequirements:\n- Instructions should be clear and specific\n- Responses should be high-quality and comprehensive\n- Include diverse examples across task types\n- Format as JSON array with \"instruction\" and \"output\" fields\n\nReturn ONLY valid JSON.\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.8  # More variety\n    )\n    \n    try:\n        data = json.loads(response.choices[0].message.content)\n        return data\n    except json.JSONDecodeError:\n        return []\n\n# Generate coding examples\nsynthetic_data = generate_instruction_pairs(\n    domain=\"Python programming\",\n    task_types=[\"code generation\", \"debugging\", \"code explanation\"],\n    n_examples=5\n)\n\n# Self-instruct: Use model to improve its own training data\ndef augment_example(example: dict) -> list:\n    \"\"\"Generate variations of an example.\"\"\"\n    prompt = f\"\"\"Given this instruction-response pair:\n    \nInstruction: {example['instruction']}\nResponse: {example['output']}\n\nGenerate 3 variations with:\n1. Same intent, different wording\n2. More specific version\n3. Harder/more complex version\n\nReturn as JSON array.\"\"\"\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    return json.loads(response.choices[0].message.content)"
              }
            ],
            "keyTakeaways": [
              "Dataset quality is the #1 factor in fine-tuning success",
              "Use consistent formats (Alpaca or ChatML) throughout",
              "Synthetic data generation can bootstrap your dataset"
            ],
            "exercises": [
              {
                "title": "Dataset Builder",
                "description": "Create a 100-example dataset for a customer service chatbot"
              },
              {
                "title": "Quality Audit",
                "description": "Download an open-source dataset and audit it for quality issues"
              }
            ],
            "sources": [
              {
                "title": "Self-Instruct Paper",
                "url": "https://arxiv.org/abs/2212.10560"
              },
              {
                "title": "LIMA: Less Is More for Alignment",
                "url": "https://arxiv.org/abs/2305.11206"
              },
              {
                "title": "Alpaca Dataset",
                "url": "https://github.com/tatsu-lab/stanford_alpaca"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "training-evaluation",
      "title": "Training and Evaluation",
      "lessons": [
        {
          "id": "training-with-trl",
          "title": "Fine-tuning with TRL",
          "author": "TRL library by Leandro von Werra, Lewis Tunstall et al. (Hugging Face)",
          "duration": "60 min",
          "content": {
            "overview": "The TRL (Transformer Reinforcement Learning) library from Hugging Face provides high-level trainers for fine-tuning LLMs. We'll use SFTTrainer for supervised fine-tuning, the most common approach.",
            "sections": [
              {
                "title": "SFTTrainer Setup",
                "content": "SFTTrainer handles the complexities of LLM training:\n- Packing multiple examples into sequences\n- Gradient accumulation for larger effective batch sizes\n- Gradient checkpointing for memory efficiency\n- Proper loss masking for instruction tuning",
                "code": "from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments\n)\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\nimport torch\n\n# Load model with quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\"  # Faster attention\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Load dataset\ndataset = load_dataset(\"your_dataset\", split=\"train\")\n\n# Format function\ndef format_prompt(example):\n    return f\"\"\"### Instruction:\n{example['instruction']}\n\n### Response:\n{example['output']}<|endoftext|>\"\"\""
              },
              {
                "title": "Training Arguments",
                "content": "Key training hyperparameters for LLM fine-tuning:",
                "code": "training_args = TrainingArguments(\n    output_dir=\"./output\",\n    \n    # Batch size and accumulation\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,  # Effective batch = 4*4 = 16\n    \n    # Learning rate\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    \n    # Training duration\n    num_train_epochs=3,\n    max_steps=-1,  # Or set specific step count\n    \n    # Memory optimization\n    fp16=False,\n    bf16=True,  # Better for modern GPUs\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n    \n    # Logging and saving\n    logging_steps=10,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=3,\n    \n    # Evaluation\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    \n    # Other\n    group_by_length=True,  # Efficient batching\n    report_to=\"wandb\",  # Track with Weights & Biases\n)\n\n# Create trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    args=training_args,\n    formatting_func=format_prompt,\n    max_seq_length=2048,\n    packing=True,  # Pack multiple examples per sequence\n)\n\n# Train\ntrainer.train()\n\n# Save\ntrainer.save_model(\"./final_model\")"
              },
              {
                "title": "Monitoring Training",
                "content": "Track these metrics during training:\n\n**Loss curves**: Should decrease smoothly\n**Learning rate**: Watch warmup and decay\n**Gradient norm**: Spikes indicate instability\n**Memory usage**: Watch for OOM risks",
                "code": "import wandb\nfrom transformers import TrainerCallback\n\nclass CustomCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs:\n            # Log custom metrics\n            if \"loss\" in logs:\n                print(f\"Step {state.global_step}: loss={logs['loss']:.4f}\")\n    \n    def on_step_end(self, args, state, control, **kwargs):\n        # Early stopping if loss explodes\n        if state.log_history and state.log_history[-1].get(\"loss\", 0) > 10:\n            print(\"Loss exploded! Stopping training.\")\n            control.should_training_stop = True\n\n# Add to trainer\ntrainer.add_callback(CustomCallback())\n\n# Manual evaluation during training\ndef generate_sample(model, tokenizer, prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=256,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id\n        )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Test during training\ntest_prompt = \"\"\"### Instruction:\nExplain what a neural network is.\n\n### Response:\"\"\"\n\nprint(generate_sample(model, tokenizer, test_prompt))"
              }
            ],
            "keyTakeaways": [
              "Use SFTTrainer for streamlined instruction fine-tuning",
              "Gradient checkpointing + paged optimizer saves significant memory",
              "Monitor loss curves and sample outputs during training"
            ],
            "exercises": [
              {
                "title": "Fine-tune Mistral",
                "description": "Fine-tune Mistral-7B on a custom dataset using QLoRA"
              },
              {
                "title": "Hyperparameter Search",
                "description": "Compare different learning rates and LoRA ranks"
              }
            ],
            "sources": [
              {
                "title": "TRL Documentation",
                "url": "https://huggingface.co/docs/trl"
              },
              {
                "title": "Fine-tuning LLMs with PEFT",
                "url": "https://huggingface.co/blog/peft"
              },
              {
                "title": "Unsloth - Fast Fine-tuning",
                "url": "https://github.com/unslothai/unsloth"
              }
            ]
          }
        },
        {
          "id": "evaluation-methods",
          "title": "Evaluating Fine-tuned Models",
          "duration": "45 min",
          "content": {
            "overview": "Evaluating fine-tuned LLMs is challenging because traditional metrics often don't capture what matters. This lesson covers multiple evaluation approaches from automated metrics to human evaluation.",
            "sections": [
              {
                "title": "Automated Metrics",
                "content": "**Perplexity**: How surprised the model is by test data\n- Lower is better\n- Only meaningful on held-out data from same distribution\n\n**BLEU/ROUGE**: Text similarity to references\n- Limited for open-ended generation\n- Useful for translation/summarization\n\n**Task-specific accuracy**: Best for structured outputs",
                "code": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom evaluate import load\nimport numpy as np\n\ndef calculate_perplexity(model, tokenizer, texts):\n    \"\"\"Calculate perplexity on a list of texts.\"\"\"\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    \n    for text in texts:\n        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n        \n        with torch.no_grad():\n            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n            loss = outputs.loss\n        \n        total_loss += loss.item() * inputs[\"input_ids\"].size(1)\n        total_tokens += inputs[\"input_ids\"].size(1)\n    \n    avg_loss = total_loss / total_tokens\n    perplexity = np.exp(avg_loss)\n    return perplexity\n\n# ROUGE for summarization\nrouge = load(\"rouge\")\n\ndef evaluate_rouge(predictions, references):\n    results = rouge.compute(\n        predictions=predictions,\n        references=references,\n        use_stemmer=True\n    )\n    return {\n        \"rouge1\": results[\"rouge1\"],\n        \"rouge2\": results[\"rouge2\"],\n        \"rougeL\": results[\"rougeL\"]\n    }\n\n# Task accuracy (for structured outputs)\ndef evaluate_json_accuracy(model, tokenizer, test_cases):\n    \"\"\"Evaluate if model produces valid JSON.\"\"\"\n    correct = 0\n    \n    for case in test_cases:\n        output = generate_sample(model, tokenizer, case[\"prompt\"])\n        \n        try:\n            import json\n            parsed = json.loads(output)\n            if parsed == case[\"expected\"]:\n                correct += 1\n        except:\n            pass\n    \n    return correct / len(test_cases)"
              },
              {
                "title": "LLM-as-Judge",
                "content": "Use a stronger model to evaluate outputs. This correlates well with human judgment and scales better:",
                "code": "from openai import OpenAI\n\nclient = OpenAI()\n\ndef llm_evaluate(\n    instruction: str,\n    response_a: str,\n    response_b: str\n) -> dict:\n    \"\"\"Use GPT-4 to compare two responses.\"\"\"\n    \n    prompt = f\"\"\"You are an expert evaluator. Compare these two responses to the instruction.\n\n## Instruction\n{instruction}\n\n## Response A\n{response_a}\n\n## Response B  \n{response_b}\n\nEvaluate on:\n1. Correctness (0-10)\n2. Helpfulness (0-10)\n3. Clarity (0-10)\n4. Overall preference (A/B/Tie)\n\nProvide scores for both and explain your reasoning.\nFormat as JSON with keys: scores_a, scores_b, preference, reasoning\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0\n    )\n    \n    return json.loads(response.choices[0].message.content)\n\n# Compare base vs fine-tuned\ndef compare_models(base_model, finetuned_model, test_prompts):\n    results = {\"base_wins\": 0, \"finetuned_wins\": 0, \"ties\": 0}\n    \n    for prompt in test_prompts:\n        base_response = generate_sample(base_model, tokenizer, prompt)\n        ft_response = generate_sample(finetuned_model, tokenizer, prompt)\n        \n        eval_result = llm_evaluate(prompt, base_response, ft_response)\n        \n        if eval_result[\"preference\"] == \"A\":\n            results[\"base_wins\"] += 1\n        elif eval_result[\"preference\"] == \"B\":\n            results[\"finetuned_wins\"] += 1\n        else:\n            results[\"ties\"] += 1\n    \n    return results"
              },
              {
                "title": "Benchmark Evaluation",
                "content": "Run on standard benchmarks to check for capability preservation and improvement:",
                "code": "# Using lm-evaluation-harness\nimport subprocess\n\ndef run_benchmarks(model_path: str, tasks: list):\n    \"\"\"\n    Run lm-evaluation-harness benchmarks.\n    \n    Install: pip install lm-eval\n    \"\"\"\n    task_str = \",\".join(tasks)\n    \n    cmd = f\"\"\"\n    lm_eval --model hf \\\n        --model_args pretrained={model_path} \\\n        --tasks {task_str} \\\n        --batch_size 8 \\\n        --output_path ./eval_results\n    \"\"\"\n    \n    subprocess.run(cmd, shell=True)\n\n# Common benchmarks for fine-tuned models\nBENCHMARKS = {\n    \"general\": [\"mmlu\", \"hellaswag\", \"arc_challenge\"],\n    \"reasoning\": [\"gsm8k\", \"math\"],\n    \"coding\": [\"humaneval\", \"mbpp\"],\n    \"instruction_following\": [\"ifeval\"]\n}\n\n# Quick custom benchmark\ndef custom_benchmark(model, tokenizer, test_suite):\n    \"\"\"Run custom evaluation suite.\"\"\"\n    results = {}\n    \n    for category, tests in test_suite.items():\n        correct = 0\n        for test in tests:\n            response = generate_sample(model, tokenizer, test[\"prompt\"])\n            if test[\"check_fn\"](response):\n                correct += 1\n        results[category] = correct / len(tests)\n    \n    return results\n\n# Example test suite\ntest_suite = {\n    \"json_output\": [\n        {\n            \"prompt\": \"Return a JSON with name and age fields.\",\n            \"check_fn\": lambda r: '\"name\"' in r and '\"age\"' in r\n        }\n    ],\n    \"code_generation\": [\n        {\n            \"prompt\": \"Write a Python function to reverse a string.\",\n            \"check_fn\": lambda r: \"def \" in r and \"return\" in r\n        }\n    ]\n}"
              }
            ],
            "keyTakeaways": [
              "Perplexity measures fit but not quality",
              "LLM-as-Judge scales better than human evaluation",
              "Always check benchmark performance to verify no regression"
            ],
            "exercises": [
              {
                "title": "Evaluation Pipeline",
                "description": "Build an end-to-end evaluation pipeline for a fine-tuned model"
              },
              {
                "title": "Human Eval Setup",
                "description": "Design a human evaluation protocol with clear rubrics"
              }
            ],
            "sources": [
              {
                "title": "LM Evaluation Harness",
                "url": "https://github.com/EleutherAI/lm-evaluation-harness"
              },
              {
                "title": "Judging LLM-as-a-Judge",
                "url": "https://arxiv.org/abs/2306.05685"
              },
              {
                "title": "AlpacaEval",
                "url": "https://github.com/tatsu-lab/alpaca_eval"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "deployment",
      "title": "Deployment",
      "lessons": [
        {
          "id": "merging-serving",
          "title": "Merging Adapters and Serving",
          "duration": "45 min",
          "content": {
            "overview": "After fine-tuning, you need to prepare your model for production. This includes merging LoRA weights, quantization for efficient inference, and setting up serving infrastructure.",
            "sections": [
              {
                "title": "Merging LoRA Weights",
                "content": "Merge adapters into base model for simpler deployment:",
                "code": "from peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ndef merge_lora_weights(base_model_path: str, adapter_path: str, output_path: str):\n    \"\"\"Merge LoRA adapters into base model.\"\"\"\n    \n    # Load base model\n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_model_path,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n    \n    # Load and merge adapters\n    model = PeftModel.from_pretrained(base_model, adapter_path)\n    model = model.merge_and_unload()  # Merge weights\n    \n    # Save merged model\n    model.save_pretrained(output_path)\n    \n    # Save tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n    tokenizer.save_pretrained(output_path)\n    \n    print(f\"Merged model saved to {output_path}\")\n    return model\n\n# Merge\nmerged = merge_lora_weights(\n    base_model_path=\"mistralai/Mistral-7B-v0.1\",\n    adapter_path=\"./output/checkpoint-1000\",\n    output_path=\"./merged_model\"\n)\n\n# Alternative: Keep adapters separate (useful for A/B testing)\n# Load base + adapters at inference time"
              },
              {
                "title": "Quantization for Inference",
                "content": "Reduce model size for faster inference:",
                "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom awq import AutoAWQForCausalLM\n\n# GGUF quantization (for llama.cpp)\ndef quantize_to_gguf(model_path: str, output_path: str, quant_type: str = \"q4_k_m\"):\n    \"\"\"\n    Quantize to GGUF format using llama.cpp.\n    \n    quant_types: q4_0, q4_1, q4_k_m, q5_0, q5_k_m, q8_0\n    \"\"\"\n    import subprocess\n    \n    # Convert to GGUF\n    subprocess.run([\n        \"python\", \"llama.cpp/convert.py\",\n        model_path,\n        \"--outfile\", f\"{output_path}/model.gguf\",\n        \"--outtype\", \"f16\"\n    ])\n    \n    # Quantize\n    subprocess.run([\n        \"./llama.cpp/quantize\",\n        f\"{output_path}/model.gguf\",\n        f\"{output_path}/model-{quant_type}.gguf\",\n        quant_type\n    ])\n\n# AWQ quantization (faster than GPTQ)\ndef quantize_awq(model_path: str, output_path: str):\n    model = AutoAWQForCausalLM.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    quant_config = {\n        \"zero_point\": True,\n        \"q_group_size\": 128,\n        \"w_bit\": 4,\n        \"version\": \"GEMM\"\n    }\n    \n    model.quantize(tokenizer, quant_config=quant_config)\n    model.save_quantized(output_path)\n    tokenizer.save_pretrained(output_path)"
              },
              {
                "title": "Serving with vLLM",
                "content": "vLLM provides high-throughput inference with continuous batching:",
                "code": "from vllm import LLM, SamplingParams\n\n# Load model\nllm = LLM(\n    model=\"./merged_model\",\n    tensor_parallel_size=1,  # Number of GPUs\n    dtype=\"float16\",\n    max_model_len=4096,\n    gpu_memory_utilization=0.9\n)\n\n# Sampling parameters\nsampling_params = SamplingParams(\n    temperature=0.7,\n    top_p=0.9,\n    max_tokens=256,\n    stop=[\"### Instruction:\", \"\\n\\n\\n\"]\n)\n\n# Generate\nprompts = [\n    \"### Instruction:\\nWrite a haiku about coding.\\n\\n### Response:\",\n    \"### Instruction:\\nExplain quantum computing.\\n\\n### Response:\"\n]\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    print(f\"Prompt: {output.prompt[:50]}...\")\n    print(f\"Response: {output.outputs[0].text}\")\n    print()\n\n# OpenAI-compatible server\n# Run: python -m vllm.entrypoints.openai.api_server \\\n#        --model ./merged_model --port 8000"
              },
              {
                "title": "Production API with FastAPI",
                "content": "Wrap your model in a production-ready API:",
                "code": "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom vllm import LLM, SamplingParams\nimport uvicorn\n\napp = FastAPI()\n\n# Load model at startup\nllm = None\n\n@app.on_event(\"startup\")\nasync def load_model():\n    global llm\n    llm = LLM(model=\"./merged_model\", dtype=\"float16\")\n\nclass GenerateRequest(BaseModel):\n    instruction: str\n    max_tokens: int = 256\n    temperature: float = 0.7\n\nclass GenerateResponse(BaseModel):\n    response: str\n    tokens_used: int\n\n@app.post(\"/generate\", response_model=GenerateResponse)\nasync def generate(request: GenerateRequest):\n    prompt = f\"\"\"### Instruction:\n{request.instruction}\n\n### Response:\"\"\"\n    \n    sampling_params = SamplingParams(\n        temperature=request.temperature,\n        max_tokens=request.max_tokens,\n        stop=[\"### Instruction:\"]\n    )\n    \n    outputs = llm.generate([prompt], sampling_params)\n    response_text = outputs[0].outputs[0].text.strip()\n    tokens = len(outputs[0].outputs[0].token_ids)\n    \n    return GenerateResponse(\n        response=response_text,\n        tokens_used=tokens\n    )\n\n@app.get(\"/health\")\nasync def health():\n    return {\"status\": \"healthy\"}\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
              }
            ],
            "keyTakeaways": [
              "Merge LoRA weights for simpler deployment",
              "Quantize to 4-bit for 4x memory reduction with minimal quality loss",
              "vLLM provides high-throughput inference with continuous batching"
            ],
            "exercises": [
              {
                "title": "End-to-End Pipeline",
                "description": "Build a complete pipeline from training to deployed API"
              },
              {
                "title": "Benchmark Serving",
                "description": "Compare vLLM vs Text Generation Inference latency and throughput"
              }
            ],
            "sources": [
              {
                "title": "vLLM Documentation",
                "url": "https://docs.vllm.ai/"
              },
              {
                "title": "AWQ Quantization",
                "url": "https://github.com/mit-han-lab/llm-awq"
              },
              {
                "title": "Text Generation Inference",
                "url": "https://github.com/huggingface/text-generation-inference"
              }
            ]
          }
        },
        {
          "id": "llm-finetuning-quiz",
          "title": "LLM Fine-tuning Quiz",
          "type": "quiz",
          "duration": "15 min",
          "questions": [
            {
              "id": "finetune-vs-rag",
              "question": "When is fine-tuning generally preferred over RAG?",
              "options": [
                "When you need the model to know about breaking news from yesterday",
                "When you want to change the model's behavior, style, or format",
                "When you have a very small budget and no GPU",
                "When you need to cite sources accurately"
              ],
              "explanation": "Fine-tuning is best for adapting behavior, style, and format. RAG is better for injecting new knowledge or retrieving specific facts.",
              "correct": 1
            },
            {
              "id": "lora-mechanism",
              "question": "How does LoRA (Low-Rank Adaptation) reduce memory usage during training?",
              "options": [
                "It deletes half the layers of the model",
                "It freezes the pre-trained weights and trains only small rank-decomposition matrices",
                "It uses lower precision (8-bit) for all calculations",
                "It reduces the context window size"
              ],
              "explanation": "LoRA freezes the massive pre-trained weights and injects trainable low-rank matrices, significantly reducing the number of trainable parameters and memory footprint.",
              "correct": 1
            },
            {
              "id": "dataset-quality",
              "question": "What is the most critical factor for a successful instruction fine-tuning dataset?",
              "options": [
                "Quantity (having millions of examples)",
                "Quality (clear instructions and high-quality responses)",
                "Length (having very long prompts)",
                "Complexity (having only difficult math problems)"
              ],
              "explanation": "A small, high-quality dataset (e.g., 1,000 excellent examples) often outperforms a large, noisy dataset. 'Garbage in, garbage out' applies strongly to fine-tuning.",
              "correct": 1
            },
            {
              "id": "catastrophic-forgetting",
              "question": "What is 'Catastrophic Forgetting' in the context of fine-tuning?",
              "options": [
                "When the model runs out of memory",
                "When the model loses its ability to perform tasks it was previously trained on",
                "When the training data is lost due to disk failure",
                "When the model refuses to answer questions"
              ],
              "explanation": "Catastrophic forgetting occurs when a model forgets its general pre-training knowledge while over-optimizing for the specific fine-tuning task.",
              "correct": 1
            },
            {
              "id": "qlora-advantage",
              "question": "What is the main advantage of QLoRA over standard LoRA?",
              "options": [
                "It is faster to train",
                "It allows fine-tuning large models on consumer GPUs by quantizing the base model to 4-bit",
                "It eliminates the need for a GPU entirely",
                "It produces higher accuracy models"
              ],
              "explanation": "QLoRA combines 4-bit quantization of the frozen base model with LoRA, drastically reducing VRAM requirements (e.g., fine-tuning a 65B model on a single 48GB GPU).",
              "correct": 1
            },
            {
              "id": "instruction-tuning",
              "question": "What is instruction tuning in the context of LLM fine-tuning?",
              "options": [
                "Teaching the model to follow a specific programming language syntax",
                "Training the model on instruction-response pairs to improve its ability to follow user instructions",
                "Removing all instructions from the training data",
                "Converting the model to a different architecture"
              ],
              "explanation": "Instruction tuning trains the model on curated instruction-response pairs, teaching it to better understand and follow user instructions across diverse tasks.",
              "correct": 1
            },
            {
              "id": "rank-in-lora",
              "question": "In LoRA, what does the 'rank' (r) parameter control?",
              "options": [
                "The priority of the adapter over the base model",
                "The number of trainable parameters via the dimensionality of the low-rank matrices",
                "The training speed multiplier",
                "The output token limit"
              ],
              "explanation": "The rank parameter determines the dimensionality of the low-rank matrices. Lower rank means fewer trainable parameters but potentially less expressive power; higher rank adds more parameters and capacity.",
              "correct": 1
            },
            {
              "id": "sft-vs-dpo",
              "question": "What is the key difference between SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization)?",
              "options": [
                "SFT requires GPU while DPO runs on CPU",
                "SFT trains on instruction-output pairs, while DPO trains on preference pairs (chosen vs rejected)",
                "DPO is only for image models",
                "SFT is newer than DPO"
              ],
              "explanation": "SFT trains on examples of correct behavior, while DPO uses preference data to learn which responses are better than others, enabling alignment with human preferences.",
              "correct": 1
            },
            {
              "id": "foundation-model-adaptation",
              "question": "According to IBM, what is a key benefit of fine-tuning foundation models?",
              "options": [
                "It completely replaces the original model's knowledge",
                "It allows organizations to specialize pre-trained models for specific tasks without training from scratch",
                "It eliminates the need for any training data",
                "It makes the model smaller automatically"
              ],
              "explanation": "Fine-tuning allows organizations to leverage the broad knowledge of foundation models while adapting them for specialized tasks, domains, or enterprise requirements efficiently.",
              "correct": 1
            },
            {
              "id": "training-data-format",
              "question": "What is the Alpaca format commonly used for in LLM fine-tuning?",
              "options": [
                "Storing model weights",
                "Structuring instruction-tuning datasets with instruction, input, and output fields",
                "Compressing training data",
                "Evaluating model performance"
              ],
              "explanation": "The Alpaca format is a popular structure for instruction-tuning datasets, organizing examples into instruction, optional input, and expected output fields for training.",
              "correct": 1
            },
            {
              "id": "rlhf-steps",
              "question": "What are the three main steps of RLHF (Reinforcement Learning from Human Feedback)?",
              "options": [
                "Pre-training, fine-tuning, evaluation",
                "SFT, reward model training, PPO optimization",
                "Data collection, tokenization, embedding",
                "Prompting, retrieval, generation"
              ],
              "explanation": "RLHF involves three steps: Supervised Fine-Tuning (SFT) on demonstrations, training a reward model on human preferences, and PPO optimization to maximize the reward while staying close to the SFT model.",
              "correct": 1
            },
            {
              "id": "dpo-advantage",
              "question": "What is the main advantage of DPO over traditional RLHF?",
              "options": [
                "DPO uses more training data",
                "DPO eliminates the need for a reward model and RL, using a simple classification loss instead",
                "DPO is only for image models",
                "DPO requires more computational resources"
              ],
              "explanation": "DPO's key innovation is deriving the optimal policy directly from preferences without explicitly training a reward model or using unstable RL algorithms, making it simpler, faster, and more stable.",
              "correct": 1
            },
            {
              "id": "constitutional-ai",
              "question": "What is Constitutional AI?",
              "options": [
                "A legal framework for AI regulations",
                "An alignment approach using AI feedback based on principles instead of human feedback",
                "A type of model architecture",
                "A dataset for training language models"
              ],
              "explanation": "Constitutional AI (from Anthropic) uses AI feedback instead of human feedback to evaluate and improve responses based on a set of principles (constitution), enabling more scalable alignment.",
              "correct": 1
            },
            {
              "id": "alignment-tax",
              "question": "What is the 'alignment tax' in the context of LLM fine-tuning?",
              "options": [
                "A government fee for training AI models",
                "The observation that alignment can sometimes reduce raw model capability",
                "The cost of human annotation",
                "The computational overhead of RLHF"
              ],
              "explanation": "The alignment tax refers to the trade-off where making models safer and more aligned may sometimes reduce their raw capabilities, make them more conservative, or lead to over-refusal of valid requests.",
              "correct": 1
            }
          ],
          "references": {
            "lessonRefs": [
              "when-to-fine-tune",
              "lora-qlora",
              "data-preparation"
            ],
            "externalRefs": [
              {
                "title": "Hugging Face PEFT",
                "url": "https://huggingface.co/docs/peft"
              },
              {
                "title": "Unsloth (Fast Fine-tuning)",
                "url": "https://github.com/unslothai/unsloth"
              }
            ]
          }
        }
      ]
    }
  ]
}