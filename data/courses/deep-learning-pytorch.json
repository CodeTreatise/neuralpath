{
  "id": "deep-learning-pytorch",
  "title": "Deep Learning with PyTorch",
  "description": "Build neural networks from fundamentals to advanced architectures",
  "icon": "ðŸ”¥",
  "level": "intermediate",
  "duration": "8 weeks",
  "totalLessons": 10,
  "validationSources": [
    "https://www.ibm.com/topics/deep-learning",
    "https://www.ibm.com/topics/neural-networks",
    "https://www.ibm.com/topics/backpropagation",
    "https://www.ibm.com/topics/gradient-descent",
    "https://www.ibm.com/topics/convolutional-neural-networks",
    "https://www.ibm.com/topics/recurrent-neural-networks"
  ],
  "keyThinkers": [
    {
      "name": "Frank Rosenblatt",
      "contribution": "Invented the perceptron (1958) - the first trainable neural network",
      "era": "1950s"
    },
    {
      "name": "Rumelhart, Hinton, Williams",
      "contribution": "Popularized backpropagation (1986) - the algorithm that enables training deep networks",
      "era": "1986"
    },
    {
      "name": "Yann LeCun",
      "contribution": "Pioneered CNNs and developed LeNet - revolutionized computer vision",
      "era": "1980s-1990s"
    },
    {
      "name": "Hochreiter & Schmidhuber",
      "contribution": "Invented LSTM (1997) - solved vanishing gradients for sequence modeling",
      "era": "1997"
    },
    {
      "name": "Vaswani et al.",
      "contribution": "Created the Transformer architecture (2017) - 'Attention Is All You Need'",
      "era": "2017"
    },
    {
      "name": "Geoffrey Hinton, Yoshua Bengio, Yann LeCun",
      "contribution": "The 'Godfathers of AI' - Turing Award 2018 for deep learning breakthroughs",
      "era": "2000s-present"
    }
  ],
  "prerequisites": [
    "Python proficiency",
    "NumPy basics",
    "ML fundamentals"
  ],
  "learningOutcomes": [
    "Understand neural network fundamentals and backpropagation",
    "Build and train models using PyTorch",
    "Implement CNNs, RNNs, and Transformers",
    "Apply best practices for training and debugging",
    "Diagnose and fix common neural network training issues",
    "Use transfer learning with pretrained models effectively"
  ],
  "modules": [
    {
      "id": "nn-fundamentals",
      "title": "Neural Network Fundamentals",
      "lessons": [
        {
          "id": "perceptron-mlp",
          "title": "From Perceptrons to Multilayer Networks",
          "duration": "45 min",
          "authorCredits": {
            "keyContributors": ["Frank Rosenblatt", "Marvin Minsky", "Seymour Papert"],
            "historicalNote": "Frank Rosenblatt invented the perceptron in 1958. Minsky and Papert's 1969 book 'Perceptrons' famously proved single-layer limitations (XOR problem), leading to the first 'AI winter'. Multi-layer networks with backpropagation overcame these limitations in the 1980s."
          },
          "content": {
            "overview": "Neural networks are computational models inspired by the brain. Starting from a single perceptron, we build up to multilayer networks capable of learning complex patterns. Understanding these fundamentals is essential for all deep learning.\n\n**Industry Definition**: Deep learning is a subset of machine learning driven by multilayered neural networks (at least 4 hidden layers) whose design is inspired by the structure of the human brain. Deep neural networks are universal approximatorsâ€”it has been proven that for any function, there exists a neural network arrangement that can reproduce it. The power of deep learning comes from its ability to learn hierarchical representations, with earlier layers identifying broader patterns and deeper layers identifying more granular patterns.",
            "sections": [
              {
                "title": "The Perceptron",
                "content": "The perceptron (Rosenblatt, 1958) is the simplest neural networkâ€”a single neuron that computes a weighted sum of inputs and applies an activation function:\n\n$$y = \\sigma(\\sum_{i=1}^{n} w_i x_i + b)$$\n\nWhere:\n- $x_i$ are inputs\n- $w_i$ are learnable weights\n- $b$ is the bias term\n- $\\sigma$ is the activation function\n\n**Limitation**: A single perceptron can only learn linearly separable patterns (it cannot learn XOR).",
                "code": "import numpy as np\n\nclass Perceptron:\n    def __init__(self, n_inputs: int):\n        self.weights = np.random.randn(n_inputs) * 0.01\n        self.bias = 0.0\n    \n    def forward(self, x: np.ndarray) -> float:\n        \"\"\"Compute output for input x.\"\"\"\n        z = np.dot(self.weights, x) + self.bias\n        return 1 if z > 0 else 0  # Step activation\n    \n    def train(self, X: np.ndarray, y: np.ndarray, lr: float = 0.1, epochs: int = 100):\n        \"\"\"Train using perceptron learning rule.\"\"\"\n        for _ in range(epochs):\n            for xi, yi in zip(X, y):\n                prediction = self.forward(xi)\n                error = yi - prediction\n                \n                # Update rule: w = w + lr * error * x\n                self.weights += lr * error * xi\n                self.bias += lr * error\n\n# Training on AND gate\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 0, 0, 1])  # AND truth table\n\np = Perceptron(2)\np.train(X, y)\nprint([p.forward(xi) for xi in X])  # [0, 0, 0, 1]"
              },
              {
                "title": "Multilayer Perceptron (MLP)",
                "content": "Adding hidden layers enables learning non-linear patterns:\n\n**Architecture**:\n- Input layer: Receives features\n- Hidden layers: Learn intermediate representations (where most 'learning' occurs)\n- Output layer: Produces predictions\n\n**Industry Standards**:\n- **Weights**: Control how strongly each input feature influences decisionsâ€”adjusted during training via backpropagation\n- **Biases**: Built-in values that shift the decision threshold, allowing neurons to activate even with weak inputs\n- **Parameters**: Total count of weights + biases (e.g., an 8B parameter LLM has 8 billion weighted connections)\n\n**Key insight**: Each hidden neuron learns a different 'feature detector.' Together, they can approximate any continuous function (Universal Approximation Theorem).",
                "diagram": {
                  "title": "MLP Architecture",
                  "code": "flowchart LR\n    subgraph Input[\"Input Layer\"]\n        I1((xâ‚))\n        I2((xâ‚‚))\n        I3((xâ‚ƒ))\n    end\n    \n    subgraph Hidden[\"Hidden Layer\"]\n        H1((hâ‚))\n        H2((hâ‚‚))\n        H3((hâ‚ƒ))\n        H4((hâ‚„))\n    end\n    \n    subgraph Output[\"Output Layer\"]\n        O1((Å·â‚))\n        O2((Å·â‚‚))\n    end\n    \n    I1 --> H1 & H2 & H3 & H4\n    I2 --> H1 & H2 & H3 & H4\n    I3 --> H1 & H2 & H3 & H4\n    H1 & H2 & H3 & H4 --> O1\n    H1 & H2 & H3 & H4 --> O2"
                },
                "code": "import numpy as np\n\nclass MLP:\n    def __init__(self, layer_sizes: list):\n        \"\"\"layer_sizes: [input_dim, hidden1, hidden2, ..., output_dim]\"\"\"\n        self.weights = []\n        self.biases = []\n        \n        # Initialize weights between layers\n        for i in range(len(layer_sizes) - 1):\n            # Xavier initialization\n            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n            b = np.zeros((1, layer_sizes[i+1]))\n            self.weights.append(w)\n            self.biases.append(b)\n    \n    def relu(self, x):\n        return np.maximum(0, x)\n    \n    def relu_derivative(self, x):\n        return (x > 0).astype(float)\n    \n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_x / exp_x.sum(axis=1, keepdims=True)\n    \n    def forward(self, X):\n        \"\"\"Forward pass through all layers.\"\"\"\n        self.activations = [X]\n        self.z_values = []\n        \n        for i, (W, b) in enumerate(zip(self.weights, self.biases)):\n            z = self.activations[-1] @ W + b\n            self.z_values.append(z)\n            \n            # ReLU for hidden, softmax for output\n            if i < len(self.weights) - 1:\n                a = self.relu(z)\n            else:\n                a = self.softmax(z)\n            \n            self.activations.append(a)\n        \n        return self.activations[-1]"
              },
              {
                "title": "Activation Functions",
                "content": "Activation functions introduce non-linearity. Without them, any network collapses to a linear transformationâ€”this is what distinguishes deep neural networks from linear regression models.\n\n**Industry Terminology**: The activation function is a mathematical operation applied to the output of each neuron. It controls the magnitude of output, keeping values within a specified range, which helps prevent exploding values during forward and backward passes.\n\n**Common activations**:\n\n| Function | Formula | Range | Use Case |\n|----------|---------|-------|----------|\n| ReLU | max(0, x) | [0, âˆž) | Hidden layers (default) |\n| Sigmoid | 1/(1+e^-x) | (0, 1) | Binary output |\n| Tanh | (e^x - e^-x)/(e^x + e^-x) | (-1, 1) | Hidden (centered, better gradient flow) |\n| Softmax | e^xi / Î£e^xj | (0, 1) | Multi-class output (probabilities) |\n| GELU | xÂ·Î¦(x) | (-âˆž, âˆž) | Transformers |",
                "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n\ndef leaky_relu(x, alpha=0.01):\n    return np.where(x > 0, x, alpha * x)\n\n# Plot activations\nx = np.linspace(-5, 5, 100)\n\nplt.figure(figsize=(12, 4))\nfor i, (name, func) in enumerate([\n    ('ReLU', relu), \n    ('Sigmoid', sigmoid), \n    ('GELU', gelu)\n], 1):\n    plt.subplot(1, 3, i)\n    plt.plot(x, func(x))\n    plt.title(name)\n    plt.grid(True)\n    plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n    plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\nplt.tight_layout()\nplt.show()"
              }
            ],
            "keyTakeaways": [
              "Neural networks are compositions of layers of neurons",
              "Hidden layers enable learning non-linear patterns",
              "Activation functions introduce necessary non-linearity"
            ],
            "exercises": [
              {
                "title": "XOR Network",
                "description": "Train a 2-layer MLP to solve the XOR problem (not linearly separable)"
              },
              {
                "title": "Activation Comparison",
                "description": "Compare training with ReLU vs. Sigmoid vs. Tanh on MNIST"
              }
            ],
            "sources": [
              {
                "title": "Deep Learning Book - Ch. 6",
                "author": "Goodfellow et al.",
                "url": "https://www.deeplearningbook.org/contents/mlp.html"
              },
              {
                "title": "Universal Approximation Theorem",
                "url": "https://en.wikipedia.org/wiki/Universal_approximation_theorem"
              },
              {
                "title": "IBM: What is Deep Learning?",
                "url": "https://www.ibm.com/topics/deep-learning"
              },
              {
                "title": "IBM: What is a Neural Network?",
                "url": "https://www.ibm.com/topics/neural-networks"
              }
            ]
          }
        },
        {
          "id": "backpropagation",
          "title": "Backpropagation and Gradient Descent",
          "duration": "60 min",
          "authorCredits": {
            "keyContributors": ["David Rumelhart", "Geoffrey Hinton", "Ronald Williams"],
            "historicalNote": "While backpropagation was discovered multiple times (Werbos 1974, Rumelhart et al. 1986), the 1986 Nature paper by Rumelhart, Hinton, and Williams popularized it, enabling the training of multi-layer networks and launching the modern deep learning era."
          },
          "content": {
            "overview": "Backpropagation is the algorithm that makes neural networks trainable. It efficiently computes gradients of the loss with respect to all weights using the chain rule. Combined with gradient descent, it enables learning from data.\n\n**Industry Definition**: Backpropagation (backward propagation of error) is an elegant method to calculate how changes to any individual weight or bias will affect the accuracy of model predictions. It entails a single end-to-end backwards pass through the network, using the chain rule of calculus to compute the 'gradient' of the loss functionâ€”a vector of partial derivatives for every equation in the network. Remarkably efficient: while a naive approach to compute gradients for 1 million weights would require 1,000,001 forward passes, backpropagation achieves the same in just 2 passes (1 forward + 1 backward).",
            "sections": [
              {
                "title": "The Gradient Descent Algorithm",
                "content": "Gradient descent minimizes a loss function by iteratively moving in the direction of steepest descent:\n\n$$w_{t+1} = w_t - \\eta \\nabla L(w_t)$$\n\nWhere:\n- $w$ are the weights\n- $\\eta$ is the learning rate (step size)\n- $\\nabla L$ is the gradient of the loss\n\n**Industry Standards**:\n- **Learning Rate**: Size of steps taken to reach minimum. High = faster but risks overshooting; Low = precise but slow\n- **Cost/Loss Function**: Measures difference between predicted and actual output. Loss = single example; Cost = average across training set\n- **Point of Convergence**: When slope of cost function is at/near zeroâ€”model stops learning\n\n**Three Types of Gradient Descent**:\n- **Batch GD**: Use full dataset for each update (stable but memory-intensive)\n- **Stochastic GD (SGD)**: Use single sample per update (fast but noisy)\n- **Mini-batch GD**: Use small batchesâ€”best balance of efficiency and stability (most common)",
                "code": "import numpy as np\n\ndef gradient_descent(\n    f, \n    grad_f, \n    x0, \n    learning_rate=0.01, \n    n_iterations=100\n):\n    \"\"\"Minimize f starting from x0.\"\"\"\n    x = x0.copy()\n    history = [x.copy()]\n    \n    for _ in range(n_iterations):\n        gradient = grad_f(x)\n        x = x - learning_rate * gradient\n        history.append(x.copy())\n    \n    return x, history\n\n# Example: Minimize f(x) = x^2\nf = lambda x: x**2\ngrad_f = lambda x: 2*x\n\nx_min, history = gradient_descent(f, grad_f, np.array([5.0]), learning_rate=0.1)\nprint(f\"Minimum at x = {x_min[0]:.4f}\")  # ~0"
              },
              {
                "title": "The Chain Rule",
                "content": "Backpropagation applies the chain rule to compute gradients layer by layer:\n\nFor a network: $y = f_L(f_{L-1}(...f_1(x)))$\n\n$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial f_{L-1}} \\cdot ... \\cdot \\frac{\\partial f_2}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial w_1}$$\n\nWe compute gradients from output to input (backwards), reusing intermediate values from the forward pass.",
                "code": "# Backpropagation for a 2-layer network\nimport numpy as np\n\nclass TwoLayerNet:\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n        self.b1 = np.zeros((1, hidden_dim))\n        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.01\n        self.b2 = np.zeros((1, output_dim))\n    \n    def forward(self, X):\n        # Layer 1\n        self.z1 = X @ self.W1 + self.b1\n        self.a1 = np.maximum(0, self.z1)  # ReLU\n        \n        # Layer 2\n        self.z2 = self.a1 @ self.W2 + self.b2\n        exp_z = np.exp(self.z2 - np.max(self.z2, axis=1, keepdims=True))\n        self.a2 = exp_z / exp_z.sum(axis=1, keepdims=True)  # Softmax\n        \n        return self.a2\n    \n    def backward(self, X, y, learning_rate=0.01):\n        m = X.shape[0]  # Batch size\n        \n        # Output layer gradient (softmax + cross-entropy)\n        dz2 = self.a2.copy()\n        dz2[range(m), y] -= 1\n        dz2 /= m\n        \n        # Gradients for W2, b2\n        dW2 = self.a1.T @ dz2\n        db2 = np.sum(dz2, axis=0, keepdims=True)\n        \n        # Backprop to hidden layer\n        da1 = dz2 @ self.W2.T\n        dz1 = da1 * (self.z1 > 0)  # ReLU derivative\n        \n        # Gradients for W1, b1\n        dW1 = X.T @ dz1\n        db1 = np.sum(dz1, axis=0, keepdims=True)\n        \n        # Update weights\n        self.W2 -= learning_rate * dW2\n        self.b2 -= learning_rate * db2\n        self.W1 -= learning_rate * dW1\n        self.b1 -= learning_rate * db1"
              },
              {
                "title": "Loss Functions",
                "content": "The loss function measures how wrong predictions are:\n\n**Cross-Entropy Loss** (classification):\n$$L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{ic} \\log(\\hat{y}_{ic})$$\n\n**Mean Squared Error** (regression):\n$$L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n\n**Industry Terminology**:\n- **Ground Truth**: The correct/desired output for a given input (from labeled data or original sample in self-supervised learning)\n- **Loss vs Cost vs Error Function**: Generally interchangeable. Loss = error for one example; Cost = average error across training set\n- **Objective Function**: Broader term for any evaluation function to minimize or maximize\n\nThe choice of loss affects training dynamics and what the model optimizes for.",
                "code": "import numpy as np\n\ndef cross_entropy_loss(y_pred, y_true):\n    \"\"\"Cross-entropy for classification.\n    \n    y_pred: (N, C) probabilities\n    y_true: (N,) class indices\n    \"\"\"\n    n_samples = len(y_true)\n    # Clip to avoid log(0)\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    # Select probabilities for correct classes\n    correct_probs = y_pred[range(n_samples), y_true]\n    # Negative log likelihood\n    loss = -np.mean(np.log(correct_probs))\n    return loss\n\ndef mse_loss(y_pred, y_true):\n    \"\"\"Mean Squared Error for regression.\"\"\"\n    return np.mean((y_pred - y_true) ** 2)\n\ndef binary_cross_entropy(y_pred, y_true):\n    \"\"\"Binary cross-entropy.\"\"\"\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    return -np.mean(\n        y_true * np.log(y_pred) + \n        (1 - y_true) * np.log(1 - y_pred)\n    )\n\n# Example\ny_pred = np.array([[0.9, 0.1], [0.2, 0.8], [0.6, 0.4]])\ny_true = np.array([0, 1, 0])\n\nloss = cross_entropy_loss(y_pred, y_true)\nprint(f\"Cross-entropy loss: {loss:.4f}\")  # ~0.27"
              }
            ],
            "keyTakeaways": [
              "Gradient descent iteratively updates weights to minimize loss",
              "Backpropagation uses the chain rule to compute gradients efficiently",
              "Loss function choice depends on the task (classification vs regression)"
            ],
            "exercises": [
              {
                "title": "Manual Backprop",
                "description": "Implement backpropagation for a 3-layer network without any framework"
              },
              {
                "title": "Gradient Checking",
                "description": "Verify your backprop implementation with numerical gradient checking"
              }
            ],
            "sources": [
              {
                "title": "Calculus on Computational Graphs: Backpropagation",
                "url": "https://colah.github.io/posts/2015-08-Backprop/"
              },
              {
                "title": "CS231n Backpropagation",
                "url": "https://cs231n.github.io/optimization-2/"
              },
              {
                "title": "IBM: What is Backpropagation?",
                "url": "https://www.ibm.com/topics/backpropagation"
              },
              {
                "title": "IBM: What is Gradient Descent?",
                "url": "https://www.ibm.com/topics/gradient-descent"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "pytorch-basics",
      "title": "PyTorch Fundamentals",
      "lessons": [
        {
          "id": "tensors-autograd",
          "title": "Tensors and Automatic Differentiation",
          "duration": "45 min",
          "content": {
            "overview": "PyTorch is a deep learning framework built around tensors (multi-dimensional arrays) and automatic differentiation. It provides GPU acceleration and a dynamic computation graph that makes debugging intuitive.",
            "sections": [
              {
                "title": "PyTorch Tensors",
                "content": "Tensors are the fundamental data structure in PyTorchâ€”similar to NumPy arrays but with GPU support and automatic differentiation:",
                "code": "import torch\n\n# Creating tensors\nx = torch.tensor([1, 2, 3, 4])            # From list\ny = torch.zeros(3, 4)                      # 3x4 zeros\nz = torch.randn(2, 3)                      # Random normal\nones = torch.ones(5)                        # All ones\narange = torch.arange(0, 10, 2)            # [0, 2, 4, 6, 8]\n\n# Tensor properties\nprint(f\"Shape: {z.shape}\")                 # torch.Size([2, 3])\nprint(f\"Dtype: {z.dtype}\")                 # torch.float32\nprint(f\"Device: {z.device}\")               # cpu\n\n# Move to GPU\nif torch.cuda.is_available():\n    z_gpu = z.to('cuda')\n    # Or: z_gpu = z.cuda()\n\n# Convert to/from NumPy\nimport numpy as np\nnp_array = z.numpy()                        # Tensor â†’ NumPy\nfrom_np = torch.from_numpy(np_array)        # NumPy â†’ Tensor\n\n# Operations (similar to NumPy)\na = torch.randn(3, 4)\nb = torch.randn(3, 4)\n\nsum_ab = a + b                              # Element-wise add\nprod = a * b                                # Element-wise multiply\nmatmul = a @ b.T                            # Matrix multiply\nmean = a.mean()                             # Mean of all\nsum_dim = a.sum(dim=1)                      # Sum along dimension"
              },
              {
                "title": "Automatic Differentiation (Autograd)",
                "content": "PyTorch automatically computes gradientsâ€”you don't need to implement backpropagation manually:\n\n**Key concept**: Set `requires_grad=True` on tensors you want gradients for. PyTorch tracks all operations and computes gradients on `.backward()`.",
                "code": "import torch\n\n# Create tensors with gradient tracking\nx = torch.tensor([2.0, 3.0], requires_grad=True)\n\n# Forward pass: compute y = x^2 + 2x\ny = x ** 2 + 2 * x\nz = y.sum()  # Scalar output\n\n# Backward pass: compute gradients\nz.backward()\n\n# dy/dx = 2x + 2\nprint(f\"x.grad = {x.grad}\")  # tensor([6., 8.])\n# For x=2: 2*2+2=6, for x=3: 2*3+2=8\n\n# Gradient example with weights\nw = torch.randn(3, 2, requires_grad=True)\nb = torch.zeros(2, requires_grad=True)\nx = torch.randn(5, 3)  # 5 samples, 3 features\n\n# Forward\ny = x @ w + b\nloss = y.pow(2).mean()\n\n# Backward\nloss.backward()\n\nprint(f\"w.grad shape: {w.grad.shape}\")  # (3, 2)\nprint(f\"b.grad shape: {b.grad.shape}\")  # (2,)\n\n# IMPORTANT: Zero gradients before next backward\nw.grad.zero_()\nb.grad.zero_()"
              },
              {
                "title": "Building a Neural Network",
                "content": "PyTorch provides `nn.Module` as the base class for all neural networks:",
                "diagram": {
                  "title": "Simple Neural Network",
                  "code": "flowchart LR\n    Input[Input Layer] --> H1[Hidden Layer 1]\n    H1 --> H2[Hidden Layer 2]\n    H2 --> Output[Output Layer]\n    \n    subgraph Neuron[Neuron Operation]\n        W[Weights] --> Sum((Sum))\n        B[Bias] --> Sum\n        In[Input] --> Sum\n        Sum --> Act[Activation]\n        Act --> Out[Output]\n    end\n    \n    style Input fill:#fff,stroke:#333\n    style H1 fill:#dbeafe,stroke:#2563eb\n    style H2 fill:#dbeafe,stroke:#2563eb\n    style Output fill:#dcfce7,stroke:#16a34a"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleNet(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        \n        # Define layers\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x):\n        # Define forward pass\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)  # No activation (for logits)\n        return x\n\n# Create model\nmodel = SimpleNet(input_dim=784, hidden_dim=256, output_dim=10)\n\n# View architecture\nprint(model)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable: {trainable:,}\")\n\n# Forward pass\nx = torch.randn(32, 784)  # Batch of 32\noutput = model(x)\nprint(f\"Output shape: {output.shape}\")  # (32, 10)"
              }
            ],
            "keyTakeaways": [
              "Tensors are like NumPy arrays with GPU support and autograd",
              "Set requires_grad=True to track gradients automatically",
              "Subclass nn.Module to create custom neural networks"
            ],
            "exercises": [
              {
                "title": "Gradient Exploration",
                "description": "Compute gradients for different mathematical expressions and verify by hand"
              },
              {
                "title": "Custom Network",
                "description": "Build a 4-layer MLP with batch normalization and different activation functions"
              }
            ],
            "sources": [
              {
                "title": "PyTorch Tensors Tutorial",
                "url": "https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
              },
              {
                "title": "Autograd Tutorial",
                "url": "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
              }
            ]
          }
        },
        {
          "id": "training-loop",
          "title": "The Training Loop",
          "duration": "60 min",
          "content": {
            "overview": "The training loop is the core of deep learning: iterate through data, compute predictions, calculate loss, backpropagate gradients, and update weights. Understanding each step deeply is essential for debugging and optimization.",
            "sections": [
              {
                "title": "DataLoaders and Datasets",
                "content": "PyTorch provides utilities for efficient data loading with batching, shuffling, and parallel loading:",
                "code": "import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass CustomDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.FloatTensor(X)\n        self.y = torch.LongTensor(y)\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# Create dataset\nX = np.random.randn(1000, 784)\ny = np.random.randint(0, 10, 1000)\ndataset = CustomDataset(X, y)\n\n# Create DataLoader\ntrain_loader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,       # Shuffle each epoch\n    num_workers=4,      # Parallel data loading\n    pin_memory=True     # Faster GPU transfer\n)\n\n# Iterate through batches\nfor batch_X, batch_y in train_loader:\n    print(f\"Batch X shape: {batch_X.shape}\")  # (32, 784)\n    print(f\"Batch y shape: {batch_y.shape}\")  # (32,)\n    break\n\n# Built-in datasets\nfrom torchvision import datasets, transforms\n\nmnist_train = datasets.MNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transforms.ToTensor()\n)"
              },
              {
                "title": "Optimizers",
                "content": "Optimizers update weights based on gradients. Different optimizers have different properties:\n\n**SGD**: Simple, needs tuning\n**Adam**: Adaptive learning rates, good default\n**AdamW**: Adam with proper weight decay\n**LARS/LAMB**: For large batch training",
                "code": "import torch.optim as optim\n\n# SGD with momentum\noptimizer = optim.SGD(\n    model.parameters(),\n    lr=0.01,\n    momentum=0.9,\n    weight_decay=1e-4  # L2 regularization\n)\n\n# Adam (most common)\noptimizer = optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    betas=(0.9, 0.999),\n    weight_decay=1e-4\n)\n\n# AdamW (better for transformers)\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=1e-4,\n    weight_decay=0.01\n)\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=10,   # Every 10 epochs\n    gamma=0.1       # Multiply LR by 0.1\n)\n\n# Cosine annealing (popular)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=100,      # Total epochs\n    eta_min=1e-6    # Minimum LR\n)"
              },
              {
                "title": "Complete Training Loop",
                "content": "Putting it all together: the standard PyTorch training loop:",
                "diagram": {
                  "title": "Training Loop Flow",
                  "code": "flowchart TD\n    START([Start Epoch]) --> BATCH[Get Next Batch]\n    BATCH --> FWD[Forward Pass]\n    FWD --> LOSS[Compute Loss]\n    LOSS --> BACK[Backward Pass]\n    BACK --> UPDATE[Update Weights]\n    UPDATE --> MORE{More Batches?}\n    MORE -->|Yes| BATCH\n    MORE -->|No| VAL[Validate]\n    VAL --> SCHED[Update LR Scheduler]\n    SCHED --> EPOCH{More Epochs?}\n    EPOCH -->|Yes| START\n    EPOCH -->|No| DONE([Done])"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef train_epoch(model, loader, criterion, optimizer, device):\n    model.train()  # Set to training mode\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for batch_X, batch_y in loader:\n        # Move to device\n        batch_X = batch_X.to(device)\n        batch_y = batch_y.to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        optimizer.step()\n        \n        # Track metrics\n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(batch_y).sum().item()\n        total += batch_y.size(0)\n    \n    return total_loss / len(loader), correct / total\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()  # Set to evaluation mode\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():  # No gradient computation\n        for batch_X, batch_y in loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            \n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(batch_y).sum().item()\n            total += batch_y.size(0)\n    \n    return total_loss / len(loader), correct / total\n\n# Full training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SimpleNet(784, 256, 10).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n\nfor epoch in range(20):\n    train_loss, train_acc = train_epoch(\n        model, train_loader, criterion, optimizer, device\n    )\n    val_loss, val_acc = evaluate(\n        model, val_loader, criterion, device\n    )\n    \n    scheduler.step()\n    \n    print(f\"Epoch {epoch+1}: \"\n          f\"Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n          f\"Val Loss={val_loss:.4f}, Acc={val_acc:.4f}\")"
              }
            ],
            "keyTakeaways": [
              "DataLoaders handle batching, shuffling, and parallel loading",
              "Adam is a good default optimizer; AdamW for transformers",
              "Always use model.train() and model.eval() appropriately"
            ],
            "exercises": [
              {
                "title": "MNIST Classifier",
                "description": "Train a model on MNIST achieving >98% accuracy"
              },
              {
                "title": "Learning Rate Finder",
                "description": "Implement a learning rate finder that suggests optimal LR"
              }
            ],
            "sources": [
              {
                "title": "PyTorch Training Tutorial",
                "url": "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html"
              },
              {
                "title": "Adam Optimizer Paper",
                "url": "https://arxiv.org/abs/1412.6980"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "cnn-architectures",
      "title": "Convolutional Neural Networks",
      "lessons": [
        {
          "id": "cnn-basics",
          "title": "Convolutions and Pooling",
          "duration": "60 min",
          "authorCredits": {
            "keyContributors": ["Yann LeCun", "Kaiming He", "Alex Krizhevsky"],
            "historicalNote": "Yann LeCun developed LeNet in 1989 for digit recognition. Alex Krizhevsky's AlexNet (2012) won ImageNet and sparked the deep learning revolution. Kaiming He's ResNet (2015) introduced skip connections, enabling training of 100+ layer networks."
          },
          "content": {
            "overview": "Convolutional Neural Networks (CNNs) are designed for spatial data like images. They use convolution operations to detect local patterns (edges, textures, shapes) and build up hierarchical representations.\n\n**Industry Definition**: CNNs use three-dimensional data for image classification and object recognition. They have three main layer types: Convolutional (feature detection), Pooling (dimensionality reduction), and Fully-connected (classification). Prior to CNNs, manual time-consuming feature extraction was required. CNNs provide scalable pattern recognition leveraging matrix multiplication. Key hyperparameters: number of filters (output depth), stride (kernel movement distance), and zero-padding (Valid/Same/Full).",
            "sections": [
              {
                "title": "The Convolution Operation",
                "content": "A convolution slides a small filter (kernel) over an image, computing dot products at each position:\n\n**Key concepts**:\n- **Kernel/Feature Detector**: Small learnable filter (e.g., 3Ã—3) that represents part of an image pattern\n- **Stride**: Distance (in pixels) the kernel movesâ€”larger stride yields smaller output\n- **Padding**: Adding zeros around edges (Valid = no padding; Same = output equals input size; Full = increases output)\n- **Feature Map/Activation Map**: Output of a convolution (also called 'convolved feature')\n- **Receptive Field**: The local region of input each output depends on\n\n**Why convolutions work (Industry Principles)**:\n- **Local connectivity**: Each output depends on a local region\n- **Parameter sharing**: Same kernel weights applied everywhere (reduces total parameters)\n- **Translation invariance**: Detects patterns anywhere in image\n- **Feature hierarchy**: Later layers see larger portions, combining low-level features into high-level patterns (e.g., edges â†’ wheels â†’ bicycle)",
                "code": "import torch\nimport torch.nn as nn\n\n# 2D Convolution\nconv = nn.Conv2d(\n    in_channels=3,      # RGB input\n    out_channels=32,    # 32 filters\n    kernel_size=3,      # 3x3 kernel\n    stride=1,           # Move 1 pixel at a time\n    padding=1           # Same size output\n)\n\n# Input: (batch, channels, height, width)\nx = torch.randn(1, 3, 224, 224)\nout = conv(x)\nprint(f\"Output shape: {out.shape}\")  # (1, 32, 224, 224)\n\n# Visualize a learned kernel\nprint(f\"First kernel shape: {conv.weight[0].shape}\")  # (3, 3, 3)\n# This is one 3x3x3 filter that produces one feature map\n\n# Calculate output size:\n# out_size = (in_size + 2*padding - kernel_size) / stride + 1\n# (224 + 2*1 - 3) / 1 + 1 = 224"
              },
              {
                "title": "Pooling Layers",
                "content": "Pooling (downsampling) conducts dimensionality reduction, reducing the number of parameters:\n\n**Industry Benefits**:\n- **Dimensionality reduction**: Fewer parameters downstream, reduced complexity\n- **Translation invariance**: Small shifts don't change output\n- **Larger receptive field**: Each neuron 'sees' more of the input\n- **Overfitting prevention**: Reduced parameters limit risk of overfitting\n- **Efficiency**: Improves computational efficiency\n\n**Types**:\n- **Max Pooling**: Select maximum value in each region (most commonly used)\n- **Average Pooling**: Calculate average value in each region\n- **Global Average Pooling**: Average entire feature map to single value (common at network end)",
                "diagram": {
                  "title": "Max Pooling Operation",
                  "code": "flowchart TB\n    subgraph Input[\"Input (4x4)\"]\n        direction TB\n        R1[\"1  1  2  4\"]\n        R2[\"5  6  7  8\"]\n        R3[\"3  2  1  0\"]\n        R4[\"1  2  3  4\"]\n    end\n    \n    subgraph Output[\"Output (2x2)\"]\n        direction TB\n        O1[\"6  8\"]\n        O2[\"3  4\"]\n    end\n    \n    Input -->|Max Pool 2x2| Output\n    \n    style R1 fill:#fff,stroke:#333\n    style O1 fill:#dbeafe,stroke:#2563eb"
                },
                "code": "import torch.nn as nn\n\n# Max pooling 2x2 with stride 2\nmax_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\nx = torch.randn(1, 32, 224, 224)\nout = max_pool(x)\nprint(f\"After pooling: {out.shape}\")  # (1, 32, 112, 112)\n\n# Average pooling\navg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n\n# Global average pooling (common at end of network)\nglobal_avg = nn.AdaptiveAvgPool2d((1, 1))  # Output 1x1 regardless of input\nx = torch.randn(1, 512, 7, 7)\nout = global_avg(x)\nprint(f\"Global avg: {out.shape}\")  # (1, 512, 1, 1)\n# Then flatten: out.view(out.size(0), -1) â†’ (1, 512)"
              },
              {
                "title": "Building a CNN",
                "content": "A typical CNN stacks conv-ReLU-pool blocks, then flattens to fully connected layers:",
                "code": "import torch\nimport torch.nn as nn\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        \n        # Convolutional layers\n        self.conv_layers = nn.Sequential(\n            # Block 1: 3 -> 32 channels\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # 224 -> 112\n            \n            # Block 2: 32 -> 64 channels\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # 112 -> 56\n            \n            # Block 3: 64 -> 128 channels\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # 56 -> 28\n            \n            # Block 4: 128 -> 256 channels\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n        )\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.classifier(x)\n        return x\n\n# Test\nmodel = SimpleCNN(num_classes=10)\nx = torch.randn(4, 3, 224, 224)\nout = model(x)\nprint(f\"Output: {out.shape}\")  # (4, 10)"
              }
            ],
            "keyTakeaways": [
              "Convolutions detect local patterns with shared weights",
              "Pooling reduces dimensions and adds translation invariance",
              "Stack conv-BN-ReLU-pool blocks, end with global pooling and FC"
            ],
            "exercises": [
              {
                "title": "CIFAR-10 Classifier",
                "description": "Build a CNN achieving >85% accuracy on CIFAR-10"
              },
              {
                "title": "Receptive Field",
                "description": "Calculate the receptive field of a 5-layer CNN and visualize what each layer sees"
              }
            ],
            "sources": [
              {
                "title": "CS231n CNNs",
                "url": "https://cs231n.github.io/convolutional-networks/"
              },
              {
                "title": "CNN Explainer",
                "url": "https://poloclub.github.io/cnn-explainer/"
              },
              {
                "title": "IBM: What are Convolutional Neural Networks?",
                "url": "https://www.ibm.com/topics/convolutional-neural-networks"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "rnn-lstm",
      "title": "Recurrent Neural Networks",
      "lessons": [
        {
          "id": "rnn-fundamentals",
          "title": "RNNs and LSTMs",
          "duration": "60 min",
          "authorCredits": {
            "keyContributors": ["Sepp Hochreiter", "JÃ¼rgen Schmidhuber", "Kyunghyun Cho"],
            "historicalNote": "Hochreiter and Schmidhuber invented the LSTM in 1997, solving the vanishing gradient problem that plagued vanilla RNNs. Cho et al. introduced the simpler GRU in 2014. Both architectures dominated sequence modeling until Transformers (2017)."
          },
          "content": {
            "overview": "Recurrent Neural Networks (RNNs) are designed for sequential data where order matters. They maintain a hidden state that captures information from previous time steps. Long Short-Term Memory (LSTM) networks solve the vanishing gradient problem, enabling learning of long-term dependencies.\n\n**Industry Definition**: RNNs process sequences by maintaining internal memory (hidden state) that captures dependencies across time steps. They're foundational for sequence modeling tasks like language modeling, speech recognition, and time series prediction. While largely replaced by Transformers for NLP, understanding RNNs is essential for grasping the evolution of sequence models.",
            "sections": [
              {
                "title": "Basic RNN",
                "content": "An RNN processes sequences by updating a hidden state at each time step:\n\n$$h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$\n$$y_t = W_{hy} h_t + b_y$$\n\n**Key properties:**\n- Same weights are shared across all time steps\n- Hidden state h_t carries information from previous inputs\n- Can process sequences of any length\n\n**Limitation:** Vanilla RNNs suffer from vanishing gradientsâ€”unable to learn dependencies spanning more than ~10 time steps.",
                "code": "import torch\nimport torch.nn as nn\n\n# Manual RNN cell\nclass RNNCell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.Wxh = nn.Linear(input_size, hidden_size)\n        self.Whh = nn.Linear(hidden_size, hidden_size, bias=False)\n    \n    def forward(self, x, h_prev):\n        # x: (batch, input_size)\n        # h_prev: (batch, hidden_size)\n        h_new = torch.tanh(self.Wxh(x) + self.Whh(h_prev))\n        return h_new\n\n# Using PyTorch's RNN\nrnn = nn.RNN(\n    input_size=32,\n    hidden_size=64,\n    num_layers=2,\n    batch_first=True,\n    dropout=0.2\n)\n\n# Input: (batch, seq_len, input_size)\nx = torch.randn(4, 10, 32)  # Batch of 4, 10 time steps\noutput, h_n = rnn(x)\n\nprint(f\"Output: {output.shape}\")  # (4, 10, 64) - all hidden states\nprint(f\"Final hidden: {h_n.shape}\")  # (2, 4, 64) - final state per layer"
              },
              {
                "title": "LSTM Networks",
                "content": "LSTMs introduce gates to control information flow, solving the vanishing gradient problem:\n\n**Gates:**\n- **Forget gate (f):** What to remove from cell state\n- **Input gate (i):** What new information to add\n- **Output gate (o):** What to output\n- **Cell state (c):** Long-term memory highway\n\n$$f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)$$\n$$i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)$$\n$$\\tilde{c}_t = \\tanh(W_c [h_{t-1}, x_t] + b_c)$$\n$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n$$o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)$$\n$$h_t = o_t \\odot \\tanh(c_t)$$",
                "code": "import torch.nn as nn\n\n# LSTM\nlstm = nn.LSTM(\n    input_size=32,\n    hidden_size=64,\n    num_layers=2,\n    batch_first=True,\n    dropout=0.2,\n    bidirectional=True  # Process sequence both directions\n)\n\nx = torch.randn(4, 10, 32)\noutput, (h_n, c_n) = lstm(x)\n\nprint(f\"Output: {output.shape}\")  # (4, 10, 128) - bidirectional doubles size\nprint(f\"Hidden: {h_n.shape}\")  # (4, 4, 64) - 2 layers * 2 directions\nprint(f\"Cell: {c_n.shape}\")  # (4, 4, 64)\n\n# GRU - simplified LSTM (2 gates instead of 3)\ngru = nn.GRU(\n    input_size=32,\n    hidden_size=64,\n    num_layers=2,\n    batch_first=True\n)\n\noutput, h_n = gru(x)\nprint(f\"GRU Output: {output.shape}\")  # (4, 10, 64)"
              },
              {
                "title": "Sequence Classification Example",
                "content": "Using LSTM for sentiment classification:",
                "code": "import torch\nimport torch.nn as nn\n\nclass SentimentLSTM(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(\n            embed_dim, hidden_dim,\n            num_layers=2,\n            batch_first=True,\n            dropout=0.3,\n            bidirectional=True\n        )\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, x):\n        # x: (batch, seq_len)\n        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n        \n        output, (h_n, c_n) = self.lstm(embedded)\n        \n        # Use last hidden state from both directions\n        # h_n: (num_layers * num_directions, batch, hidden_dim)\n        hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)  # (batch, hidden_dim * 2)\n        \n        out = self.dropout(hidden)\n        out = self.fc(out)\n        return out\n\nmodel = SentimentLSTM(\n    vocab_size=10000,\n    embed_dim=128,\n    hidden_dim=256,\n    num_classes=2\n)\n\n# Test\nx = torch.randint(0, 10000, (4, 50))  # Batch of 4, 50 tokens\nout = model(x)\nprint(f\"Output: {out.shape}\")  # (4, 2)"
              }
            ],
            "keyTakeaways": [
              "RNNs maintain hidden state to capture sequence dependencies",
              "LSTMs use gates to control information flow and solve vanishing gradients",
              "Bidirectional LSTMs capture context from both directions"
            ],
            "exercises": [
              {
                "title": "Character-Level Language Model",
                "description": "Train an LSTM to generate text character by character"
              },
              {
                "title": "Time Series Prediction",
                "description": "Use LSTM to predict stock prices or weather data"
              }
            ],
            "sources": [
              {
                "title": "Understanding LSTM Networks",
                "url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
              },
              {
                "title": "IBM: What are Recurrent Neural Networks?",
                "url": "https://www.ibm.com/topics/recurrent-neural-networks"
              },
              {
                "title": "The Unreasonable Effectiveness of RNNs",
                "url": "https://karpathy.github.io/2015/05/21/rnn-effectiveness/"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "practical-dl",
      "title": "Practical Deep Learning",
      "lessons": [
        {
          "id": "transfer-learning",
          "title": "Transfer Learning & Pretrained Models",
          "duration": "45 min",
          "content": {
            "overview": "Transfer learning leverages knowledge from models trained on large datasets to solve new tasks with less data. It's the most practical approach for real-world deep learning, as training from scratch requires massive datasets and compute.\n\n**Industry Standard**: Almost all modern DL applications use transfer learning. Pretrained models (ImageNet for vision, BERT/GPT for NLP) provide strong starting points that can be fine-tuned on domain-specific data.",
            "sections": [
              {
                "title": "Transfer Learning Strategies",
                "content": "**Feature Extraction**: Freeze pretrained weights, only train new head\n- Use when: Small dataset, similar domain\n- Fast, prevents overfitting\n\n**Fine-tuning**: Unfreeze some/all layers, train with low LR\n- Use when: Larger dataset, different domain\n- Better performance but risk of overfitting\n\n**Progressive Unfreezing**: Gradually unfreeze layers from top to bottom\n- Start with head only, then unfreeze deeper layers\n- Best of both worlds",
                "code": "import torch\nimport torch.nn as nn\nfrom torchvision import models\n\n# Load pretrained ResNet\nmodel = models.resnet50(weights='IMAGENET1K_V2')\n\n# Strategy 1: Feature Extraction\n# Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace classifier head\nnum_features = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Dropout(0.5),\n    nn.Linear(num_features, 256),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(256, 10)  # 10 classes\n)\n\n# Only new layers are trainable\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Trainable params: {trainable:,}\")\n\n# Strategy 2: Fine-tuning (unfreeze last layers)\nfor param in model.layer4.parameters():\n    param.requires_grad = True\n\n# Use different learning rates\noptimizer = torch.optim.Adam([\n    {'params': model.layer4.parameters(), 'lr': 1e-5},\n    {'params': model.fc.parameters(), 'lr': 1e-3}\n])"
              },
              {
                "title": "Pretrained Models for Vision",
                "content": "Common pretrained models and when to use them:\n\n| Model | Params | Accuracy | Speed | Use Case |\n|-------|--------|----------|-------|----------|\n| ResNet-50 | 25M | Good | Fast | General purpose |\n| EfficientNet-B0 | 5M | Great | Fast | Mobile/efficient |\n| ViT-Base | 86M | Excellent | Slow | High accuracy |\n| ConvNeXt | 89M | Excellent | Medium | Modern CNN |\n\n**Rule of thumb**: Start with ResNet-50 or EfficientNet, upgrade to ViT if you have enough data and compute.",
                "code": "import torchvision.models as models\n\n# Various pretrained models\nresnet = models.resnet50(weights='IMAGENET1K_V2')\nefficientnet = models.efficientnet_b0(weights='IMAGENET1K_V1')\nvit = models.vit_b_16(weights='IMAGENET1K_V1')\nconvnext = models.convnext_tiny(weights='IMAGENET1K_V1')\n\n# Get classifier for each\nprint(f\"ResNet fc: {resnet.fc.in_features}\")  # 2048\nprint(f\"EfficientNet classifier: {efficientnet.classifier[-1].in_features}\")  # 1280\nprint(f\"ViT head: {vit.heads.head.in_features}\")  # 768\n\n# Replace heads for your task\nresnet.fc = nn.Linear(2048, 10)\nefficientnet.classifier[-1] = nn.Linear(1280, 10)\nvit.heads.head = nn.Linear(768, 10)"
              },
              {
                "title": "Data Augmentation",
                "content": "Data augmentation artificially increases training data variety, reducing overfitting:\n\n**Common augmentations:**\n- Random crop, flip, rotation\n- Color jitter, normalization\n- Cutout, Mixup, CutMix (advanced)",
                "code": "from torchvision import transforms\n\n# Training transforms (with augmentation)\ntrain_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# Validation transforms (no augmentation)\nval_transforms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# Advanced: Mixup augmentation\ndef mixup(x, y, alpha=0.2):\n    lam = torch.distributions.Beta(alpha, alpha).sample()\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size)\n    \n    mixed_x = lam * x + (1 - lam) * x[index]\n    y_a, y_b = y, y[index]\n    \n    return mixed_x, y_a, y_b, lam"
              }
            ],
            "keyTakeaways": [
              "Transfer learning is essential for practical deep learning applications",
              "Feature extraction is fast and safe; fine-tuning gives better results",
              "Use different learning rates for pretrained and new layers"
            ],
            "exercises": [
              {
                "title": "Custom Image Classifier",
                "description": "Fine-tune ResNet-50 on a custom dataset (e.g., flowers, pets)"
              },
              {
                "title": "Compare Strategies",
                "description": "Compare feature extraction vs fine-tuning on the same dataset"
              }
            ],
            "sources": [
              {
                "title": "Transfer Learning Tutorial",
                "url": "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
              },
              {
                "title": "fast.ai: Practical Deep Learning",
                "url": "https://course.fast.ai/"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "transformers",
      "title": "Transformers",
      "lessons": [
        {
          "id": "attention-mechanism",
          "title": "The Attention Mechanism",
          "duration": "60 min",
          "content": {
            "overview": "The attention mechanism, introduced in 'Attention Is All You Need' (Vaswani et al., 2017), revolutionized deep learning. It allows models to focus on relevant parts of the input, enabling parallel processing and long-range dependencies.\n\n**Industry Context**: Unlike RNNs which suffer from vanishing gradients and sequential processing limitations, transformers use only attention layers and standard feedforward layers. The self-attention mechanism enables transformers to discern relationships (dependencies) between each part of an input sequence and selectively focus on the most relevant parts. This architecture powers modern LLMs for NLP, and vision transformers (ViTs) for computer vision, though CNNs remain faster for real-time image tasks.",
            "sections": [
              {
                "title": "Self-Attention",
                "content": "Self-attention computes a weighted sum of all positions, where weights depend on similarity:\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\n**Steps**:\n1. Project input to Query (Q), Key (K), Value (V)\n2. Compute attention scores: Q @ K.T\n3. Scale by âˆšd_k (prevents large values)\n4. Apply softmax (normalize to probabilities)\n5. Weighted sum of Values",
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.embed_dim = embed_dim\n        \n        # Linear projections for Q, K, V\n        self.query = nn.Linear(embed_dim, embed_dim)\n        self.key = nn.Linear(embed_dim, embed_dim)\n        self.value = nn.Linear(embed_dim, embed_dim)\n    \n    def forward(self, x):\n        # x: (batch, seq_len, embed_dim)\n        batch_size, seq_len, _ = x.shape\n        \n        # Project to Q, K, V\n        Q = self.query(x)  # (batch, seq_len, embed_dim)\n        K = self.key(x)\n        V = self.value(x)\n        \n        # Compute attention scores\n        scores = Q @ K.transpose(-2, -1)  # (batch, seq_len, seq_len)\n        scores = scores / math.sqrt(self.embed_dim)  # Scale\n        \n        # Softmax for attention weights\n        attn_weights = F.softmax(scores, dim=-1)\n        \n        # Weighted sum of values\n        output = attn_weights @ V  # (batch, seq_len, embed_dim)\n        \n        return output, attn_weights\n\n# Example\nattention = SelfAttention(embed_dim=256)\nx = torch.randn(2, 10, 256)  # Batch of 2, 10 tokens\nout, weights = attention(x)\nprint(f\"Output: {out.shape}\")  # (2, 10, 256)\nprint(f\"Weights: {weights.shape}\")  # (2, 10, 10)"
              },
              {
                "title": "Multi-Head Attention",
                "content": "Multiple attention heads allow the model to focus on different aspects simultaneously:\n\n- Each head learns different attention patterns\n- Heads are computed in parallel\n- Outputs are concatenated and projected",
                "code": "class MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n    \n    def forward(self, x, mask=None):\n        batch_size, seq_len, _ = x.shape\n        \n        # Project and reshape to (batch, num_heads, seq_len, head_dim)\n        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Attention scores\n        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.head_dim)\n        \n        # Apply mask (for causal attention)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        \n        # Apply attention to values\n        out = attn_weights @ V  # (batch, heads, seq, head_dim)\n        \n        # Concatenate heads\n        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n        \n        # Final projection\n        return self.out_proj(out)\n\n# Example\nmha = MultiHeadAttention(embed_dim=256, num_heads=8)\nx = torch.randn(2, 10, 256)\nout = mha(x)\nprint(f\"Output: {out.shape}\")  # (2, 10, 256)"
              },
              {
                "title": "Transformer Block",
                "content": "A Transformer block combines attention with feedforward layers and residual connections:",
                "code": "class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n        super().__init__()\n        \n        # Multi-head attention\n        self.attention = MultiHeadAttention(embed_dim, num_heads)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        \n        # Feedforward network\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, ff_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(ff_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Self-attention with residual\n        attn_out = self.attention(self.norm1(x), mask)\n        x = x + self.dropout(attn_out)\n        \n        # Feedforward with residual\n        ff_out = self.ff(self.norm2(x))\n        x = x + ff_out\n        \n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, max_len=512):\n        super().__init__()\n        \n        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n        self.position_embedding = nn.Embedding(max_len, embed_dim)\n        \n        self.layers = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, ff_dim)\n            for _ in range(num_layers)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, vocab_size)\n    \n    def forward(self, x):\n        batch_size, seq_len = x.shape\n        \n        # Embeddings\n        positions = torch.arange(seq_len, device=x.device)\n        x = self.token_embedding(x) + self.position_embedding(positions)\n        \n        # Causal mask for autoregressive generation\n        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))\n        \n        # Transformer layers\n        for layer in self.layers:\n            x = layer(x, mask)\n        \n        x = self.norm(x)\n        return self.head(x)"
              }
            ],
            "keyTakeaways": [
              "Attention computes weighted sums based on similarity between tokens",
              "Multi-head attention enables focusing on multiple aspects",
              "Transformer blocks add feedforward networks and residual connections"
            ],
            "exercises": [
              {
                "title": "Attention Visualization",
                "description": "Visualize attention patterns on a sentence and interpret what the model focuses on"
              },
              {
                "title": "Mini GPT",
                "description": "Implement a small GPT-style model and train it on a text dataset"
              }
            ],
            "sources": [
              {
                "title": "Attention Is All You Need",
                "url": "https://arxiv.org/abs/1706.03762"
              },
              {
                "title": "The Illustrated Transformer",
                "url": "https://jalammar.github.io/illustrated-transformer/"
              },
              {
                "title": "Andrej Karpathy - Let's build GPT",
                "url": "https://www.youtube.com/watch?v=kCc8FmEb1nY"
              }
            ]
          },
          "authorCredits": {
            "researchers": ["Vaswani", "Shazeer", "Parmar", "Uszkoreit", "Jones", "Gomez", "Kaiser", "Polosukhin"],
            "note": "The Transformer architecture was introduced by Vaswani et al. at Google Brain/Google Research in 'Attention Is All You Need' (NeurIPS 2017). This paper eliminated recurrence entirely, using only attention mechanisms for sequence modeling, achieving SOTA results in machine translation with massive parallelization. The architecture became the foundation for GPT, BERT, T5, and modern LLMs."
          }
        },
        {
          "id": "dl-quiz",
          "title": "Deep Learning Quiz",
          "type": "quiz",
          "duration": "15 min",
          "questions": [
            {
              "question": "What is the purpose of an activation function in a neural network?",
              "options": [
                "To initialize weights",
                "To introduce non-linearity, allowing the network to learn complex patterns",
                "To reduce the learning rate",
                "To normalize the input data"
              ],
              "correct": 1,
              "explanation": "Without non-linear activation functions, a neural network of any depth would behave like a single linear layer, unable to solve non-linear problems like XOR."
            },
            {
              "question": "Which algorithm is used to compute gradients in a neural network?",
              "options": [
                "Forward Propagation",
                "Backpropagation",
                "K-Means Clustering",
                "Random Forest"
              ],
              "correct": 1,
              "explanation": "Backpropagation uses the chain rule of calculus to efficiently compute the gradient of the loss function with respect to each weight in the network."
            },
            {
              "question": "What does the 'learning rate' control in gradient descent?",
              "options": [
                "The number of layers",
                "The size of the step taken towards the minimum of the loss function",
                "The batch size",
                "The number of epochs"
              ],
              "correct": 1,
              "explanation": "The learning rate determines how much the weights are updated during each step. Too high can cause divergence; too low can result in slow convergence."
            },
            {
              "question": "Which loss function is typically used for multi-class classification problems?",
              "options": [
                "Mean Squared Error (MSE)",
                "Cross-Entropy Loss",
                "Hinge Loss",
                "Absolute Error"
              ],
              "correct": 1,
              "explanation": "Cross-Entropy Loss measures the difference between the predicted probability distribution and the true distribution, making it ideal for classification."
            },
            {
              "question": "What is the 'vanishing gradient' problem?",
              "options": [
                "When gradients become too large and explode",
                "When gradients become extremely small during backprop, preventing early layers from learning",
                "When the loss function reaches zero",
                "When the dataset is too small"
              ],
              "correct": 1,
              "explanation": "In deep networks, gradients can diminish exponentially as they are propagated back through layers (especially with Sigmoid/Tanh), stopping earlier layers from updating."
            },
            {
              "question": "What distinguishes deep learning from traditional machine learning?",
              "options": [
                "Use of Python instead of R",
                "Models with at least 4 hidden layers that can learn hierarchical representations",
                "Faster training times",
                "Smaller datasets required"
              ],
              "correct": 1,
              "explanation": "Deep learning refers to training neural networks with at least 4 layers, enabling hierarchical feature learning where earlier layers identify broader patterns and deeper layers identify more granular ones."
            },
            {
              "question": "What are the three main types of layers in a CNN?",
              "options": [
                "Input, Hidden, Output",
                "Convolutional, Pooling, Fully-connected",
                "Attention, Feedforward, Normalization",
                "Encoder, Decoder, Classifier"
              ],
              "correct": 1,
              "explanation": "CNNs use Convolutional layers for feature detection, Pooling layers for dimensionality reduction, and Fully-connected layers for final classification."
            },
            {
              "question": "Which gradient descent variant is most commonly used in practice?",
              "options": [
                "Batch Gradient Descent",
                "Stochastic Gradient Descent",
                "Mini-batch Gradient Descent",
                "Full-dataset Gradient Descent"
              ],
              "correct": 2,
              "explanation": "Mini-batch GD balances computational efficiency of batch GD with the faster convergence of stochastic GD, making it the standard for training deep networks."
            },
            {
              "question": "What problem did LSTM networks solve compared to standard RNNs?",
              "options": [
                "Slower training speed",
                "The vanishing gradient problem and inability to learn long-term dependencies",
                "Too many parameters",
                "Lack of non-linearity"
              ],
              "correct": 1,
              "explanation": "LSTM networks introduced gates (forget, input, output) that control information flow, enabling learning of long-term dependencies that standard RNNs cannot capture."
            },
            {
              "question": "What is the key innovation of the Transformer architecture over RNNs?",
              "options": [
                "Fewer parameters",
                "Self-attention mechanism enabling parallel processing and long-range dependencies",
                "Faster sequential processing",
                "No need for training data"
              ],
              "correct": 1,
              "explanation": "Transformers replace recurrence with self-attention, which can capture relationships between any positions in a sequence simultaneously, enabling parallelization and better long-range dependency modeling."
            }
          ],
          "references": {
            "lessonRefs": [
              "neural-network-basics",
              "backpropagation",
              "pytorch-tensors"
            ],
            "externalRefs": [
              {
                "title": "PyTorch Tutorials",
                "url": "https://pytorch.org/tutorials/"
              },
              {
                "title": "Fast.ai Course",
                "url": "https://course.fast.ai/"
              }
            ]
          }
        }
      ]
    }
  ]
}