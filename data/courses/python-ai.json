{
  "id": "python-ai",
  "title": "Python for AI & Machine Learning",
  "description": "Master Python from fundamentals to advanced AI programming",
  "icon": "ðŸ",
  "level": "beginner",
  "duration": "8 weeks",
  "prerequisites": [
    "No programming experience required!",
    "A computer with internet access",
    "Curiosity to learn"
  ],
  "whatYouNeed": [
    "A computer (Windows, Mac, or Linux)",
    "Internet connection to install Python",
    "No prior coding experience needed!",
    "30-60 minutes per day to practice"
  ],
  "validationSources": [
    {
      "title": "IBM: What is Machine Learning?",
      "url": "https://www.ibm.com/topics/machine-learning",
      "topics": ["ML types", "supervised learning", "feature engineering"]
    },
    {
      "title": "IBM: What is Scikit-Learn?",
      "url": "https://www.ibm.com/topics/scikit-learn",
      "topics": ["preprocessing", "fit/predict pattern", "metrics"]
    },
    {
      "title": "IBM: What is Data Science?",
      "url": "https://www.ibm.com/topics/data-science",
      "topics": ["data lifecycle", "Python tools", "data cleaning"]
    }
  ],
  "learningOutcomes": [
    "Write efficient Python code for data manipulation",
    "Use NumPy and Pandas for numerical computing",
    "Visualize data and model performance with Matplotlib & Seaborn",
    "Implement machine learning algorithms from scratch",
    "Handle errors, debug code, and write production-ready Python",
    "Understand Python's limitations and when to use alternatives",
    "Build end-to-end AI projects"
  ],
  "modules": [
    {
      "id": "getting-started",
      "title": "Getting Started with Python",
      "lessons": [
        {
          "id": "py-first-steps",
          "title": "Your First Steps with Python",
          "duration": "20 min",
          "content": {
            "overview": "Welcome! This lesson will get you from zero to running Python code. No prior experience needed.",
            "sections": [
              {
                "title": "What is Python?",
                "content": "Python is a **programming language** - a way to give instructions to computers.\n\nThink of it like:\n- English is for talking to humans\n- Python is for talking to computers\n\n**Why Python for AI?**\n- It's easy to read (looks almost like English)\n- It's free and works on all computers\n- All AI tools are built for Python\n- You can see results instantly"
              },
              {
                "title": "Installing Python",
                "content": "**Option 1: Google Colab (Easiest - No Install!)**\n1. Go to [colab.research.google.com](https://colab.research.google.com)\n2. Click 'New Notebook'\n3. Start typing code!\n\n**Option 2: Install on Your Computer**\n1. Go to [python.org/downloads](https://www.python.org/downloads/)\n2. Download Python 3.11 or later\n3. Run the installer\n4. âœ… Check 'Add Python to PATH'\n5. Click Install\n\n**Option 3: Anaconda (Best for AI)**\n1. Go to [anaconda.com/download](https://www.anaconda.com/download)\n2. Download and install\n3. Open 'Anaconda Navigator'\n4. Launch 'Jupyter Notebook'"
              },
              {
                "title": "Your First Python Code",
                "content": "Let's write our first code! Type this and press Enter (or Shift+Enter in Jupyter):",
                "code": "# This is a comment - Python ignores it\n# Comments explain what code does\n\nprint(\"Hello, AI World!\")\n\n# Congratulations! You just:\n# 1. Used the print() function\n# 2. Created a text string with quotes\n# 3. Ran Python code!\n\n# Try changing the message:\nprint(\"I'm learning Python for AI!\")"
              },
              {
                "title": "Common Beginner Questions",
                "content": "**Q: What if I get an error?**\nA: That's normal! Read the error message - it tells you what's wrong. Usually it's a typo.\n\n**Q: Do I need to understand everything?**\nA: No! Start by copying and modifying code. Understanding comes with practice.\n\n**Q: How long to learn Python?**\nA: You can write useful AI code in weeks. Mastery takes years (for everyone!).\n\n**Q: Do I need math?**\nA: Basic arithmetic helps. Libraries handle complex math for you."
              }
            ],
            "keyTakeaways": [
              "Python is a beginner-friendly language used for AI",
              "You can start coding in 5 minutes with Google Colab",
              "print() shows output - your first function!",
              "Errors are normal and help you learn"
            ]
          }
        },
        {
          "id": "py-basics",
          "title": "Python Basics: Variables, Types, and Operations",
          "duration": "30 min",
          "content": {
            "overview": "Learn the building blocks of Python: storing data, doing math, and working with text.",
            "sections": [
              {
                "title": "Variables: Storing Information",
                "content": "A **variable** is a name for a piece of data. Like a labeled box:",
                "code": "# Create variables with = (assignment)\nname = \"Alice\"          # Text (string)\nage = 25                 # Whole number (integer)\nheight = 5.6             # Decimal number (float)\nis_student = True        # Yes/No (boolean)\n\n# Use variables\nprint(name)              # Alice\nprint(age + 5)           # 30\nprint(f\"{name} is {age} years old\")  # Alice is 25 years old"
              },
              {
                "title": "Math Operations",
                "content": "Python works like a calculator:",
                "code": "# Basic math\nadd = 5 + 3        # 8\nsubtract = 10 - 4  # 6\nmultiply = 6 * 7   # 42\ndivide = 15 / 3    # 5.0 (always gives decimal)\n\n# More operations\npower = 2 ** 10    # 1024 (2 to the power of 10)\nfloor_div = 17 // 5  # 3 (divide and round down)\nremainder = 17 % 5   # 2 (what's left over)\n\nprint(f\"2^10 = {power}\")"
              },
              {
                "title": "Working with Text (Strings)",
                "content": "Text in Python is called a **string**:",
                "code": "# Create strings with quotes\ngreeting = \"Hello\"\nname = 'World'  # Single or double quotes work\n\n# Combine strings\nmessage = greeting + \", \" + name + \"!\"  # Hello, World!\n\n# f-strings (modern way - use these!)\nage = 25\nbio = f\"I am {age} years old\"  # I am 25 years old\n\n# Useful string operations\ntext = \"  hello world  \"\nprint(text.strip())      # Remove spaces: \"hello world\"\nprint(text.upper())      # HELLO WORLD\nprint(text.split())      # ['hello', 'world']\nprint(len(text))         # 15 (length)"
              },
              {
                "title": "Lists: Collections of Data",
                "content": "A **list** stores multiple items in order. Very important for AI!",
                "code": "# Create a list with square brackets\nscores = [85, 92, 78, 95, 88]\nnames = [\"Alice\", \"Bob\", \"Charlie\"]\nmixed = [1, \"hello\", 3.14, True]  # Can mix types\n\n# Access items (counting starts at 0!)\nprint(scores[0])     # 85 (first item)\nprint(scores[-1])    # 88 (last item)\nprint(scores[1:3])   # [92, 78] (slice)\n\n# Modify lists\nscores.append(100)   # Add to end\nscores[0] = 90       # Change first item\n\n# List info\nprint(len(scores))   # 6 (how many items)\nprint(sum(scores))   # Total\nprint(max(scores))   # Highest"
              }
            ],
            "keyTakeaways": [
              "Variables store data with meaningful names",
              "Python has numbers (int, float), text (str), and yes/no (bool)",
              "Lists hold multiple items - essential for datasets",
              "Indexing starts at 0, not 1!"
            ]
          }
        }
      ]
    },
    {
      "id": "py-fundamentals",
      "title": "Python Fundamentals",
      "lessons": [
        {
          "id": "py-intro",
          "title": "Introduction to Python for AI",
          "duration": "30 min",
          "content": {
            "overview": "Python has become the dominant language for AI and machine learning due to its simplicity, readability, and extensive ecosystem of libraries. Created by Guido van Rossum in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.",
            "sections": [
              {
                "title": "Why Python for AI?",
                "content": "Python dominates AI development for several key reasons:\n\n1. **Readable Syntax**: Python's clean syntax allows researchers to focus on algorithms rather than language complexities\n2. **Rich Ecosystem**: Libraries like NumPy, Pandas, TensorFlow, and PyTorch provide optimized implementations\n3. **Community Support**: Millions of developers contribute to open-source AI tools\n4. **Rapid Prototyping**: Quick iteration from idea to working prototype\n5. **Integration**: Easy binding to C/C++ for performance-critical code",
                "code": "# Python's readable syntax\ndef train_model(data, epochs=10):\n    \"\"\"Train a simple model on data.\"\"\"\n    for epoch in range(epochs):\n        loss = compute_loss(data)\n        update_weights(loss)\n        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n    return model",
                "diagram": {
                  "title": "Python AI Ecosystem",
                  "code": "flowchart TB\n    PY[Python] --> DS[Data Science]\n    PY --> ML[Machine Learning]\n    PY --> DL[Deep Learning]\n    \n    DS --> NP[NumPy]\n    DS --> PD[Pandas]\n    \n    ML --> SK[Scikit-learn]\n    ML --> XG[XGBoost]\n    \n    DL --> PT[PyTorch]\n    DL --> TF[TensorFlow]"
                }
              },
              {
                "title": "Setting Up Your Environment",
                "content": "For AI development, we recommend using virtual environments to manage dependencies:\n\n**Option 1: venv (Built-in)**\n```bash\npython -m venv ai_env\nsource ai_env/bin/activate  # Linux/Mac\nai_env\\Scripts\\activate     # Windows\n```\n\n**Option 2: Conda (Recommended for AI)**\n```bash\nconda create -n ai_env python=3.11\nconda activate ai_env\n```\n\nConda is preferred because it handles complex dependencies like CUDA for GPU computing.",
                "code": "# Verify your installation\nimport sys\nprint(f\"Python version: {sys.version}\")\n\n# Install essential AI packages\n# pip install numpy pandas matplotlib scikit-learn torch"
              },
              {
                "title": "Python Data Types for AI",
                "content": "Understanding Python's data types is crucial for AI programming:\n\n- **int/float**: Numerical computations\n- **list**: Mutable sequences (often converted to NumPy arrays)\n- **tuple**: Immutable sequences (used for shapes, dimensions)\n- **dict**: Key-value mappings (hyperparameters, configs)\n- **set**: Unique collections (vocabulary, unique labels)",
                "code": "# Common data structures in AI\nhyperparams = {\n    'learning_rate': 0.001,\n    'batch_size': 32,\n    'epochs': 100\n}\n\n# Tuple for tensor shapes\ninput_shape = (224, 224, 3)  # Height, Width, Channels\n\n# List of training samples\nsamples = [\n    {'text': 'Great product!', 'label': 'positive'},\n    {'text': 'Terrible service', 'label': 'negative'}\n]"
              }
            ],
            "keyTakeaways": [
              "Python's simplicity makes it ideal for AI research and development",
              "Use Conda for managing AI/ML environments with GPU support",
              "Understanding data types helps in efficient data manipulation"
            ],
            "exercises": [
              {
                "title": "Environment Setup",
                "description": "Create a new Conda environment and install NumPy, Pandas, and Matplotlib"
              },
              {
                "title": "Data Structure Practice",
                "description": "Create a dictionary to store model hyperparameters and a list of training samples"
              }
            ],
            "sources": [
              {
                "title": "Python Official Documentation",
                "url": "https://docs.python.org/3/"
              },
              {
                "title": "Conda Documentation",
                "url": "https://docs.conda.io/"
              },
              {
                "title": "Python for Data Science Handbook",
                "author": "Jake VanderPlas",
                "year": 2016
              }
            ]
          }
        },
        {
          "id": "py-functions",
          "title": "Functions and Functional Programming",
          "duration": "45 min",
          "content": {
            "overview": "Functions are the building blocks of modular AI code. Python supports multiple programming paradigms including functional programming, which is particularly useful for data transformations in ML pipelines.",
            "sections": [
              {
                "title": "Defining Functions",
                "content": "Well-designed functions make AI code reusable and testable. Use type hints for clarity:",
                "code": "from typing import List, Tuple, Optional\nimport numpy as np\n\ndef normalize(data: np.ndarray, \n              method: str = 'minmax') -> np.ndarray:\n    \"\"\"Normalize data using specified method.\n    \n    Args:\n        data: Input array to normalize\n        method: 'minmax' or 'zscore'\n    \n    Returns:\n        Normalized array\n    \"\"\"\n    if method == 'minmax':\n        return (data - data.min()) / (data.max() - data.min())\n    elif method == 'zscore':\n        return (data - data.mean()) / data.std()\n    else:\n        raise ValueError(f\"Unknown method: {method}\")"
              },
              {
                "title": "Lambda Functions and Map/Filter",
                "content": "Lambda functions enable concise data transformations common in preprocessing pipelines:",
                "code": "# Lambda for quick transformations\nsquare = lambda x: x ** 2\n\n# Map applies function to each element\ndata = [1, 2, 3, 4, 5]\nsquared = list(map(lambda x: x ** 2, data))  # [1, 4, 9, 16, 25]\n\n# Filter selects elements matching condition\npositive_samples = list(filter(\n    lambda x: x['label'] == 'positive', \n    samples\n))\n\n# List comprehension (often preferred)\nsquared = [x ** 2 for x in data]\npositive = [s for s in samples if s['label'] == 'positive']"
              },
              {
                "title": "Decorators for ML",
                "content": "Decorators add functionality to functions without modifying them. Common uses in ML include timing, caching, and logging:",
                "code": "import time\nfrom functools import wraps\n\ndef timer(func):\n    \"\"\"Decorator to time function execution.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.perf_counter()\n        result = func(*args, **kwargs)\n        elapsed = time.perf_counter() - start\n        print(f\"{func.__name__} took {elapsed:.4f}s\")\n        return result\n    return wrapper\n\n@timer\ndef train_epoch(model, data):\n    # Training logic here\n    pass\n\n# Usage\ntrain_epoch(model, data)  # Prints: train_epoch took 2.3456s"
              }
            ],
            "keyTakeaways": [
              "Type hints improve code readability and enable IDE support",
              "Lambda functions are useful for quick data transformations",
              "Decorators add reusable functionality like timing and caching"
            ],
            "exercises": [
              {
                "title": "Preprocessing Pipeline",
                "description": "Create functions for text preprocessing: lowercase, remove punctuation, tokenize"
              },
              {
                "title": "Custom Decorator",
                "description": "Write a decorator that logs function inputs and outputs"
              }
            ],
            "sources": [
              {
                "title": "Fluent Python",
                "author": "Luciano Ramalho",
                "year": 2022
              },
              {
                "title": "Python Type Hints Documentation",
                "url": "https://docs.python.org/3/library/typing.html"
              }
            ]
          }
        },
        {
          "id": "py-oop",
          "title": "Object-Oriented Programming for AI",
          "duration": "60 min",
          "content": {
            "overview": "Object-oriented programming (OOP) is essential for building maintainable AI systems. PyTorch and TensorFlow both use OOP extensivelyâ€”models are classes, layers are objects, and training loops use polymorphism.",
            "sections": [
              {
                "title": "Classes and Objects",
                "content": "In AI frameworks, models are typically implemented as classes:",
                "code": "class NeuralNetwork:\n    \"\"\"A simple neural network implementation.\"\"\"\n    \n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # Initialize weights\n        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n        self.b1 = np.zeros((1, hidden_size))\n        self.b2 = np.zeros((1, output_size))\n    \n    def forward(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Forward pass through the network.\"\"\"\n        self.z1 = X @ self.W1 + self.b1\n        self.a1 = np.maximum(0, self.z1)  # ReLU\n        self.z2 = self.a1 @ self.W2 + self.b2\n        return self.softmax(self.z2)\n    \n    @staticmethod\n    def softmax(x: np.ndarray) -> np.ndarray:\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_x / exp_x.sum(axis=1, keepdims=True)"
              },
              {
                "title": "Inheritance and Polymorphism",
                "content": "Inheritance enables code reuse. Different model architectures can share common functionality:",
                "code": "from abc import ABC, abstractmethod\n\nclass BaseModel(ABC):\n    \"\"\"Abstract base class for all models.\"\"\"\n    \n    def __init__(self, name: str):\n        self.name = name\n        self.is_trained = False\n    \n    @abstractmethod\n    def fit(self, X, y):\n        \"\"\"Train the model.\"\"\"\n        pass\n    \n    @abstractmethod\n    def predict(self, X):\n        \"\"\"Make predictions.\"\"\"\n        pass\n    \n    def save(self, path: str):\n        \"\"\"Save model to disk.\"\"\"\n        import pickle\n        with open(path, 'wb') as f:\n            pickle.dump(self, f)\n\nclass LinearRegression(BaseModel):\n    \"\"\"Linear regression implementation.\"\"\"\n    \n    def __init__(self):\n        super().__init__('LinearRegression')\n        self.weights = None\n    \n    def fit(self, X, y):\n        # Add bias term\n        X_b = np.c_[np.ones(len(X)), X]\n        # Normal equation\n        self.weights = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n        self.is_trained = True\n        return self\n    \n    def predict(self, X):\n        X_b = np.c_[np.ones(len(X)), X]\n        return X_b @ self.weights"
              },
              {
                "title": "Data Classes for Configurations",
                "content": "Python's dataclasses are perfect for ML configurations and experiment tracking:",
                "code": "from dataclasses import dataclass, field\nfrom typing import List\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Configuration for model training.\"\"\"\n    model_name: str\n    learning_rate: float = 0.001\n    batch_size: int = 32\n    epochs: int = 100\n    hidden_layers: List[int] = field(default_factory=lambda: [128, 64])\n    dropout: float = 0.1\n    \n    def __post_init__(self):\n        # Validation\n        if self.learning_rate <= 0:\n            raise ValueError(\"Learning rate must be positive\")\n\n# Usage\nconfig = TrainingConfig(\n    model_name='sentiment_classifier',\n    learning_rate=0.0001,\n    epochs=50\n)\nprint(config)  # Pretty printed"
              }
            ],
            "keyTakeaways": [
              "AI frameworks like PyTorch use classes to define models",
              "Abstract base classes enforce consistent interfaces",
              "Dataclasses simplify configuration management"
            ],
            "exercises": [
              {
                "title": "Custom Model Class",
                "description": "Implement a KNearestNeighbors class with fit() and predict() methods"
              },
              {
                "title": "Experiment Config",
                "description": "Create a dataclass for experiment configuration with validation"
              }
            ],
            "sources": [
              {
                "title": "PyTorch Documentation - Modules",
                "url": "https://pytorch.org/docs/stable/nn.html"
              },
              {
                "title": "Python dataclasses",
                "url": "https://docs.python.org/3/library/dataclasses.html"
              }
            ]
          }
        },
        {
          "id": "py-file-io",
          "title": "File I/O and Data Handling",
          "duration": "30 min",
          "content": {
            "overview": "Real-world AI projects involve reading and writing data in various formats. This lesson covers Python's file handling capabilities, from basic text files to JSON, CSV, and binary formats like pickleâ€”essential skills for data pipelines.",
            "sections": [
              {
                "title": "Basic File Operations",
                "content": "Python's built-in file handling uses context managers for safe resource management:",
                "code": "# Reading files\nwith open('data.txt', 'r') as f:\n    content = f.read()        # Entire file as string\n    \nwith open('data.txt', 'r') as f:\n    lines = f.readlines()     # List of lines\n\n# Writing files\nwith open('output.txt', 'w') as f:\n    f.write('Hello, AI!')     # Overwrite\n\nwith open('log.txt', 'a') as f:\n    f.write('New entry\\n')    # Append\n\n# The 'with' statement ensures file is closed even if error occurs\n# This is called a 'context manager'"
              },
              {
                "title": "JSON: The Universal Data Format",
                "content": "JSON is the standard format for configs, API responses, and structured data:",
                "code": "import json\n\n# Reading JSON\nwith open('config.json', 'r') as f:\n    config = json.load(f)\nprint(config['learning_rate'])\n\n# Writing JSON\nresults = {\n    'model': 'RandomForest',\n    'accuracy': 0.95,\n    'params': {'n_estimators': 100}\n}\nwith open('results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\n# JSON strings\njson_str = json.dumps(results)      # Dict to string\ndata = json.loads(json_str)         # String to dict\n\n# Handle complex types (datetime, numpy)\nimport numpy as np\nfrom datetime import datetime\n\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        return super().default(obj)"
              },
              {
                "title": "Pickle: Saving Python Objects",
                "content": "Pickle serializes Python objects for saving models, arrays, and complex data structures:",
                "code": "import pickle\nimport numpy as np\n\n# Save any Python object\nmodel = {'weights': np.random.randn(10, 5), 'bias': np.zeros(5)}\n\nwith open('model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\n# Load it back\nwith open('model.pkl', 'rb') as f:\n    loaded_model = pickle.load(f)\n\n# âš ï¸ Security Warning: Never unpickle untrusted data!\n# Pickle can execute arbitrary code when loading\n\n# Alternative: joblib (faster for large arrays)\nfrom joblib import dump, load\ndump(model, 'model.joblib')\nmodel = load('model.joblib')"
              },
              {
                "title": "Pathlib: Modern Path Handling",
                "content": "Pathlib provides an object-oriented interface for file pathsâ€”cross-platform and intuitive:",
                "code": "from pathlib import Path\n\n# Create paths\ndata_dir = Path('data')\ntrain_file = data_dir / 'train.csv'      # Use / for joining!\n\n# Path operations\nprint(train_file.exists())                # Check existence\nprint(train_file.suffix)                  # '.csv'\nprint(train_file.stem)                    # 'train'\nprint(train_file.parent)                  # Path('data')\n\n# Create directories\noutput_dir = Path('outputs/experiment_1')\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Find files\nall_csvs = list(data_dir.glob('*.csv'))        # Current dir\nall_jsons = list(data_dir.rglob('*.json'))     # Recursive\n\n# Read/write (Python 3.5+)\ncontent = train_file.read_text()\nPath('out.txt').write_text('Hello!')\n\n# Common pattern: iterate over dataset files\nfor image_path in Path('images').glob('*.jpg'):\n    process_image(image_path)"
              },
              {
                "title": "When to Use Each Format",
                "content": "Choose the right format for your use case:\n\n| Format | Best For | Pros | Cons |\n|--------|----------|------|------|\n| **JSON** | Configs, APIs, logs | Human-readable, universal | Slow for large data |\n| **CSV** | Tabular data | Simple, Excel-compatible | No types, no nested data |\n| **Pickle** | Python objects, models | Fast, preserves types | Python-only, security risk |\n| **Parquet** | Large datasets | Fast, compressed, typed | Needs pyarrow |\n| **HDF5** | Scientific arrays | Fast for slicing | Complex API |\n\n**Real-World Pattern:**\n- Use JSON for configs and metadata\n- Use Parquet/CSV for tabular datasets\n- Use Pickle/joblib for trained models"
              }
            ],
            "keyTakeaways": [
              "Always use 'with' statements for safe file handling",
              "JSON for configs and APIs, Pickle for Python objects",
              "Pathlib is the modern way to handle file paths",
              "Choose format based on data type and use case"
            ],
            "exercises": [
              {
                "title": "Config Manager",
                "description": "Create a function that loads a JSON config file, updates values, and saves it back"
              },
              {
                "title": "Dataset Scanner",
                "description": "Use pathlib to recursively find all CSV files in a directory and report their sizes"
              }
            ],
            "sources": [
              {
                "title": "Python File I/O Documentation",
                "url": "https://docs.python.org/3/tutorial/inputoutput.html"
              },
              {
                "title": "Pathlib Documentation",
                "url": "https://docs.python.org/3/library/pathlib.html"
              },
              {
                "title": "Real Python: Working with Files",
                "url": "https://realpython.com/working-with-files-in-python/"
              }
            ]
          }
        },
        {
          "id": "py-error-handling",
          "title": "Error Handling and Debugging",
          "duration": "35 min",
          "content": {
            "overview": "Robust AI code handles errors gracefully. This lesson covers Python's exception handling, debugging techniques, and loggingâ€”essential for production ML systems where data can be messy and APIs can fail.",
            "sections": [
              {
                "title": "Try/Except: Handling Errors Gracefully",
                "content": "Python uses exceptions to handle errors. Use try/except to catch and handle them:",
                "code": "# Basic try/except\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    result = 0\n    print('Cannot divide by zero!')\n\n# Catch multiple exceptions\ntry:\n    data = load_data('file.csv')\n    processed = process(data)\nexcept FileNotFoundError:\n    print('File not found')\nexcept ValueError as e:\n    print(f'Invalid data: {e}')\nexcept Exception as e:\n    print(f'Unexpected error: {e}')  # Catch-all\nfinally:\n    cleanup()  # Always runs\n\n# The 'else' clause runs if no exception\ntry:\n    result = risky_operation()\nexcept SomeError:\n    handle_error()\nelse:\n    print('Success!')  # Only runs if no exception"
              },
              {
                "title": "Common AI/ML Exceptions",
                "content": "Know the errors you'll encounter in ML development:",
                "code": "import numpy as np\n\n# ValueError - wrong input shape or type\ntry:\n    arr = np.array([[1, 2], [3, 4]])\n    arr.reshape(3, 3)  # Can't reshape 4 elements to 9\nexcept ValueError as e:\n    print(f'Shape mismatch: {e}')\n\n# KeyError - missing dictionary key\nconfig = {'lr': 0.01}\ntry:\n    batch_size = config['batch_size']\nexcept KeyError:\n    batch_size = 32  # Default value\n\n# Better: use .get() with default\nbatch_size = config.get('batch_size', 32)\n\n# IndexError - array out of bounds\ndata = [1, 2, 3]\ntry:\n    item = data[10]\nexcept IndexError:\n    item = None\n\n# API errors (common in LLM apps)\nimport openai\ntry:\n    response = openai.chat.completions.create(...)\nexcept openai.RateLimitError:\n    time.sleep(60)  # Wait and retry\nexcept openai.APIError as e:\n    log_error(e)"
              },
              {
                "title": "Raising and Custom Exceptions",
                "content": "Raise exceptions to signal errors in your own code:",
                "code": "# Raise built-in exceptions\ndef train_model(X, y, epochs):\n    if epochs <= 0:\n        raise ValueError('epochs must be positive')\n    if len(X) != len(y):\n        raise ValueError(f'X and y must have same length: {len(X)} != {len(y)}')\n    # ... training logic\n\n# Custom exception classes\nclass ModelNotTrainedError(Exception):\n    \"\"\"Raised when predict is called before fit.\"\"\"\n    pass\n\nclass DataValidationError(Exception):\n    \"\"\"Raised when input data fails validation.\"\"\"\n    def __init__(self, message, column=None):\n        super().__init__(message)\n        self.column = column\n\n# Using custom exceptions\nclass Model:\n    def __init__(self):\n        self.is_fitted = False\n    \n    def predict(self, X):\n        if not self.is_fitted:\n            raise ModelNotTrainedError('Call fit() before predict()')\n        return self._predict(X)"
              },
              {
                "title": "Debugging with pdb",
                "content": "Python's built-in debugger lets you step through code and inspect variables:",
                "code": "# Insert a breakpoint\ndef buggy_function(data):\n    processed = preprocess(data)\n    breakpoint()  # Execution pauses here (Python 3.7+)\n    result = compute(processed)\n    return result\n\n# pdb commands:\n# n (next)      - Execute next line\n# s (step)      - Step into function\n# c (continue)  - Continue to next breakpoint\n# p variable    - Print variable value\n# l (list)      - Show current code\n# q (quit)      - Exit debugger\n\n# Example debugging session:\n# > processed = preprocess(data)\n# (Pdb) p data.shape\n# (100, 10)\n# (Pdb) p processed[:5]\n# array([[0.1, 0.2, ...]])\n# (Pdb) n\n# > result = compute(processed)\n\n# Alternative: run script in debug mode\n# python -m pdb script.py"
              },
              {
                "title": "Logging for Production",
                "content": "Use logging instead of print() for production code:",
                "code": "import logging\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('training.log'),\n        logging.StreamHandler()  # Also print to console\n    ]\n)\n\nlogger = logging.getLogger(__name__)\n\n# Log levels (in order of severity)\nlogger.debug('Detailed info for debugging')\nlogger.info('Training started')\nlogger.warning('GPU memory running low')\nlogger.error('Failed to load checkpoint')\nlogger.critical('Training crashed!')\n\n# Use in ML training\ndef train(model, data, epochs):\n    logger.info(f'Starting training for {epochs} epochs')\n    for epoch in range(epochs):\n        loss = train_epoch(model, data)\n        logger.info(f'Epoch {epoch}: loss={loss:.4f}')\n        \n        if loss > 100:\n            logger.warning(f'Loss spike detected: {loss}')\n    \n    logger.info('Training complete')"
              }
            ],
            "keyTakeaways": [
              "Use try/except to handle errors gracefully",
              "Use .get() for dictionaries to avoid KeyError",
              "breakpoint() pauses execution for debugging",
              "Use logging instead of print() in production"
            ],
            "exercises": [
              {
                "title": "Robust Data Loader",
                "description": "Create a function that loads a CSV file and handles FileNotFoundError, empty files, and invalid data"
              },
              {
                "title": "Training Logger",
                "description": "Set up a logger that writes to both console and file with timestamps"
              }
            ],
            "sources": [
              {
                "title": "Python Errors and Exceptions",
                "url": "https://docs.python.org/3/tutorial/errors.html"
              },
              {
                "title": "Python Logging Documentation",
                "url": "https://docs.python.org/3/library/logging.html"
              },
              {
                "title": "Effective Python",
                "author": "Brett Slatkin",
                "year": 2019
              }
            ]
          }
        }
      ]
    },
    {
      "id": "numpy-pandas",
      "title": "NumPy & Pandas for Data Science",
      "lessons": [
        {
          "id": "numpy-basics",
          "title": "NumPy: The Foundation of Scientific Python",
          "duration": "60 min",
          "content": {
            "overview": "NumPy (Numerical Python) is the fundamental package for scientific computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a vast collection of mathematical functions. All major AI frameworks build upon NumPy's array interface.",
            "sections": [
              {
                "title": "Why NumPy for Machine Learning?",
                "content": "**IBM Insight:** 'Data points in machine learning are usually represented in vector form, in which each element of a data point's vector embedding corresponds to its numerical value for a specific feature.' (ibm.com/topics/machine-learning)\n\n**NumPy enables this by providing:**\n- Efficient N-dimensional arrays for data representation\n- Vectorized operations (100x faster than Python loops)\n- Mathematical functions for statistics and linear algebra\n- Memory-efficient storage in contiguous blocks\n\n| Python List | NumPy Array |\n|------------|-------------|\n| Slow loops | Vectorized ops |\n| Any types | Homogeneous |\n| Scattered memory | Contiguous |\n| Limited math | Full linear algebra |"
              },
              {
                "title": "Understanding NumPy Arrays",
                "content": "NumPy arrays are the backbone of data representation in ML. Unlike Python lists, they store homogeneous data in contiguous memory blocks, enabling fast vectorized operations:",
                "code": "import numpy as np\n\n# Creating arrays\narr_1d = np.array([1, 2, 3, 4, 5])\narr_2d = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Array from functions\nzeros = np.zeros((3, 4))           # 3x4 matrix of zeros\nones = np.ones((2, 3))             # 2x3 matrix of ones\nidentity = np.eye(4)               # 4x4 identity matrix\nrandom = np.random.randn(3, 3)     # Standard normal distribution\n\n# Ranges\nsequence = np.arange(0, 10, 2)     # [0, 2, 4, 6, 8]\nlinspace = np.linspace(0, 1, 5)    # [0, 0.25, 0.5, 0.75, 1]\n\n# Array properties\nprint(f\"Shape: {arr_2d.shape}\")    # (2, 3)\nprint(f\"Dtype: {arr_2d.dtype}\")    # int64\nprint(f\"Size: {arr_2d.size}\")      # 6"
              },
              {
                "title": "Vectorized Operations",
                "content": "Vectorization is the key to fast numerical computing. Operations are applied element-wise without explicit loops:",
                "code": "# Vectorized math (100x faster than loops)\na = np.array([1, 2, 3, 4])\nb = np.array([10, 20, 30, 40])\n\n# Element-wise operations\nsum_ab = a + b         # [11, 22, 33, 44]\nproduct = a * b        # [10, 40, 90, 160]\nsquared = a ** 2       # [1, 4, 9, 16]\n\n# Universal functions (ufuncs)\nsqrt = np.sqrt(a)      # [1, 1.414, 1.732, 2]\nexp = np.exp(a)        # [2.718, 7.389, 20.086, 54.598]\nlog = np.log(a)        # [0, 0.693, 1.099, 1.386]\n\n# Aggregations\nprint(f\"Sum: {a.sum()}\")       # 10\nprint(f\"Mean: {a.mean()}\")     # 2.5\nprint(f\"Std: {a.std()}\")       # 1.118\nprint(f\"Max: {a.max()}\")       # 4"
              },
              {
                "title": "Broadcasting",
                "content": "Broadcasting allows operations between arrays of different shapes, automatically expanding smaller arrays:",
                "code": "# Broadcasting examples\nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6],\n                   [7, 8, 9]])\n\n# Scalar broadcasting\nmatrix_scaled = matrix * 2  # Multiply each element by 2\n\n# 1D to 2D broadcasting\nrow_vector = np.array([1, 0, 1])\nresult = matrix + row_vector  # Add to each row\n# [[2, 2, 4],\n#  [5, 5, 7],\n#  [8, 8, 10]]\n\n# Column broadcasting\ncol_vector = np.array([[10], [20], [30]])\nresult = matrix + col_vector  # Add to each column\n\n# Common ML pattern: subtract mean per feature\ndata = np.random.randn(100, 5)  # 100 samples, 5 features\nmean = data.mean(axis=0)        # Mean of each column\ncentered = data - mean          # Broadcast subtract"
              },
              {
                "title": "Indexing and Slicing",
                "content": "Efficient data access is crucial for preprocessing and batching:",
                "code": "arr = np.arange(20).reshape(4, 5)\n# [[ 0,  1,  2,  3,  4],\n#  [ 5,  6,  7,  8,  9],\n#  [10, 11, 12, 13, 14],\n#  [15, 16, 17, 18, 19]]\n\n# Basic slicing\nrow = arr[1, :]           # [5, 6, 7, 8, 9]\ncol = arr[:, 2]           # [2, 7, 12, 17]\nsubset = arr[1:3, 2:4]    # [[7, 8], [12, 13]]\n\n# Boolean indexing (filtering)\nmask = arr > 10\nfiltered = arr[mask]       # [11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n# Fancy indexing\nindices = [0, 2, 3]\nselected_rows = arr[indices]  # Rows 0, 2, 3\n\n# Common ML pattern: train/test split\nnp.random.seed(42)\nindices = np.random.permutation(len(arr))\ntrain_idx = indices[:3]\ntest_idx = indices[3:]"
              }
            ],
            "keyTakeaways": [
              "NumPy arrays are faster than Python lists due to contiguous memory and vectorization",
              "Broadcasting enables operations between arrays of different shapes",
              "Boolean indexing is powerful for filtering data"
            ],
            "exercises": [
              {
                "title": "Matrix Operations",
                "description": "Create two random matrices and compute their dot product, element-wise product, and sum along each axis"
              },
              {
                "title": "Data Normalization",
                "description": "Normalize a dataset to have zero mean and unit variance using broadcasting"
              }
            ],
            "sources": [
              {
                "title": "NumPy User Guide",
                "url": "https://numpy.org/doc/stable/user/"
              },
              {
                "title": "NumPy Absolute Beginners Guide",
                "url": "https://numpy.org/doc/stable/user/absolute_beginners.html"
              },
              {
                "title": "From Python to NumPy",
                "author": "Nicolas Rougier",
                "url": "https://www.labri.fr/perso/nrougier/from-python-to-numpy/"
              },
              {
                "title": "Guide to NumPy",
                "author": "Travis Oliphant",
                "year": 2006,
                "note": "Travis Oliphant created NumPy in 2005, unifying Numeric and Numarray into the foundation of Python's scientific computing ecosystem"
              }
            ]
          }
        },
        {
          "id": "numpy-advanced",
          "title": "NumPy: Views, Copies, and Performance Tips",
          "duration": "30 min",
          "content": {
            "overview": "Understanding views vs copies is crucial for writing bug-free, memory-efficient code. This lesson covers these concepts plus reshape, flatten, and performance optimization.",
            "sections": [
              {
                "title": "Views vs Copies: A Critical Concept",
                "content": "When you slice a NumPy array, you get a **view**, not a copy. A view shares data with the original array - modifying the view changes the original!\n\n**Analogy:** Think of views like looking at the same photo through different windows. If you draw on the photo, everyone sees the change.",
                "code": "import numpy as np\n\n# Create original array\na = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n\n# Slicing creates a VIEW (shares memory)\nb = a[0, :]        # First row\nprint(b)           # [1 2 3 4]\n\nb[0] = 99          # Modify the view\nprint(b)           # [99 2 3 4]\nprint(a)           # [[99, 2, 3, 4], ...] - Original changed too!\n\n# To avoid this, use .copy()\nc = a[1, :].copy()  # Deep copy - new memory\nc[0] = 999\nprint(a)            # Original unchanged"
              },
              {
                "title": "When Views Are Created",
                "content": "**Operations that create VIEWS (share memory):**\n- Slicing: `a[1:3]`, `a[:, 2]`\n- Transposing: `a.T`\n- Reshaping (usually): `a.reshape(3, 4)`\n- `np.ravel(a)` (when possible)\n\n**Operations that create COPIES (new memory):**\n- Fancy indexing: `a[[0, 2, 3]]`\n- Boolean indexing: `a[a > 5]`\n- `a.flatten()` (always)\n- `a.copy()`\n- Mathematical operations: `a + 1`, `a * 2`",
                "code": "# Check if it's a view or copy\na = np.arange(12).reshape(3, 4)\n\nview = a[1:3, :]\nprint(view.base is a)       # True - it's a view\n\nfancy = a[[0, 2], :]\nprint(fancy.base is None)   # True - it's a copy\n\n# Flatten vs Ravel\nflat = a.flatten()          # Always a copy\nrav = a.ravel()             # View if possible\n\nprint(flat.base is None)    # True - copy\nprint(rav.base is a)        # True - view"
              },
              {
                "title": "Reshape, Resize, and newaxis",
                "content": "Reshaping is essential for preparing data for ML models. The key rule: **total elements must stay the same**.",
                "code": "import numpy as np\n\na = np.arange(12)  # [0, 1, 2, ..., 11]\n\n# Reshape to 2D\nmatrix = a.reshape(3, 4)     # 3 rows, 4 columns\nmatrix = a.reshape(4, -1)    # -1 means 'calculate automatically'\n\n# Add new dimension (common for batching)\nvector = np.array([1, 2, 3, 4, 5, 6])\nprint(vector.shape)           # (6,)\n\n# Add batch dimension\nrow_vector = vector[np.newaxis, :]    # (1, 6)\ncol_vector = vector[:, np.newaxis]    # (6, 1)\n\n# Or use expand_dims\nbatched = np.expand_dims(vector, axis=0)  # (1, 6)\n\n# Common ML pattern: reshape for model input\nX = np.random.randn(100, 28, 28)  # 100 images, 28x28\nX_flat = X.reshape(100, -1)       # Flatten to (100, 784)"
              },
              {
                "title": "Performance Tips",
                "content": "Write NumPy code that's 10-100x faster with these tips:",
                "code": "import numpy as np\nimport time\n\n# âŒ SLOW: Python loops\ndef slow_normalize(arr):\n    result = []\n    for x in arr:\n        result.append((x - arr.min()) / (arr.max() - arr.min()))\n    return result\n\n# âœ… FAST: Vectorized\ndef fast_normalize(arr):\n    return (arr - arr.min()) / (arr.max() - arr.min())\n\n# Benchmark\ndata = np.random.randn(100000)\n\nstart = time.time()\nslow_normalize(data)\nprint(f\"Slow: {time.time() - start:.3f}s\")\n\nstart = time.time()\nfast_normalize(data)\nprint(f\"Fast: {time.time() - start:.3f}s\")  # ~100x faster!\n\n# Other tips:\n# 1. Pre-allocate arrays instead of appending\nresult = np.zeros(1000)\nfor i in range(1000):\n    result[i] = compute(i)  # Better than append\n\n# 2. Use appropriate dtype\narr_float32 = np.array([1.0, 2.0], dtype=np.float32)  # Half memory of float64"
              }
            ],
            "keyTakeaways": [
              "Slicing creates views that share memory with the original",
              "Use .copy() when you need independent data",
              "Reshape requires the total number of elements to stay the same",
              "Vectorize operations - avoid Python loops for big speedups"
            ],
            "sources": [
              {
                "title": "NumPy: Copies and Views",
                "url": "https://numpy.org/doc/stable/user/basics.copies.html"
              },
              {
                "title": "NumPy Absolute Beginners Guide",
                "url": "https://numpy.org/doc/stable/user/absolute_beginners.html"
              }
            ]
          }
        },
        {
          "id": "pandas-basics",
          "title": "Pandas for Data Manipulation",
          "duration": "60 min",
          "content": {
            "overview": "Pandas provides high-performance, easy-to-use data structures for data analysis. The DataFrame is the primary structure, representing tabular data with labeled rows and columnsâ€”essential for loading, cleaning, and transforming datasets.",
            "sections": [
              {
                "title": "The Data Science Lifecycle",
                "content": "**IBM Definition:** 'Data science combines math and statistics, specialized programming, advanced analytics, and machine learning to uncover actionable insights hidden in data.' (ibm.com/topics/data-science)\n\n**The Data Lifecycle (from IBM):**\n1. **Data Ingestion**: Collect raw data from various sources\n2. **Data Storage/Processing**: Clean, deduplicate, transform with ETL\n3. **Data Analysis**: Exploratory analysis, pattern discovery\n4. **Communicate**: Visualize insights for decision-makers\n\n**Pandas handles stages 2-4!**"
              },
              {
                "title": "DataFrames and Series",
                "content": "DataFrames are 2D labeled data structures with columns of potentially different types:",
                "code": "import pandas as pd\n\n# Creating DataFrames\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n    'age': [25, 30, 35, 28],\n    'salary': [50000, 60000, 75000, 55000],\n    'department': ['Engineering', 'Marketing', 'Engineering', 'Sales']\n})\n\n# From CSV (most common in ML)\ndf = pd.read_csv('data/train.csv')\n\n# From JSON\ndf = pd.read_json('data/samples.json')\n\n# Basic info\nprint(df.shape)        # (rows, columns)\nprint(df.columns)      # Column names\nprint(df.dtypes)       # Data types\nprint(df.describe())   # Statistical summary\nprint(df.head())       # First 5 rows"
              },
              {
                "title": "Selection and Filtering",
                "content": "Pandas provides powerful selection mechanisms for extracting and filtering data:",
                "code": "# Column selection\nnames = df['name']              # Single column (Series)\nsubset = df[['name', 'age']]    # Multiple columns (DataFrame)\n\n# Row selection\nfirst_row = df.iloc[0]          # By integer index\nrows_1_3 = df.iloc[1:4]         # Slice by index\nby_label = df.loc[0]            # By label\n\n# Boolean filtering\neng_team = df[df['department'] == 'Engineering']\nhigh_salary = df[df['salary'] > 55000]\n\n# Multiple conditions\nsenior_eng = df[(df['department'] == 'Engineering') & (df['age'] > 30)]\n\n# Query method (more readable)\nresult = df.query('age > 25 and department == \"Engineering\"')\n\n# Filter with isin\nselected_depts = df[df['department'].isin(['Engineering', 'Sales'])]"
              },
              {
                "title": "Data Cleaning",
                "content": "Real-world data is messy. Pandas provides tools for handling missing values, duplicates, and type conversions:",
                "code": "# Handling missing values\ndf.isna().sum()                    # Count missing per column\ndf.dropna()                        # Drop rows with any missing\ndf.dropna(subset=['age'])          # Drop only if 'age' is missing\ndf.fillna(0)                       # Fill missing with 0\ndf['age'].fillna(df['age'].mean()) # Fill with mean\n\n# Remove duplicates\ndf.drop_duplicates()\ndf.drop_duplicates(subset=['name'], keep='first')\n\n# Type conversion\ndf['age'] = df['age'].astype(int)\ndf['date'] = pd.to_datetime(df['date'])\n\n# String operations\ndf['name_lower'] = df['name'].str.lower()\ndf['name_len'] = df['name'].str.len()\n\n# Apply custom function\ndf['age_group'] = df['age'].apply(\n    lambda x: 'young' if x < 30 else 'senior'\n)"
              },
              {
                "title": "Grouping and Aggregation",
                "content": "GroupBy operations are essential for feature engineering and data analysis:",
                "code": "# Group by single column\ngrouped = df.groupby('department')\n\n# Aggregations\ndf.groupby('department')['salary'].mean()\ndf.groupby('department')['salary'].agg(['mean', 'std', 'count'])\n\n# Multiple aggregations\nagg_result = df.groupby('department').agg({\n    'salary': ['mean', 'max'],\n    'age': 'mean'\n})\n\n# Group by multiple columns\ndf.groupby(['department', 'age_group'])['salary'].mean()\n\n# Transform (preserves original shape)\ndf['dept_avg_salary'] = df.groupby('department')['salary'].transform('mean')\ndf['salary_vs_dept'] = df['salary'] - df['dept_avg_salary']\n\n# Pivot tables\npivot = df.pivot_table(\n    values='salary',\n    index='department',\n    columns='age_group',\n    aggfunc='mean'\n)"
              }
            ],
            "keyTakeaways": [
              "DataFrames are the go-to structure for tabular data in ML",
              "The data science lifecycle: ingest â†’ process â†’ analyze â†’ communicate",
              "Boolean indexing enables powerful filtering",
              "GroupBy operations are essential for feature engineering"
            ],
            "exercises": [
              {
                "title": "Data Cleaning Pipeline",
                "description": "Load a CSV, handle missing values, remove duplicates, and convert types"
              },
              {
                "title": "Feature Engineering",
                "description": "Create new features using groupby transformations and apply functions"
              }
            ],
            "sources": [
              {
                "title": "IBM: What is Data Science?",
                "url": "https://www.ibm.com/topics/data-science"
              },
              {
                "title": "Pandas User Guide",
                "url": "https://pandas.pydata.org/docs/user_guide/"
              },
              {
                "title": "Pandas Getting Started Tutorials",
                "url": "https://pandas.pydata.org/docs/getting_started/intro_tutorials/"
              },
              {
                "title": "Python for Data Analysis",
                "author": "Wes McKinney",
                "year": 2022
              }
            ]
          }
        },
        {
          "id": "sklearn-basics",
          "title": "Scikit-learn: Machine Learning Made Easy",
          "duration": "60 min",
          "content": {
            "overview": "Scikit-learn is the most popular machine learning library for Python. It provides simple, consistent APIs for training models, preprocessing data, and evaluating performance. Built on top of NumPy, SciPy, and Matplotlib, it streamlines AI and statistical modeling with a consistent interface.",
            "sections": [
              {
                "title": "The Three Types of Machine Learning",
                "content": "Before diving into scikit-learn, understand the three fundamental ML paradigms:\n\n| Type | Goal | Example |\n|------|------|--------|\n| **Supervised** | Predict labels from features | Spam detection, price prediction |\n| **Unsupervised** | Find patterns in unlabeled data | Customer segmentation, anomaly detection |\n| **Reinforcement** | Learn actions via trial and error | Game AI, robotics |\n\n**IBM Definition:** 'Machine learning is the subset of AI focused on algorithms that can learn patterns from training data and make accurate inferences about new data.' (ibm.com/topics/machine-learning)",
                "diagram": {
                  "title": "Machine Learning Types",
                  "code": "flowchart TB\n    ML[Machine Learning] --> SUP[Supervised]\n    ML --> UNSUP[Unsupervised]\n    ML --> RL[Reinforcement]\n    \n    SUP --> CLASS[Classification]\n    SUP --> REG[Regression]\n    \n    UNSUP --> CLUST[Clustering]\n    UNSUP --> DIM[Dimensionality Reduction]\n    \n    RL --> AGENT[Agent learns via rewards]"
                }
              },
              {
                "title": "The Estimator Pattern: fit() and predict()",
                "content": "Every scikit-learn model follows the same pattern:\n1. Create the estimator (model)\n2. Call `fit(X, y)` to train it\n3. Call `predict(X)` to make predictions\n\n**Analogy:** Think of it like teaching a student. First you show examples (fit), then they can answer new questions (predict).\n\n**IBM Insight:** 'Scikit-learn's robust suite of pretrained neural networks and ML algorithms allows newcomers to quickly preprocess datasets for supervised learning without needing in-depth mathematical understanding.' (ibm.com/topics/scikit-learn)",
                "code": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# All models follow the same API!\n\n# Example 1: Random Forest\nclf = RandomForestClassifier(random_state=42)\nX_train = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]  # Features\ny_train = [0, 1, 0]                           # Labels\n\nclf.fit(X_train, y_train)           # Train\npredictions = clf.predict([[2, 3, 4]])  # Predict: [0]\n\n# Example 2: Logistic Regression - same API!\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr.predict([[2, 3, 4]])             # Same interface!"
              },
              {
                "title": "Preprocessing: Transformers and Scalers",
                "content": "Real data needs preprocessing. Scikit-learn transformers also follow a consistent pattern:\n- `fit()`: Learn parameters from training data\n- `transform()`: Apply the transformation\n- `fit_transform()`: Do both in one step\n\n**Common Preprocessing Tasks (from IBM):**\n- **Normalization**: Scale numeric features with MinMaxScaler or StandardScaler\n- **Encoding**: Convert categorical to numerical with OneHotEncoder or LabelEncoder\n- **Missing values**: Impute with SimpleImputer using mean/median\n- **Feature selection**: Use RFE or mutual information to choose relevant features",
                "code": "from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# StandardScaler: zero mean, unit variance\nscaler = StandardScaler()\nX = [[1, 2], [3, 4], [5, 6]]\n\nscaler.fit(X)                    # Learn mean and std\nX_scaled = scaler.transform(X)   # Apply scaling\n# Or in one step:\nX_scaled = scaler.fit_transform(X)\n\nprint(f\"Mean: {X_scaled.mean(axis=0)}\")  # [0, 0]\nprint(f\"Std: {X_scaled.std(axis=0)}\")    # [1, 1]\n\n# IMPORTANT: Only fit on training data!\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Use same scaler!\n\n# Handle missing values\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X_with_nans)\n\n# Encode categorical variables\nencoder = OneHotEncoder(sparse_output=False)\ncategories = [['cat'], ['dog'], ['cat'], ['bird']]\nencoded = encoder.fit_transform(categories)  # Binary vectors"
              },
              {
                "title": "Train-Test Split and Model Evaluation",
                "content": "Never evaluate on training data! Split your data and use proper metrics.",
                "code": "from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.datasets import load_iris\n\n# Load sample dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split: 80% train, 20% test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42,\n    stratify=y  # Keep class proportions\n)\n\n# Train and evaluate\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\nprint(classification_report(y_test, y_pred))\n\n# Cross-validation (more robust)\nscores = cross_val_score(clf, X, y, cv=5)  # 5-fold CV\nprint(f\"CV Accuracy: {scores.mean():.2f} (+/- {scores.std():.2f})\")"
              },
              {
                "title": "Pipelines: Avoid Data Leakage",
                "content": "**Data leakage** happens when information from the test set leaks into training. Pipelines prevent this by chaining preprocessing and model training.\n\n**Common mistake:** Scaling the entire dataset before splitting leaks test info into training!",
                "code": "from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# âŒ WRONG: Data leakage!\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)  # Uses ALL data including test!\nX_train, X_test = train_test_split(X_scaled, ...)\n\n# âœ… RIGHT: Use a Pipeline\npipe = make_pipeline(\n    StandardScaler(),\n    LogisticRegression()\n)\n\n# Pipeline handles everything correctly\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\npipe.fit(X_train, y_train)          # Scaler fits only on X_train\npipe.predict(X_test)                 # Scaler transforms X_test\n\n# Cross-validation with pipeline\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(pipe, X, y, cv=5)  # Proper CV!"
              },
              {
                "title": "Hyperparameter Tuning",
                "content": "Find the best model settings automatically with grid search or random search.",
                "code": "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 10, None],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Grid Search (tries all combinations)\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy'\n)\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best params: {grid_search.best_params_}\")\nprint(f\"Best score: {grid_search.best_score_:.3f}\")\n\n# Use best model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# RandomizedSearchCV is faster for large param spaces\nfrom scipy.stats import randint\nparam_dist = {\n    'n_estimators': randint(50, 200),\n    'max_depth': randint(3, 20)\n}\nrandom_search = RandomizedSearchCV(\n    RandomForestClassifier(), param_dist, \n    n_iter=20, cv=5, random_state=42\n)"
              }
            ],
            "keyTakeaways": [
              "All sklearn models use fit(X, y) and predict(X) - learn once, use everywhere",
              "Machine learning has 3 types: supervised, unsupervised, reinforcement",
              "Always split data before preprocessing to avoid data leakage",
              "Use Pipelines to chain preprocessing and models safely",
              "Cross-validation gives more reliable performance estimates than a single split"
            ],
            "sources": [
              {
                "title": "IBM: What is Scikit-Learn?",
                "url": "https://www.ibm.com/topics/scikit-learn"
              },
              {
                "title": "IBM: What is Machine Learning?",
                "url": "https://www.ibm.com/topics/machine-learning"
              },
              {
                "title": "Scikit-learn Getting Started",
                "url": "https://scikit-learn.org/stable/getting_started.html"
              },
              {
                "title": "Scikit-learn User Guide",
                "url": "https://scikit-learn.org/stable/user_guide.html"
              },
              {
                "title": "Hands-On Machine Learning with Scikit-Learn",
                "author": "AurÃ©lien GÃ©ron",
                "year": 2022
              }
            ]
          }
        },
        {
          "id": "data-visualization",
          "title": "Data Visualization with Matplotlib & Seaborn",
          "duration": "45 min",
          "content": {
            "overview": "Visualization is essential for exploratory data analysis (EDA), understanding model performance, and communicating results. Matplotlib is the foundation for Python plotting, while Seaborn provides beautiful statistical visualizations with less code.",
            "sections": [
              {
                "title": "Why Visualization Matters in ML",
                "content": "**Visualization helps you:**\n- Understand data distributions before modeling\n- Detect outliers and anomalies\n- Identify correlations between features\n- Diagnose model issues (overfitting, bias)\n- Communicate results to stakeholders\n\n**The EDA Process:**\n1. Look at data shape and types\n2. Check for missing values\n3. Visualize distributions (histograms)\n4. Explore relationships (scatter plots)\n5. Check correlations (heatmaps)\n\n**John Tukey (statistician):** *'The greatest value of a picture is when it forces us to notice what we never expected to see.'*"
              },
              {
                "title": "Matplotlib Fundamentals",
                "content": "Matplotlib is the most widely used plotting library in Python. Created by John D. Hunter in 2003.",
                "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Basic line plot\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label='sin(x)', color='blue', linewidth=2)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sine Wave')\nplt.legend()\nplt.grid(True)\nplt.savefig('plot.png', dpi=150)\nplt.show()\n\n# Multiple subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\naxes[0, 0].plot(x, np.sin(x))\naxes[0, 0].set_title('Sine')\n\naxes[0, 1].plot(x, np.cos(x), color='orange')\naxes[0, 1].set_title('Cosine')\n\naxes[1, 0].scatter(np.random.randn(100), np.random.randn(100))\naxes[1, 0].set_title('Scatter')\n\naxes[1, 1].hist(np.random.randn(1000), bins=30)\naxes[1, 1].set_title('Histogram')\n\nplt.tight_layout()\nplt.show()"
              },
              {
                "title": "Essential Plot Types for ML",
                "content": "These plots cover 90% of ML visualization needs:",
                "code": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX, y = iris.data, iris.target\n\n# 1. Histogram - Check feature distributions\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nfor i, ax in enumerate(axes):\n    ax.hist(X[:, i], bins=20, edgecolor='black')\n    ax.set_title(iris.feature_names[i])\nplt.suptitle('Feature Distributions')\nplt.show()\n\n# 2. Scatter plot - Relationship between features\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\nplt.colorbar(scatter, label='Species')\nplt.title('Sepal Length vs Width')\nplt.show()\n\n# 3. Box plot - Compare distributions across groups\nfig, ax = plt.subplots(figsize=(10, 6))\ndata = [X[y == i, 0] for i in range(3)]\nax.boxplot(data, labels=iris.target_names)\nax.set_ylabel('Sepal Length')\nax.set_title('Sepal Length by Species')\nplt.show()"
              },
              {
                "title": "Seaborn: Statistical Visualization",
                "content": "Seaborn is built on Matplotlib but provides a high-level interface for statistical graphics. Created by Michael Waskom.",
                "code": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Set style\nsns.set_style('whitegrid')\n\n# Load sample data\ntips = sns.load_dataset('tips')\n\n# 1. Distribution plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nsns.histplot(data=tips, x='total_bill', kde=True, ax=axes[0])\nsns.kdeplot(data=tips, x='total_bill', hue='time', ax=axes[1])\nplt.show()\n\n# 2. Relationship plots\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nsns.scatterplot(data=tips, x='total_bill', y='tip', \n                hue='day', size='size', ax=axes[0])\nsns.regplot(data=tips, x='total_bill', y='tip', ax=axes[1])\nplt.show()\n\n# 3. Categorical plots\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nsns.boxplot(data=tips, x='day', y='total_bill', ax=axes[0])\nsns.violinplot(data=tips, x='day', y='total_bill', ax=axes[1])\nsns.barplot(data=tips, x='day', y='total_bill', ax=axes[2])\nplt.show()"
              },
              {
                "title": "Correlation Heatmaps",
                "content": "Heatmaps reveal relationships between all features at onceâ€”essential for feature selection:",
                "code": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n# Create correlation matrix\ndf = pd.DataFrame(X, columns=iris.feature_names)\ndf['species'] = y\ncorr_matrix = df.corr()\n\n# Heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, \n            annot=True,           # Show values\n            cmap='coolwarm',      # Color scheme\n            center=0,             # Center at 0\n            square=True,          # Square cells\n            linewidths=0.5)\nplt.title('Feature Correlation Matrix')\nplt.tight_layout()\nplt.show()\n\n# Pair plot - All relationships at once\nsns.pairplot(df, hue='species', diag_kind='kde')\nplt.show()"
              },
              {
                "title": "ML Model Visualization",
                "content": "Visualize model performance and diagnostics:",
                "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Train a model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# 1. Confusion Matrix\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=iris.target_names,\n            yticklabels=iris.target_names)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# 2. Feature Importance\nimportances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), importances[indices])\nplt.xticks(range(X.shape[1]), \n           [iris.feature_names[i] for i in indices], rotation=45)\nplt.title('Feature Importance')\nplt.tight_layout()\nplt.show()\n\n# 3. Learning Curve\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(\n    clf, X, y, cv=5, n_jobs=-1,\n    train_sizes=np.linspace(0.1, 1.0, 10)\n)\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_sizes, train_scores.mean(axis=1), label='Training')\nplt.plot(train_sizes, val_scores.mean(axis=1), label='Validation')\nplt.xlabel('Training Set Size')\nplt.ylabel('Accuracy')\nplt.title('Learning Curve')\nplt.legend()\nplt.show()"
              }
            ],
            "keyTakeaways": [
              "Always visualize data before modeling (EDA)",
              "Matplotlib for customization, Seaborn for quick statistical plots",
              "Histograms and box plots reveal distributions",
              "Correlation heatmaps help with feature selection",
              "Confusion matrices and learning curves diagnose model issues"
            ],
            "exercises": [
              {
                "title": "EDA Dashboard",
                "description": "Create a 2x2 subplot figure showing: histograms, scatter plot, box plot, and correlation heatmap for a dataset"
              },
              {
                "title": "Model Diagnostics",
                "description": "Train a classifier and create visualizations: confusion matrix, feature importance, and ROC curve"
              }
            ],
            "sources": [
              {
                "title": "Matplotlib Documentation",
                "url": "https://matplotlib.org/stable/contents.html"
              },
              {
                "title": "Seaborn Tutorial",
                "url": "https://seaborn.pydata.org/tutorial.html"
              },
              {
                "title": "Python Data Science Handbook - Visualization",
                "author": "Jake VanderPlas",
                "year": 2016
              },
              {
                "title": "The Visual Display of Quantitative Information",
                "author": "Edward Tufte",
                "year": 2001,
                "note": "Classic text on data visualization principles"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "advanced-python",
      "title": "Advanced Python for AI",
      "lessons": [
        {
          "id": "generators-iterators",
          "title": "Generators and Iterators for Large Datasets",
          "duration": "45 min",
          "content": {
            "overview": "When working with datasets too large to fit in memory, generators provide a memory-efficient solution. They generate values on-the-fly, enabling processing of massive datasets, streaming data, and efficient batching for training.",
            "sections": [
              {
                "title": "Understanding Generators",
                "content": "Generators are functions that yield values one at a time instead of returning all at once:",
                "code": "# Regular function - loads all data in memory\ndef get_all_data(n):\n    return [i ** 2 for i in range(n)]  # Creates full list\n\n# Generator - produces values on demand\ndef generate_data(n):\n    for i in range(n):\n        yield i ** 2  # Yields one value at a time\n\n# Memory comparison\nimport sys\nlist_data = get_all_data(1000000)\ngen_data = generate_data(1000000)\n\nprint(f\"List size: {sys.getsizeof(list_data)} bytes\")  # ~8 MB\nprint(f\"Generator size: {sys.getsizeof(gen_data)} bytes\")  # ~120 bytes"
              },
              {
                "title": "Data Loader Pattern",
                "content": "Generators are perfect for creating custom data loaders for ML training:",
                "code": "import numpy as np\nfrom pathlib import Path\n\ndef batch_generator(data_path: str, batch_size: int = 32):\n    \"\"\"Generate batches of data from a large file.\"\"\"\n    data = np.load(data_path, mmap_mode='r')  # Memory-mapped\n    n_samples = len(data)\n    indices = np.random.permutation(n_samples)\n    \n    for start in range(0, n_samples, batch_size):\n        batch_indices = indices[start:start + batch_size]\n        yield data[batch_indices]\n\ndef image_generator(image_dir: str, batch_size: int = 32):\n    \"\"\"Generate batches of images from a directory.\"\"\"\n    image_paths = list(Path(image_dir).glob('*.jpg'))\n    np.random.shuffle(image_paths)\n    \n    batch = []\n    for path in image_paths:\n        img = load_and_preprocess(path)\n        batch.append(img)\n        \n        if len(batch) == batch_size:\n            yield np.array(batch)\n            batch = []\n    \n    if batch:  # Remaining samples\n        yield np.array(batch)\n\n# Usage\nfor batch in batch_generator('train_data.npy', batch_size=64):\n    train_step(model, batch)"
              },
              {
                "title": "Generator Expressions",
                "content": "Generator expressions provide a concise syntax for simple generators:",
                "code": "# List comprehension (creates full list)\nsquares_list = [x**2 for x in range(1000000)]  # 8 MB\n\n# Generator expression (lazy evaluation)\nsquares_gen = (x**2 for x in range(1000000))   # 120 bytes\n\n# Useful for chaining operations\ndata = [1, -2, 3, -4, 5]\npositive = (x for x in data if x > 0)\ndoubled = (x * 2 for x in positive)\nresult = sum(doubled)  # Computes lazily: 18\n\n# Reading large files\ndef process_large_file(filepath):\n    with open(filepath) as f:\n        lines = (line.strip() for line in f)\n        non_empty = (line for line in lines if line)\n        for line in non_empty:\n            yield process_line(line)"
              }
            ],
            "keyTakeaways": [
              "Generators produce values on-demand, saving memory",
              "Use generators for data loaders and processing large datasets",
              "Generator expressions provide concise syntax for simple cases"
            ],
            "exercises": [
              {
                "title": "Custom Data Loader",
                "description": "Create a generator that yields batches of (X, y) tuples for training"
              },
              {
                "title": "Infinite Generator",
                "description": "Create a generator that yields augmented samples indefinitely for training"
              }
            ],
            "sources": [
              {
                "title": "Python Generators",
                "url": "https://docs.python.org/3/howto/functional.html#generators"
              },
              {
                "title": "PyTorch DataLoader",
                "url": "https://pytorch.org/docs/stable/data.html"
              }
            ]
          }
        },
        {
          "id": "async-python",
          "title": "Async Python for AI Applications",
          "duration": "45 min",
          "content": {
            "overview": "Asynchronous programming enables efficient I/O-bound operations like API calls to LLMs, fetching datasets, and serving model predictions. Understanding async is essential for building production AI applications.",
            "sections": [
              {
                "title": "Async Basics",
                "content": "Async functions run concurrently, allowing other code to execute while waiting for I/O:",
                "code": "import asyncio\nimport aiohttp\n\nasync def fetch_embedding(session, text: str) -> list:\n    \"\"\"Fetch embedding from OpenAI API.\"\"\"\n    url = \"https://api.openai.com/v1/embeddings\"\n    payload = {\n        \"model\": \"text-embedding-3-small\",\n        \"input\": text\n    }\n    async with session.post(url, json=payload) as response:\n        data = await response.json()\n        return data['data'][0]['embedding']\n\nasync def embed_batch(texts: list[str]) -> list:\n    \"\"\"Embed multiple texts concurrently.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_embedding(session, text) for text in texts]\n        embeddings = await asyncio.gather(*tasks)\n        return embeddings\n\n# Run async function\ntexts = [\"Hello world\", \"Machine learning\", \"Python programming\"]\nembeddings = asyncio.run(embed_batch(texts))"
              },
              {
                "title": "Async for LLM Applications",
                "content": "Async is crucial for building responsive LLM applications:",
                "code": "import asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nasync def chat_completion(prompt: str) -> str:\n    \"\"\"Get completion from GPT-4.\"\"\"\n    response = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\nasync def parallel_completions(prompts: list[str]) -> list[str]:\n    \"\"\"Process multiple prompts concurrently.\"\"\"\n    tasks = [chat_completion(prompt) for prompt in prompts]\n    return await asyncio.gather(*tasks)\n\nasync def streaming_response(prompt: str):\n    \"\"\"Stream response tokens.\"\"\"\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=True\n    )\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")"
              },
              {
                "title": "Rate Limiting and Semaphores",
                "content": "Control concurrency to respect API rate limits:",
                "code": "import asyncio\n\nclass RateLimiter:\n    \"\"\"Rate limiter for API calls.\"\"\"\n    \n    def __init__(self, max_concurrent: int = 10):\n        self.semaphore = asyncio.Semaphore(max_concurrent)\n    \n    async def call(self, coro):\n        async with self.semaphore:\n            return await coro\n\n# Usage\nlimiter = RateLimiter(max_concurrent=5)\n\nasync def process_all(items):\n    tasks = [\n        limiter.call(process_item(item))\n        for item in items\n    ]\n    return await asyncio.gather(*tasks)\n\n# With retry logic\nasync def with_retry(coro, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return await coro\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff"
              }
            ],
            "keyTakeaways": [
              "Async enables concurrent I/O operations for faster processing",
              "Use asyncio.gather() for parallel API calls",
              "Implement rate limiting with semaphores for production use"
            ],
            "exercises": [
              {
                "title": "Batch Embeddings",
                "description": "Create an async function to embed 100 texts concurrently with rate limiting"
              },
              {
                "title": "Streaming Chat",
                "description": "Build a streaming chat interface using async generators"
              }
            ],
            "sources": [
              {
                "title": "Python asyncio documentation",
                "url": "https://docs.python.org/3/library/asyncio.html"
              },
              {
                "title": "OpenAI Python SDK",
                "url": "https://github.com/openai/openai-python"
              }
            ]
          }
        },
        {
          "id": "python-limitations",
          "title": "Python's Limitations & When NOT to Use It",
          "duration": "30 min",
          "content": {
            "overview": "Understanding Python's limitations is crucial for production AI systems. This lesson covers the GIL, performance considerations, and when to use alternativesâ€”knowledge that separates hobbyists from production engineers.",
            "sections": [
              {
                "title": "The Global Interpreter Lock (GIL)",
                "content": "Python's GIL is the most important limitation to understand:\n\n**What is the GIL?**\nThe Global Interpreter Lock is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecode at once.\n\n**What this means:**\n- Only one thread can execute Python code at a time\n- Multi-threaded Python doesn't fully utilize multiple CPU cores\n- CPU-bound tasks don't speed up with threads\n\n**David Beazley (Python expert):** *'The GIL is Python's greatest weakness and its greatest strengthâ€”it makes Python safe but slow for CPU-bound parallelism.'*",
                "code": "import threading\nimport time\n\n# CPU-bound task\ndef count(n):\n    while n > 0:\n        n -= 1\n\n# Single thread\nstart = time.time()\ncount(100_000_000)\nprint(f'Single thread: {time.time() - start:.2f}s')\n\n# Two threads - NOT faster due to GIL!\nstart = time.time()\nt1 = threading.Thread(target=count, args=(50_000_000,))\nt2 = threading.Thread(target=count, args=(50_000_000,))\nt1.start()\nt2.start()\nt1.join()\nt2.join()\nprint(f'Two threads: {time.time() - start:.2f}s')  # Often SLOWER!\n\n# Solution: Use multiprocessing for CPU-bound tasks\nfrom multiprocessing import Pool\n\nwith Pool(4) as p:\n    results = p.map(count, [25_000_000] * 4)  # Actually parallel"
              },
              {
                "title": "Python's Speed Problem",
                "content": "Python is 10-100x slower than C/C++ for raw computation:\n\n**Why Python is slow:**\n- Interpreted, not compiled\n- Dynamic typing (type checks at runtime)\n- Object overhead (everything is an object)\n- No direct memory access\n\n**Benchmarks (approximate):**\n| Task | Python | C++ | Speedup |\n|------|--------|-----|---------|\n| Loop 10M times | 1.0s | 0.01s | 100x |\n| Matrix multiply | 10s | 0.1s | 100x |\n| String processing | 1.0s | 0.05s | 20x |\n\n**Why AI still uses Python:**\n- NumPy, PyTorch, TensorFlow use C/C++ backends\n- Python is just the 'glue' language\n- Actual computation happens in optimized libraries",
                "code": "import numpy as np\nimport time\n\n# SLOW: Pure Python\ndef python_dot(a, b):\n    total = 0\n    for i in range(len(a)):\n        total += a[i] * b[i]\n    return total\n\n# FAST: NumPy (C backend)\na = np.random.randn(10_000_000)\nb = np.random.randn(10_000_000)\na_list = a.tolist()\nb_list = b.tolist()\n\nstart = time.time()\npython_dot(a_list, b_list)\nprint(f'Pure Python: {time.time() - start:.3f}s')\n\nstart = time.time()\nnp.dot(a, b)\nprint(f'NumPy: {time.time() - start:.3f}s')  # 100-1000x faster!"
              },
              {
                "title": "When NOT to Use Python",
                "content": "Choose a different language for these scenarios:\n\n**âŒ Don't use Python for:**\n\n| Use Case | Better Choice | Why |\n|----------|--------------|-----|\n| **Mobile apps** | Swift, Kotlin | No native mobile support |\n| **Real-time systems** | C, Rust | GC pauses, too slow |\n| **High-frequency trading** | C++, Rust | Microsecond latency matters |\n| **Browser frontend** | JavaScript | Browsers don't run Python |\n| **Embedded systems** | C, MicroPython | Memory constraints |\n| **Game engines** | C++, C# | Frame rate critical |\n\n**âœ… Use Python for:**\n- Data science and ML research\n- Backend APIs and web services\n- Automation and scripting\n- Prototyping and MVPs\n- Glue code between systems"
              },
              {
                "title": "Memory Management Gotchas",
                "content": "Python's automatic memory management can cause issues at scale:",
                "code": "import sys\nimport numpy as np\n\n# Object overhead is huge\nx = 42\nprint(sys.getsizeof(x))  # 28 bytes for a single integer!\n\n# Lists use more memory than arrays\npy_list = list(range(1_000_000))\nnp_array = np.arange(1_000_000, dtype=np.int64)\n\nprint(f'Python list: {sys.getsizeof(py_list) / 1e6:.1f} MB')  # ~8 MB\nprint(f'NumPy array: {np_array.nbytes / 1e6:.1f} MB')          # ~8 MB\n# But list elements each have 28+ byte overhead!\n\n# Garbage collection pauses\nimport gc\n\n# Disable GC during critical sections\ngc.disable()\ntry:\n    # Time-critical code\n    fast_inference(data)\nfinally:\n    gc.enable()\n    gc.collect()  # Manually trigger collection"
              },
              {
                "title": "Workarounds and Solutions",
                "content": "Techniques to overcome Python's limitations:",
                "code": "# 1. Use NumPy/PyTorch for numerical code\nimport numpy as np\nimport torch\n\n# 2. Multiprocessing for CPU parallelism\nfrom multiprocessing import Pool, cpu_count\n\ndef process_batch(data):\n    return heavy_computation(data)\n\nwith Pool(cpu_count()) as pool:\n    results = pool.map(process_batch, data_batches)\n\n# 3. Numba for JIT compilation (10-100x speedup)\nfrom numba import jit\n\n@jit(nopython=True)\ndef fast_loop(n):\n    total = 0\n    for i in range(n):\n        total += i * i\n    return total\n\n# 4. Cython for C-speed Python\n# Compile Python-like code to C\n\n# 5. asyncio for I/O parallelism\nimport asyncio\n\nasync def fetch_all(urls):\n    tasks = [fetch(url) for url in urls]\n    return await asyncio.gather(*tasks)\n\n# 6. Move critical code to C/Rust extensions\n# PyTorch, NumPy, etc. do this internally"
              }
            ],
            "keyTakeaways": [
              "The GIL prevents true multi-threaded parallelism for CPU-bound code",
              "Use multiprocessing (not threading) for CPU parallelism",
              "Python's speed comes from C backends (NumPy, PyTorch)",
              "Don't use Python for mobile, real-time, or browser apps",
              "Numba and Cython can dramatically speed up Python code"
            ],
            "exercises": [
              {
                "title": "GIL Demonstration",
                "description": "Write a CPU-bound benchmark comparing single-threaded, multi-threaded, and multiprocessing execution"
              },
              {
                "title": "Numba Speedup",
                "description": "Take a slow Python loop and use Numba's @jit decorator to speed it up"
              }
            ],
            "sources": [
              {
                "title": "Understanding the Python GIL",
                "author": "David Beazley",
                "url": "https://www.dabeaz.com/python/UnderstandingGIL.pdf",
                "year": 2010,
                "note": "The definitive talk on the GIL"
              },
              {
                "title": "Python Concurrency from the Ground Up",
                "author": "David Beazley",
                "url": "https://www.youtube.com/watch?v=MCs5OvhV9S4",
                "year": 2015
              },
              {
                "title": "Numba Documentation",
                "url": "https://numba.pydata.org/"
              },
              {
                "title": "Python Performance Tips",
                "url": "https://wiki.python.org/moin/PythonSpeed/PerformanceTips"
              }
            ]
          }
        },
        {
          "id": "python-ai-quiz",
          "title": "Python for AI Quiz",
          "type": "quiz",
          "duration": "15 min",
          "questions": [
            {
              "id": "python-ai-benefits",
              "question": "Which of the following is NOT a primary reason Python is dominant in AI?",
              "options": [
                "Rich ecosystem of libraries (NumPy, PyTorch)",
                "Readable and concise syntax",
                "Fastest raw execution speed compared to C++",
                "Strong community support"
              ],
              "explanation": "Python is interpreted and generally slower than C++ in raw execution. Its speed in AI comes from optimized C/C++ backends used by libraries like NumPy and PyTorch.",
              "correct": 2
            },
            {
              "id": "numpy-arrays",
              "question": "What is a key advantage of NumPy arrays over standard Python lists?",
              "options": [
                "They can hold mixed data types",
                "They support vectorized operations and are memory efficient",
                "They are built-in to the Python standard library",
                "They are slower but more flexible"
              ],
              "explanation": "NumPy arrays store data in contiguous memory blocks, allowing for SIMD vectorization and significantly faster numerical computations compared to lists.",
              "correct": 1
            },
            {
              "id": "pandas-dataframe",
              "question": "Which Pandas data structure is primarily used for 2D tabular data?",
              "options": [
                "Series",
                "DataFrame",
                "Panel",
                "Tensor"
              ],
              "explanation": "The DataFrame is the primary 2D labeled data structure in Pandas, similar to a SQL table or Excel spreadsheet.",
              "correct": 1
            },
            {
              "id": "ml-types",
              "question": "According to IBM, which type of machine learning is used to 'find patterns in unlabeled data'?",
              "options": [
                "Supervised learning",
                "Unsupervised learning",
                "Reinforcement learning",
                "Transfer learning"
              ],
              "explanation": "IBM defines unsupervised learning as algorithms that 'discern intrinsic patterns in unlabeled data, such as similarities, correlations or potential groupings' - useful for clustering and anomaly detection.",
              "correct": 1,
              "references": [
                {"title": "IBM: What is Machine Learning?", "url": "https://www.ibm.com/topics/machine-learning"}
              ]
            },
            {
              "id": "sklearn-pattern",
              "question": "What is the consistent API pattern used by all scikit-learn models?",
              "options": [
                "train() and test()",
                "fit() and predict()",
                "learn() and infer()",
                "compile() and run()"
              ],
              "explanation": "Scikit-learn uses a consistent estimator API: fit(X, y) to train the model and predict(X) to make predictions. This consistency makes it easy to swap models.",
              "correct": 1,
              "references": [
                {"title": "IBM: What is Scikit-Learn?", "url": "https://www.ibm.com/topics/scikit-learn"}
              ]
            },
            {
              "id": "data-leakage",
              "question": "What is 'data leakage' in machine learning?",
              "options": [
                "When your model is too large for memory",
                "When test data information leaks into training",
                "When data is stored insecurely",
                "When predictions are too slow"
              ],
              "explanation": "Data leakage occurs when information from the test set influences training (e.g., scaling entire dataset before splitting). Use Pipelines to prevent this!",
              "correct": 1
            },
            {
              "id": "preprocessing-scaler",
              "question": "According to IBM, what does StandardScaler do in scikit-learn?",
              "options": [
                "Converts text to numbers",
                "Scales features to zero mean and unit variance",
                "Removes duplicate rows",
                "Fills missing values"
              ],
              "explanation": "StandardScaler is a normalization technique that scales numeric features to have similar magnitudes - specifically zero mean and unit standard deviation.",
              "correct": 1,
              "references": [
                {"title": "IBM: What is Scikit-Learn?", "url": "https://www.ibm.com/topics/scikit-learn"}
              ]
            },
            {
              "id": "generators",
              "question": "Why are Python generators useful for training AI models on large datasets?",
              "options": [
                "They load the entire dataset into RAM at once",
                "They yield data one batch at a time, saving memory",
                "They automatically parallelize code on GPUs",
                "They compress the data on disk"
              ],
              "explanation": "Generators produce values lazily (on demand), allowing you to iterate over massive datasets without loading everything into memory simultaneously.",
              "correct": 1
            },
            {
              "id": "async-io",
              "question": "What is the main benefit of using `asyncio` in AI applications?",
              "options": [
                "It makes the model training faster",
                "It allows concurrent handling of I/O-bound tasks like API calls",
                "It replaces the need for GPUs",
                "It automatically optimizes hyperparameters"
              ],
              "explanation": "Asyncio enables concurrent execution, which is ideal for I/O-bound operations such as making multiple API requests to LLMs or fetching data without blocking the main thread.",
              "correct": 1
            },
            {
              "id": "cross-validation",
              "question": "Why is cross-validation more reliable than a single train/test split?",
              "options": [
                "It uses less memory",
                "It trains faster",
                "It evaluates performance across multiple data splits",
                "It automatically tunes hyperparameters"
              ],
              "explanation": "Cross-validation trains and tests on multiple different splits of the data, giving a more robust estimate of model performance that's less dependent on how the data was randomly divided.",
              "correct": 2
            }
          ],
          "references": {
            "lessonRefs": [
              "numpy-basics",
              "pandas-basics",
              "sklearn-basics"
            ],
            "externalRefs": [
              {
                "title": "IBM: What is Machine Learning?",
                "url": "https://www.ibm.com/topics/machine-learning"
              },
              {
                "title": "IBM: What is Scikit-Learn?",
                "url": "https://www.ibm.com/topics/scikit-learn"
              },
              {
                "title": "IBM: What is Data Science?",
                "url": "https://www.ibm.com/topics/data-science"
              },
              {
                "title": "NumPy Documentation",
                "url": "https://numpy.org/doc/"
              },
              {
                "title": "Pandas Documentation",
                "url": "https://pandas.pydata.org/docs/"
              }
            ]
          }
        }
      ]
    }
  ]
}