{
  "id": "rag-applications",
  "title": "Building RAG Applications",
  "icon": "ðŸ”—",
  "description": "Build Retrieval-Augmented Generation apps with LangChain and vector databases. RAG is an architecture for optimizing AI performance by connecting models with external knowledge bases, enabling more relevant, accurate responses grounded in authoritative data.",
  "level": "advanced",
  "duration": "4 weeks",
  "totalLessons": 20,
  "validationSources": [
    "https://www.ibm.com/topics/retrieval-augmented-generation"
  ],
  "prerequisites": [
    "Python basics (variables, functions, packages)",
    "Understanding of what LLMs are (ChatGPT, Claude)",
    "Ability to get an OpenAI API key"
  ],
  "whatYouNeed": {
    "skills": "If you can write Python functions and have used ChatGPT, you're ready!",
    "tools": "Python 3.9+, pip, OpenAI API key (or Ollama for free local option)",
    "time": "4 weeks, ~5 hours per week"
  },
  "outcomes": [
    "Understand RAG architecture and components",
    "Build production-ready RAG applications",
    "Implement advanced retrieval strategies",
    "Optimize RAG performance"
  ],
  "modules": [
    {
      "title": "Getting Started with RAG",
      "description": "Understand what RAG is and set up your environment",
      "lessons": [
        {
          "id": "rag-intro",
          "title": "What is RAG? (Plain English)",
          "type": "lesson",
          "duration": "10 min",
          "content": {
            "overview": "Before diving into code, let's understand what RAG is and why it's one of the most important patterns in AI applications today.",
            "sections": [
              {
                "title": "The Problem: LLMs Have Limitations",
                "content": "You've probably noticed ChatGPT sometimes:\n\nâŒ **Gets facts wrong** - 'hallucinations'\nâŒ **Doesn't know recent events** - training cutoff\nâŒ **Can't read your documents** - no access to your data\nâŒ **Says 'I don't know'** - for company-specific questions\n\n**Example**: Ask ChatGPT about your company's vacation policy. It can't answer because it has never seen your employee handbook!"
              },
              {
                "title": "The Solution: RAG",
                "content": "**RAG = Retrieval-Augmented Generation**\n\nIn plain English: **Give the AI your documents before it answers.**\n\nThink of it like:\n- âŒ Without RAG: Student taking a test from memory\n- âœ… With RAG: Student with open book, finding relevant pages first\n\nRAG lets the AI:\n1. **Search** your documents for relevant info\n2. **Read** the relevant parts\n3. **Answer** based on what it found\n\nResult: Accurate answers grounded in YOUR data!"
              },
              {
                "title": "Real-World RAG Examples",
                "content": "| Use Case | What RAG Does |\n|----------|---------------|\n| Customer Support | Searches your FAQ/docs to answer questions |\n| Legal Research | Finds relevant case law and clauses |\n| Code Assistant | Searches your codebase for context |\n| Study Helper | Answers questions from your textbooks |\n| Enterprise Search | Finds info across company docs |"
              },
              {
                "title": "The RAG Recipe (5 Key Stages)",
                "content": "Every RAG system follows five key stages (based on LlamaIndex framework):\n\n**1. LOADING** - Get data from where it lives\n- PDFs, Word docs, websites, databases, APIs\n- LlamaHub has 100+ connectors\n\n**2. INDEXING** - Create searchable structure\n- Split into chunks (nodes)\n- Convert to embeddings (vectors)\n- Add metadata for filtering\n\n**3. STORING** - Persist for reuse\n- Save to vector database\n- Avoid re-embedding on every query\n\n**4. QUERYING** - Find relevant info\n- Embed user's question\n- Search for similar chunks\n- Get top-k matches\n\n**5. EVALUATION** - Measure quality\n- Is the answer correct?\n- Did we retrieve the right chunks?\n- How fast is it?",
                "diagram": {
                  "title": "RAG Pipeline (5 Stages)",
                  "code": "flowchart LR\n    subgraph Load[\"1. Loading\"]\n        D[Documents] --> L[Loader]\n    end\n    \n    subgraph Index[\"2. Indexing\"]\n        L --> C[Chunk]\n        C --> E[Embed]\n    end\n    \n    subgraph Store[\"3. Storing\"]\n        E --> V[(Vector DB)]\n    end\n    \n    subgraph Query[\"4. Querying\"]\n        Q[Question] --> QE[Embed Query]\n        QE --> S{Search}\n        V --> S\n        S --> LLM[Generate]\n    end\n    \n    subgraph Eval[\"5. Evaluation\"]\n        LLM --> M[Metrics]\n    end"
                }
              },
              {
                "title": "Official Learning Resources",
                "content": "**Recommended Documentation:**\n\n| Resource | Best For |\n|----------|----------|\n| [LlamaIndex RAG Guide](https://developers.llamaindex.ai/python/framework/understanding/rag/) | Understanding RAG concepts |\n| [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/) | Hands-on implementation |\n| [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings) | Understanding embeddings |\n| [Pinecone Learning](https://www.pinecone.io/learn/) | Vector database concepts |"
              },
              {
                "title": "Beginner Questions",
                "content": "**Q: Is this hard to build?**\nA: With LangChain, you can build a basic RAG in 50 lines of code!\n\n**Q: Do I need expensive GPUs?**\nA: No! We use cloud APIs or lightweight local models.\n\n**Q: What documents can I use?**\nA: PDFs, Word docs, text files, web pages, databases - almost anything!\n\n**Q: How much does it cost?**\nA: OpenAI: ~$0.01 per query. Ollama: Free (runs locally)."
              }
            ],
            "keyTakeaways": [
              "RAG = Give AI your documents so it can answer accurately",
              "Three steps: Prepare docs â†’ Find relevant parts â†’ Generate answer",
              "Solves hallucinations, knowledge cutoff, and private data access"
            ]
          }
        },
        {
          "id": "rag-setup",
          "title": "Setting Up Your RAG Environment",
          "type": "lesson",
          "duration": "15 min",
          "content": {
            "overview": "Let's install everything you need to build RAG applications.",
            "sections": [
              {
                "title": "Install Required Packages",
                "content": "Open your terminal and run:",
                "code": "# Create a new project folder\nmkdir my-rag-project\ncd my-rag-project\n\n# Create virtual environment (keeps things clean)\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# venv\\Scripts\\activate   # Windows\n\n# Install packages\npip install langchain langchain-openai chromadb\npip install pypdf python-docx  # For PDF and Word docs"
              },
              {
                "title": "Get Your API Key",
                "content": "**Option 1: OpenAI (Paid but powerful)**\n1. Go to [platform.openai.com](https://platform.openai.com)\n2. Sign up / Log in\n3. Go to API Keys\n4. Create new key\n5. Save it somewhere safe!\n\n**Option 2: Ollama (Free, local)**\n1. Install from [ollama.ai](https://ollama.ai)\n2. Run: `ollama pull llama3`\n3. Run: `ollama pull nomic-embed-text`\n4. No API key needed!",
                "code": "# Set your API key (Option 1: OpenAI)\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-your-key-here\"\n\n# Or use a .env file (better)\n# Create file: .env\n# Add line: OPENAI_API_KEY=sk-your-key-here\n\n# Load it:\nfrom dotenv import load_dotenv\nload_dotenv()"
              },
              {
                "title": "Test Your Setup",
                "content": "Run this to verify everything works:",
                "code": "# test_setup.py\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\n\n# Test embeddings\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\ntest_vector = embeddings.embed_query(\"Hello world\")\nprint(f\"âœ… Embeddings work! Vector length: {len(test_vector)}\")\n\n# Test LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nresponse = llm.invoke(\"Say 'Hello RAG!' in one sentence\")\nprint(f\"âœ… LLM works! Response: {response.content}\")\n\nprint(\"\\nðŸŽ‰ You're ready to build RAG apps!\")"
              }
            ],
            "keyTakeaways": [
              "Use virtual environments to keep dependencies clean",
              "You need LangChain + vector database + LLM",
              "OpenAI is easiest, Ollama is free"
            ]
          }
        }
      ]
    },
    {
      "title": "RAG Fundamentals",
      "description": "Understanding RAG architecture",
      "lessons": [
        {
          "id": "what-is-rag",
          "title": "What is RAG and Why Use It?",
          "type": "lesson",
          "duration": "15 min",
          "content": {
            "overview": "Retrieval-Augmented Generation (RAG) is an architecture for optimizing AI performance by connecting LLMs with external knowledge bases. The system retrieves relevant data, augments the prompt with enhanced context, and generates responses grounded in authoritative sources.",
            "sections": [
              {
                "title": "The Problem RAG Solves",
                "content": "LLMs have limitations:\n\n**Knowledge Cutoff**: Training data has a date limit - models lose relevance over time\n**Hallucinations**: Can generate plausible but false information (confabulations)\n**No Private Data**: Cannot access your documents or proprietary information\n**Static Knowledge**: Cannot update without expensive retraining\n\nRAG anchors LLMs in specific knowledge backed by factual, authoritative and current data."
              },
              {
                "title": "RAG Benefits",
                "content": "**Cost-Efficient Scaling**: Avoid expensive retraining or fine-tuning - just update knowledge base\n\n**Access Current Data**: Connect to real-time data sources, breaking free from knowledge cutoff\n\n**Lower Hallucination Risk**: Ground responses in retrieved, verifiable information\n\n**Increased Trust**: Cite sources so users can verify outputs\n\n**Greater Control**: Developers can adjust knowledge sources without modifying the model\n\n**Data Security**: External data stays separate from model - access can be revoked"
              },
              {
                "title": "RAG Components",
                "content": "A RAG system has four primary components:\n\n1. **Knowledge Base**: External data repository (documents â†’ embeddings â†’ vectors)\n2. **Retriever**: AI model that searches knowledge base for relevant data\n3. **Integration Layer**: Coordinates processes, engineers augmented prompts\n4. **Generator**: LLM that creates output based on query + retrieved context"
              },
              {
                "title": "RAG Architecture",
                "content": "The RAG pipeline has three main stages:\n\n1. **Indexing**: Documents are chunked, embedded, and stored\n2. **Retrieval**: Query is embedded and similar chunks are found\n3. **Generation**: Retrieved context + query are sent to LLM",
                "diagram": {
                  "title": "RAG Pipeline Overview",
                  "code": "flowchart LR\n    subgraph Indexing[\"1. Indexing\"]\n        D[Documents] --> C[Chunking]\n        C --> E[Embedding]\n        E --> V[(Vector DB)]\n    end\n    \n    subgraph Retrieval[\"2. Retrieval\"]\n        Q[Query] --> QE[Query Embed]\n        QE --> S{Similarity Search}\n        V --> S\n        S --> R[Top-K Chunks]\n    end\n    \n    subgraph Generation[\"3. Generation\"]\n        R --> P[Prompt + Context]\n        Q --> P\n        P --> LLM[LLM]\n        LLM --> A[Answer]\n    end"
                },
                "code": "# Simple RAG with LangChain\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\n\n# 1. Create embeddings and vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(\n    documents=docs,\n    embedding=embeddings\n)\n\n# 2. Create retriever\nretriever = vectorstore.as_retriever(\n    search_kwargs={\"k\": 4}\n)\n\n# 3. Create RAG chain\nllm = ChatOpenAI(model=\"gpt-4\")\nrag_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    retriever=retriever,\n    return_source_documents=True\n)\n\n# Query\nresult = rag_chain({\"query\": \"What is our refund policy?\"})\nprint(result[\"result\"])"
              }
            ],
            "keyTakeaways": [
              "RAG grounds LLM responses in your data",
              "Three stages: Index, Retrieve, Generate",
              "Reduces hallucinations and enables private knowledge"
            ]
          }
        },
        {
          "id": "embeddings",
          "title": "Text Embeddings Explained",
          "type": "lesson",
          "duration": "20 min",
          "content": {
            "overview": "Embeddings are dense vector representations of text that capture semantic meaning.",
            "sections": [
              {
                "title": "What Are Embeddings?",
                "content": "Embeddings convert text into numerical vectors where:\n\n- Similar meanings = Close vectors\n- Different meanings = Distant vectors\n\nThis enables semantic search rather than keyword matching.",
                "diagram": {
                  "title": "Embedding Space Visualization",
                  "code": "flowchart TB\n    subgraph Vector[\"Embedding Space\"]\n        direction TB\n        K[king] --> |\"- man + woman\"| Q[queen]\n        M[man] -.-> W[woman]\n        C[cat] --> |\"similar\"| D[dog]\n        C -.-> |\"distant\"| CAR[car]\n    end\n    \n    T[\"Text: 'king'\"] --> |Embed| V1[\"[0.2, -0.5, 0.8, ...]\"]\n    V1 --> Vector"
                }
              },
              {
                "title": "Embedding Models",
                "code": "# OpenAI Embeddings\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# Open-source alternatives\nfrom langchain.embeddings import HuggingFaceEmbeddings\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n\n# Generate embeddings\ntext = \"The quick brown fox\"\nvector = embeddings.embed_query(text)\nprint(f\"Dimensions: {len(vector)}\")  # 1536 for OpenAI"
              }
            ],
            "keyTakeaways": [
              "Embeddings capture semantic meaning as vectors",
              "Similar texts have similar vectors",
              "Choose embedding model based on use case"
            ]
          }
        },
        {
          "id": "vector-dbs",
          "title": "Vector Databases Overview",
          "type": "lesson",
          "duration": "20 min",
          "content": {
            "overview": "Vector databases are optimized for storing and searching embedding vectors efficiently.",
            "sections": [
              {
                "title": "Why Vector Databases?",
                "content": "Regular databases cannot efficiently search by semantic similarity. Vector databases:\n\n- Index high-dimensional vectors\n- Find nearest neighbors quickly\n- Scale to millions of vectors\n- Support metadata filtering"
              },
              {
                "title": "Popular Vector DBs",
                "code": "# Chroma - Simple, local-first\nfrom langchain.vectorstores import Chroma\ndb = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\")\n\n# Pinecone - Managed, scalable\nfrom langchain.vectorstores import Pinecone\nimport pinecone\npinecone.init(api_key=\"xxx\", environment=\"us-west1-gcp\")\ndb = Pinecone.from_documents(docs, embeddings, index_name=\"my-index\")\n\n# Weaviate - Open source, feature-rich\nfrom langchain.vectorstores import Weaviate\nimport weaviate\nclient = weaviate.Client(url=\"http://localhost:8080\")\ndb = Weaviate.from_documents(docs, embeddings, client=client)"
              }
            ],
            "keyTakeaways": [
              "Vector DBs enable fast similarity search",
              "Choose based on scale, cost, and features",
              "Chroma is great for development, Pinecone for production"
            ]
          }
        },
        {
          "id": "architecture",
          "title": "RAG Architecture Patterns",
          "type": "lesson",
          "duration": "15 min",
          "content": {
            "overview": "Different RAG architectures for different use cases.",
            "sections": [
              {
                "title": "Naive RAG",
                "content": "The simplest RAG pattern:\n\n1. Embed query\n2. Find top-k similar chunks\n3. Send chunks + query to LLM\n\nWorks for simple Q&A but has limitations."
              },
              {
                "title": "Advanced RAG Patterns",
                "content": "**Sentence Window**: Retrieve small chunks, expand context\n**Auto-Merging**: Retrieve children, include parents\n**Hypothetical Embeddings (HyDE)**: Generate hypothetical answer, use its embedding\n**Self-Query**: LLM generates metadata filters\n**Multi-Index**: Different indexes for different doc types"
              }
            ]
          }
        }
      ]
    },
    {
      "title": "Building with LangChain",
      "description": "Implement RAG using LangChain",
      "lessons": [
        {
          "id": "langchain-intro",
          "title": "LangChain Introduction",
          "type": "lesson",
          "duration": "15 min",
          "content": {
            "overview": "LangChain is a framework for building LLM applications with composable components.",
            "sections": [
              {
                "title": "Core Concepts",
                "content": "LangChain provides:\n\n- **Models**: Wrappers for LLMs and embeddings\n- **Prompts**: Templates and management\n- **Chains**: Sequences of operations\n- **Agents**: LLMs that use tools\n- **Memory**: Conversation state\n- **Retrievers**: Document retrieval"
              },
              {
                "title": "Basic Chain",
                "code": "from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\n\n# Modern LCEL (LangChain Expression Language)\nprompt = ChatPromptTemplate.from_template(\n    \"Explain {topic} in simple terms\"\n)\nmodel = ChatOpenAI(model=\"gpt-4\")\nparser = StrOutputParser()\n\nchain = prompt | model | parser\n\nresult = chain.invoke({\"topic\": \"quantum computing\"})\nprint(result)"
              }
            ]
          }
        },
        {
          "id": "document-loaders",
          "title": "Document Loaders and Splitters",
          "type": "lesson",
          "duration": "20 min",
          "content": {
            "overview": "Loading and chunking documents properly is critical for RAG quality.",
            "sections": [
              {
                "title": "Document Loaders",
                "code": "from langchain.document_loaders import (\n    PyPDFLoader,\n    TextLoader,\n    WebBaseLoader,\n    DirectoryLoader\n)\n\n# Load PDF\nloader = PyPDFLoader(\"document.pdf\")\ndocs = loader.load()\n\n# Load website\nloader = WebBaseLoader(\"https://example.com\")\ndocs = loader.load()\n\n# Load directory\nloader = DirectoryLoader(\"./docs\", glob=\"**/*.md\")\ndocs = loader.load()"
              },
              {
                "title": "Text Splitters",
                "code": "from langchain.text_splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n)\n\nchunks = splitter.split_documents(docs)\nprint(f\"Created {len(chunks)} chunks\")"
              }
            ]
          }
        },
        {
          "id": "retrievers",
          "title": "Creating Retrievers",
          "type": "lesson",
          "duration": "20 min",
          "content": {
            "overview": "Retrievers find the most relevant documents for a given query.",
            "sections": [
              {
                "title": "Basic Retriever",
                "code": "# Vector store retriever\nretriever = vectorstore.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 4}\n)\n\n# Get relevant docs\ndocs = retriever.get_relevant_documents(\"What is the return policy?\")"
              },
              {
                "title": "Advanced Retrievers",
                "code": "# Multi-Query Retriever - generates multiple queries\nfrom langchain.retrievers import MultiQueryRetriever\nretriever = MultiQueryRetriever.from_llm(\n    retriever=base_retriever,\n    llm=llm\n)\n\n# Contextual Compression - filters results\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\ncompressor = LLMChainExtractor.from_llm(llm)\nretriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=base_retriever\n)"
              }
            ]
          }
        },
        {
          "id": "rag-chains",
          "title": "Building RAG Chains",
          "type": "hands-on",
          "duration": "30 min",
          "content": {
            "overview": "Putting it all together into a complete RAG application.",
            "sections": [
              {
                "title": "Complete RAG Chain",
                "code": "from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser\n\n# RAG prompt\ntemplate = \"\"\"Answer based on the following context:\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\nmodel = ChatOpenAI(model=\"gpt-4\")\n\n# Format docs\ndef format_docs(docs):\n    return \"\\n\\n\".join(d.page_content for d in docs)\n\n# RAG chain with LCEL\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\n# Query\nresult = rag_chain.invoke(\"What is the refund policy?\")\nprint(result)"
              }
            ]
          }
        }
      ]
    },
    {
      "title": "Advanced Retrieval",
      "description": "Improve retrieval quality with advanced techniques",
      "lessons": [
        {
          "id": "chunking-strategies",
          "title": "Advanced Chunking Strategies",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Chunking strategy dramatically affects RAG quality. Learn advanced techniques beyond simple text splitting.",
            "sections": [
              {
                "title": "Semantic Chunking",
                "content": "**Problems with Fixed-Size Chunks**\n- Breaks context mid-sentence\n- Ignores document structure\n- Loses relationships between sections\n\n**Semantic Chunking Approaches**\n- Split by sentence/paragraph boundaries\n- Use heading structure for documents\n- Cluster similar sentences together\n- Consider topic boundaries",
                "code": "from langchain.text_splitter import (\n    RecursiveCharacterTextSplitter,\n    MarkdownHeaderTextSplitter,\n    SemanticChunker\n)\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Markdown-aware splitting\nmd_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on=[\n        (\"#\", \"Header 1\"),\n        (\"##\", \"Header 2\"),\n        (\"###\", \"Header 3\")\n    ]\n)\nmd_chunks = md_splitter.split_text(markdown_doc)\n\n# Semantic chunking based on embedding similarity\nsemantic_chunker = SemanticChunker(\n    embeddings=OpenAIEmbeddings(),\n    breakpoint_threshold_type=\"percentile\",\n    breakpoint_threshold_amount=95\n)\nsemantic_chunks = semantic_chunker.split_text(text)"
              },
              {
                "title": "Parent-Child Chunking",
                "content": "Store small chunks for precise retrieval, but return larger parent context.",
                "code": "from langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\n\n# Small chunks for retrieval\nchild_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=400,\n    chunk_overlap=50\n)\n\n# Large chunks for context\nparent_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=2000,\n    chunk_overlap=200\n)\n\n# Store parent docs\ndocstore = InMemoryStore()\n\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=docstore,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter\n)\n\n# Add documents\nretriever.add_documents(docs)\n\n# Retrieves parent chunks based on child matches\nresults = retriever.get_relevant_documents(\"What is the policy?\")"
              }
            ],
            "keyTakeaways": [
              "Chunking strategy is crucial for RAG quality",
              "Respect document structure when chunking",
              "Parent-child retrieval balances precision and context",
              "Experiment with chunk sizes for your use case"
            ]
          }
        },
        {
          "id": "hybrid-search",
          "title": "Hybrid Search Strategies",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Combine semantic and keyword search for better retrieval across different query types.",
            "sections": [
              {
                "title": "Why Hybrid Search?",
                "content": "**Semantic Search Strengths**\n- Understands meaning and context\n- Handles synonyms and paraphrasing\n- Works across languages\n\n**Semantic Search Weaknesses**\n- May miss exact terms (product codes, names)\n- Can be confused by rare words\n- Computationally expensive\n\n**Keyword Search Strengths**\n- Precise for exact matches\n- Fast and efficient\n- Handles rare/technical terms\n\n**Hybrid = Best of Both Worlds**"
              },
              {
                "title": "Implementing Hybrid Search",
                "code": "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n\n# Semantic retriever (vector search)\nvector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n\n# Keyword retriever (BM25)\nbm25_retriever = BM25Retriever.from_documents(docs)\nbm25_retriever.k = 4\n\n# Combine with ensemble\nensemble_retriever = EnsembleRetriever(\n    retrievers=[vector_retriever, bm25_retriever],\n    weights=[0.5, 0.5]  # Equal weights\n)\n\n# Or use reciprocal rank fusion\nfrom langchain.retrievers import RRFRetriever\n\nrrf_retriever = RRFRetriever(\n    retrievers=[vector_retriever, bm25_retriever],\n    c=60  # RRF constant\n)\n\nresults = ensemble_retriever.get_relevant_documents(\"SKU-12345\")"
              },
              {
                "title": "Weaviate Hybrid Search",
                "code": "import weaviate\n\nclient = weaviate.Client(url=\"http://localhost:8080\")\n\n# Hybrid query with alpha parameter\n# alpha=1.0: pure vector, alpha=0.0: pure keyword\nresult = client.query.get(\n    \"Document\",\n    [\"content\", \"title\"]\n).with_hybrid(\n    query=\"machine learning fundamentals\",\n    alpha=0.75  # 75% vector, 25% keyword\n).with_limit(5).do()"
              }
            ],
            "keyTakeaways": [
              "Semantic search misses exact terms",
              "BM25 provides strong keyword matching",
              "Hybrid search handles diverse query types",
              "Tune weights based on your use case"
            ]
          }
        },
        {
          "id": "reranking",
          "title": "Reranking for Better Results",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Reranking uses a more powerful model to reorder retrieved results for higher relevance.",
            "sections": [
              {
                "title": "Why Rerank?",
                "content": "**Two-Stage Retrieval**\n1. **First Stage**: Fast retrieval of top-100 candidates\n   - Embedding similarity or BM25\n   - Optimized for speed\n2. **Second Stage**: Accurate reranking of candidates\n   - Cross-encoder or LLM\n   - Optimized for quality\n\n**Cross-Encoders vs Bi-Encoders**\n- Bi-encoder: Embed query and doc separately (fast)\n- Cross-encoder: Process query+doc together (accurate)"
              },
              {
                "title": "Implementing Reranking",
                "code": "from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder\n\n# Cross-encoder reranker\nreranker = HuggingFaceCrossEncoder(\n    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n)\ncompressor = CrossEncoderReranker(\n    model=reranker,\n    top_n=5  # Return top 5 after reranking\n)\n\n# Wrap the base retriever\nreranking_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=base_retriever  # Retrieves 20 candidates\n)\n\nresults = reranking_retriever.get_relevant_documents(query)"
              },
              {
                "title": "Cohere Rerank",
                "code": "from langchain.retrievers.document_compressors import CohereRerank\nfrom langchain.retrievers import ContextualCompressionRetriever\n\n# Cohere's powerful reranker\ncompressor = CohereRerank(\n    model=\"rerank-english-v3.0\",\n    top_n=5\n)\n\nreranking_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=base_retriever\n)\n\n# Significant quality improvement for minimal latency cost"
              }
            ],
            "keyTakeaways": [
              "Reranking significantly improves result quality",
              "Cross-encoders are more accurate than bi-encoders",
              "Retrieve many candidates, rerank to top-k",
              "Cohere Rerank offers production-ready solution"
            ]
          }
        },
        {
          "id": "query-transformation",
          "title": "Query Transformation Techniques",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Transform user queries to improve retrieval quality through expansion, decomposition, and reformulation.",
            "sections": [
              {
                "title": "Multi-Query Retrieval",
                "content": "Generate multiple perspectives of the query to improve recall.",
                "code": "from langchain.retrievers import MultiQueryRetriever\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(temperature=0)\n\nmulti_query_retriever = MultiQueryRetriever.from_llm(\n    retriever=base_retriever,\n    llm=llm\n)\n\n# Generates multiple query versions:\n# Original: \"What are the side effects?\"\n# Generated:\n# - \"What adverse reactions can occur?\"\n# - \"List negative effects and symptoms\"\n# - \"What problems may patients experience?\"\n\nresults = multi_query_retriever.get_relevant_documents(\n    \"What are the side effects?\"\n)"
              },
              {
                "title": "HyDE (Hypothetical Document Embeddings)",
                "content": "Generate a hypothetical answer, then use its embedding for retrieval.",
                "code": "from langchain.chains import HypotheticalDocumentEmbedder\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.chat_models import ChatOpenAI\n\nbase_embeddings = OpenAIEmbeddings()\nllm = ChatOpenAI(temperature=0)\n\n# HyDE embeddings\nhyde_embeddings = HypotheticalDocumentEmbedder.from_llm(\n    llm=llm,\n    base_embeddings=base_embeddings,\n    prompt_key=\"web_search\"  # or \"question_answer\"\n)\n\n# Query: \"What is photosynthesis?\"\n# HyDE generates: \"Photosynthesis is the process by which plants...\"\n# Then uses that text's embedding for retrieval\n\nvectorstore = Chroma.from_documents(docs, hyde_embeddings)\nresults = vectorstore.similarity_search(\"What is photosynthesis?\")"
              },
              {
                "title": "Step-Back Prompting",
                "content": "Ask a more general question first, then use that context.",
                "code": "from langchain.prompts import ChatPromptTemplate\n\n# Step-back: Get broader context first\nstep_back_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Generate a more general question that would help answer: {question}\n    General question:\"\"\"\n)\n\nasync def step_back_rag(question):\n    # Generate step-back question\n    general_q = await (step_back_prompt | llm).ainvoke({\"question\": question})\n    \n    # Retrieve for both questions\n    specific_docs = await retriever.aget_relevant_documents(question)\n    general_docs = await retriever.aget_relevant_documents(general_q)\n    \n    # Combine context\n    all_docs = specific_docs + general_docs\n    \n    # Generate answer with broader context\n    return await rag_chain.ainvoke({\"context\": all_docs, \"question\": question})"
              }
            ],
            "keyTakeaways": [
              "Query transformation improves recall",
              "Multi-query captures different perspectives",
              "HyDE works well for complex queries",
              "Step-back provides broader context"
            ]
          }
        }
      ]
    },
    {
      "title": "Production RAG",
      "description": "Deploy and optimize RAG systems for production",
      "lessons": [
        {
          "id": "rag-evaluation",
          "title": "Evaluating RAG Systems",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "Systematic evaluation is critical for improving RAG quality. Learn metrics and frameworks for assessment.",
            "sections": [
              {
                "title": "RAG Evaluation Metrics",
                "content": "**Retrieval Metrics**\n- **Precision@k**: Relevant docs in top-k / k\n- **Recall@k**: Relevant docs in top-k / total relevant\n- **MRR**: Mean Reciprocal Rank of first relevant\n- **NDCG**: Normalized discounted cumulative gain\n\n**Generation Metrics**\n- **Faithfulness**: Is answer supported by context?\n- **Relevance**: Does answer address the question?\n- **Completeness**: Are all aspects covered?\n- **Coherence**: Is the answer well-structured?"
              },
              {
                "title": "RAGAS Framework",
                "code": "from ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall\n)\nfrom datasets import Dataset\n\n# Prepare evaluation data\neval_data = {\n    \"question\": questions,\n    \"answer\": generated_answers,\n    \"contexts\": [[ctx] for ctx in retrieved_contexts],\n    \"ground_truth\": ground_truth_answers\n}\n\ndataset = Dataset.from_dict(eval_data)\n\n# Evaluate\nresults = evaluate(\n    dataset,\n    metrics=[\n        faithfulness,      # Is answer grounded in context?\n        answer_relevancy,  # Is answer relevant to question?\n        context_precision, # Are retrieved docs relevant?\n        context_recall     # Did we retrieve needed info?\n    ]\n)\n\nprint(results)\n# {'faithfulness': 0.87, 'answer_relevancy': 0.92, ...}"
              },
              {
                "title": "LLM-as-Judge Evaluation",
                "code": "from langchain.evaluation import load_evaluator\n\n# Criteria-based evaluation\nevaluator = load_evaluator(\n    \"criteria\",\n    criteria=\"correctness\"\n)\n\nresult = evaluator.evaluate_strings(\n    prediction=\"The capital of France is Paris.\",\n    reference=\"Paris is the capital of France.\",\n    input=\"What is the capital of France?\"\n)\n\nprint(result[\"score\"])  # 1.0 = correct\n\n# Custom criteria\ncustom_evaluator = load_evaluator(\n    \"criteria\",\n    criteria={\n        \"factual\": \"Is the answer factually accurate based on the context?\",\n        \"complete\": \"Does the answer fully address the question?\"\n    }\n)"
              }
            ],
            "keyTakeaways": [
              "Evaluate retrieval and generation separately",
              "RAGAS provides comprehensive RAG evaluation",
              "LLM-as-judge scales evaluation effectively",
              "Create domain-specific evaluation datasets"
            ]
          }
        },
        {
          "id": "rag-optimization",
          "title": "RAG Performance Optimization",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Optimize RAG for speed, cost, and quality in production environments.",
            "sections": [
              {
                "title": "Latency Optimization",
                "content": "**Retrieval Speed**\n- Pre-compute and cache embeddings\n- Use approximate nearest neighbor (ANN)\n- Reduce embedding dimensions\n- Limit top-k to what's needed\n\n**Generation Speed**\n- Use faster models for simple queries\n- Stream responses\n- Cache common query results\n- Batch similar requests"
              },
              {
                "title": "Caching Strategies",
                "code": "import hashlib\nfrom functools import lru_cache\nimport redis\n\n# Redis cache for embeddings\nredis_client = redis.Redis(host='localhost', port=6379)\n\ndef get_embedding_cached(text: str):\n    cache_key = f\"emb:{hashlib.md5(text.encode()).hexdigest()}\"\n    \n    # Check cache\n    cached = redis_client.get(cache_key)\n    if cached:\n        return json.loads(cached)\n    \n    # Generate and cache\n    embedding = embeddings.embed_query(text)\n    redis_client.setex(\n        cache_key, \n        3600,  # 1 hour TTL\n        json.dumps(embedding)\n    )\n    return embedding\n\n# Semantic cache for similar queries\nclass SemanticCache:\n    def __init__(self, vectorstore, threshold=0.95):\n        self.cache_store = vectorstore\n        self.threshold = threshold\n    \n    def get_or_compute(self, query, compute_fn):\n        # Search for similar cached queries\n        results = self.cache_store.similarity_search_with_score(\n            query, k=1\n        )\n        \n        if results and results[0][1] > self.threshold:\n            return results[0][0].metadata['response']\n        \n        # Compute and cache\n        response = compute_fn(query)\n        self.cache_store.add_documents([Document(\n            page_content=query,\n            metadata={'response': response}\n        )])\n        return response"
              },
              {
                "title": "Cost Optimization",
                "code": "class CostOptimizedRAG:\n    def __init__(self):\n        self.fast_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n        self.quality_model = ChatOpenAI(model=\"gpt-4\")\n        self.classifier = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    \n    def query(self, question, context):\n        # Classify query complexity\n        complexity = self._classify_complexity(question)\n        \n        if complexity == \"simple\":\n            # Use faster, cheaper model\n            return self.fast_model.invoke(\n                self._build_prompt(question, context)\n            )\n        else:\n            # Use more capable model\n            return self.quality_model.invoke(\n                self._build_prompt(question, context)\n            )\n    \n    def _classify_complexity(self, question):\n        response = self.classifier.invoke(\n            f\"Classify this question as 'simple' or 'complex': {question}\"\n        )\n        return \"simple\" if \"simple\" in response.content.lower() else \"complex\""
              }
            ],
            "keyTakeaways": [
              "Cache embeddings and common queries",
              "Use model routing for cost efficiency",
              "Stream responses for perceived speed",
              "Monitor and optimize hot paths"
            ]
          }
        },
        {
          "id": "rag-production",
          "title": "Production Deployment",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "Deploy RAG applications with proper infrastructure, monitoring, and reliability patterns.",
            "sections": [
              {
                "title": "RAG API Architecture",
                "diagram": {
                  "title": "RAG API Architecture",
                  "code": "flowchart TB\n    Client[Client App] --> API[FastAPI Gateway]\n    \n    subgraph Service[\"RAG Service\"]\n        API --> Val[Validation]\n        Val --> Ret[Retriever]\n        Ret -->|Query| VecDB[(Vector DB)]\n        VecDB -->|Docs| Ret\n        Ret --> Gen[LLM Generator]\n        Gen -->|Response| API\n    end\n    \n    subgraph Ops[\"Observability\"]\n        Log[Logger]\n        Met[Prometheus]\n        Trace[LangSmith]\n    end\n    \n    API -.-> Log\n    API -.-> Met\n    Gen -.-> Trace\n    \n    style Client fill:#fff,stroke:#333\n    style Service fill:#dbeafe,stroke:#2563eb\n    style Ops fill:#fce7f3,stroke:#db2777"
                },
                "code": "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport logging\n\napp = FastAPI()\nlogger = logging.getLogger(__name__)\n\nclass Query(BaseModel):\n    question: str\n    filters: dict = None\n    top_k: int = 5\n\nclass RAGResponse(BaseModel):\n    answer: str\n    sources: list\n    confidence: float\n\n@app.post(\"/query\", response_model=RAGResponse)\nasync def query_rag(query: Query):\n    try:\n        # 1. Retrieve documents\n        docs = await retriever.aget_relevant_documents(\n            query.question,\n            filter=query.filters,\n            k=query.top_k\n        )\n        \n        # 2. Generate response\n        response = await rag_chain.ainvoke({\n            \"question\": query.question,\n            \"context\": docs\n        })\n        \n        # 3. Return with sources\n        return RAGResponse(\n            answer=response,\n            sources=[d.metadata.get('source') for d in docs],\n            confidence=calculate_confidence(docs)\n        )\n    except Exception as e:\n        logger.error(f\"RAG error: {e}\")\n        raise HTTPException(500, \"Failed to process query\")"
              },
              {
                "title": "Monitoring & Observability",
                "code": "from langsmith import Client\nfrom opentelemetry import trace\nimport prometheus_client as prom\n\n# Prometheus metrics\nquery_counter = prom.Counter('rag_queries_total', 'Total RAG queries')\nlatency_histogram = prom.Histogram(\n    'rag_latency_seconds', \n    'RAG query latency',\n    buckets=[0.1, 0.5, 1, 2, 5, 10]\n)\nretrieval_docs = prom.Histogram(\n    'rag_retrieved_docs',\n    'Number of docs retrieved'\n)\n\n# LangSmith for LLM tracing\nclient = Client()\n\n@trace_decorator\ndef monitored_rag_query(question: str):\n    query_counter.inc()\n    \n    with latency_histogram.time():\n        # Retrieval\n        docs = retriever.get_relevant_documents(question)\n        retrieval_docs.observe(len(docs))\n        \n        # Generation with tracing\n        with client.trace(name=\"rag_generation\") as t:\n            t.log_inputs({\"question\": question, \"num_docs\": len(docs)})\n            response = rag_chain.invoke({\"question\": question, \"context\": docs})\n            t.log_outputs({\"response\": response})\n        \n        return response"
              },
              {
                "title": "Error Handling & Fallbacks",
                "code": "class ResilientRAG:\n    def __init__(self, primary_retriever, fallback_retriever):\n        self.primary = primary_retriever\n        self.fallback = fallback_retriever\n        self.circuit_breaker = CircuitBreaker(failure_threshold=5)\n    \n    async def query(self, question: str) -> str:\n        try:\n            # Try primary with circuit breaker\n            if self.circuit_breaker.is_open:\n                return await self._use_fallback(question)\n            \n            docs = await asyncio.wait_for(\n                self.primary.aget_relevant_documents(question),\n                timeout=5.0\n            )\n            \n            if not docs:\n                return await self._use_fallback(question)\n            \n            return await self._generate(question, docs)\n            \n        except asyncio.TimeoutError:\n            self.circuit_breaker.record_failure()\n            return await self._use_fallback(question)\n        except Exception as e:\n            logger.error(f\"Primary failed: {e}\")\n            return await self._use_fallback(question)\n    \n    async def _use_fallback(self, question: str) -> str:\n        docs = await self.fallback.aget_relevant_documents(question)\n        if not docs:\n            return \"I couldn't find relevant information to answer your question.\"\n        return await self._generate(question, docs)"
              }
            ],
            "keyTakeaways": [
              "Build RAG as a proper API service",
              "Monitor latency, throughput, and quality",
              "Implement circuit breakers and fallbacks",
              "Use LangSmith/similar for LLM observability"
            ]
          }
        },
        {
          "id": "conversational-rag",
          "title": "Conversational RAG",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Build RAG systems that maintain conversation context and handle follow-up questions.",
            "sections": [
              {
                "title": "Chat History Integration",
                "code": "from langchain.memory import ConversationBufferWindowMemory\nfrom langchain.chains import ConversationalRetrievalChain\n\n# Memory to store conversation\nmemory = ConversationBufferWindowMemory(\n    k=5,  # Keep last 5 exchanges\n    memory_key=\"chat_history\",\n    return_messages=True,\n    output_key=\"answer\"\n)\n\n# Conversational RAG chain\nconvo_rag = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(model=\"gpt-4\"),\n    retriever=retriever,\n    memory=memory,\n    return_source_documents=True,\n    verbose=True\n)\n\n# Multi-turn conversation\nq1 = convo_rag({\"question\": \"What is the return policy?\"})\nprint(q1[\"answer\"])\n\nq2 = convo_rag({\"question\": \"What about electronics?\"})  # Uses context\nprint(q2[\"answer\"])\n\nq3 = convo_rag({\"question\": \"How long do I have?\"})  # Continues topic\nprint(q3[\"answer\"])"
              },
              {
                "title": "Query Contextualization",
                "code": "from langchain.prompts import ChatPromptTemplate\n\n# Reformulate question with chat history\ncontextualize_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"Given a chat history and the latest user question, \n    reformulate the question to be standalone (understandable without history).\n    Do NOT answer the question, just reformulate it if needed.\"\"\"),\n    (\"placeholder\", \"{chat_history}\"),\n    (\"human\", \"{question}\")\n])\n\ndef get_standalone_question(question, chat_history):\n    if not chat_history:\n        return question\n    \n    response = llm.invoke(\n        contextualize_prompt.format(\n            chat_history=chat_history,\n            question=question\n        )\n    )\n    return response.content\n\n# Example:\n# History: \"What is the return policy?\" -> \"You have 30 days...\"\n# Question: \"What about electronics?\"\n# Standalone: \"What is the return policy for electronics?\""
              }
            ],
            "keyTakeaways": [
              "Conversational RAG requires query contextualization",
              "Reformulate follow-up questions to be standalone",
              "Limit history window to control context size",
              "Track conversation state for coherent responses"
            ]
          }
        },
        {
          "id": "rag-project",
          "title": "Capstone: Production RAG System",
          "type": "project",
          "duration": "90 min",
          "content": {
            "overview": "Build a complete production-ready RAG system with advanced retrieval, evaluation, and monitoring.",
            "sections": [
              {
                "title": "Project: Document Q&A System",
                "content": "**Objective**: Build a production RAG system for technical documentation.\n\n**Requirements**:\n1. Ingest markdown/PDF documentation\n2. Implement hybrid search with reranking\n3. Support conversational queries\n4. Add source attribution\n5. Include evaluation pipeline\n6. Deploy as API with monitoring\n\n**Deliverables**:\n- Working API endpoint\n- Evaluation results on test set\n- Monitoring dashboard\n- Documentation"
              },
              {
                "title": "Implementation Guide",
                "code": "# Complete Production RAG System\n\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.retrievers.document_compressors import CohereRerank\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.chat_models import ChatOpenAI\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nclass ProductionRAG:\n    def __init__(self, docs_path: str):\n        # 1. Load and chunk documents\n        loader = DirectoryLoader(docs_path, glob=\"**/*.md\")\n        docs = loader.load()\n        \n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200\n        )\n        chunks = splitter.split_documents(docs)\n        \n        # 2. Create vector store\n        embeddings = OpenAIEmbeddings()\n        self.vectorstore = Chroma.from_documents(chunks, embeddings)\n        \n        # 3. Setup hybrid retriever\n        vector_retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 10})\n        bm25_retriever = BM25Retriever.from_documents(chunks, k=10)\n        \n        ensemble = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[0.6, 0.4]\n        )\n        \n        # 4. Add reranking\n        reranker = CohereRerank(top_n=5)\n        self.retriever = ContextualCompressionRetriever(\n            base_compressor=reranker,\n            base_retriever=ensemble\n        )\n        \n        # 5. Setup conversational chain\n        self.llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n        self.chain = ConversationalRetrievalChain.from_llm(\n            llm=self.llm,\n            retriever=self.retriever,\n            return_source_documents=True\n        )\n        \n        self.chat_history = []\n    \n    def query(self, question: str):\n        result = self.chain({\n            \"question\": question,\n            \"chat_history\": self.chat_history\n        })\n        \n        self.chat_history.append((question, result[\"answer\"]))\n        \n        return {\n            \"answer\": result[\"answer\"],\n            \"sources\": [doc.metadata for doc in result[\"source_documents\"]]\n        }"
              }
            ],
            "keyTakeaways": [
              "Production RAG requires multiple components",
              "Hybrid search + reranking improves quality",
              "Evaluation is essential for iteration",
              "Monitoring enables continuous improvement"
            ]
          }
        },
        {
          "id": "advanced-rag-paradigms",
          "title": "Advanced RAG Paradigms",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "Understand the evolution of RAG from naive implementations to advanced paradigms including Self-RAG, Corrective RAG, RAG Fusion, and GraphRAG.",
            "sections": [
              {
                "title": "RAG Evolution: Three Paradigms",
                "content": "RAG has evolved through three main paradigms:\n\n**1. Naive RAG** (Basic)\n- Simple retrieve â†’ read â†’ generate pipeline\n- Fixed retrieval, no quality checks\n- Works for simple Q&A but has limitations\n\n**2. Advanced RAG** (Enhanced)\n- Pre-retrieval optimization (query rewriting, HyDE)\n- Post-retrieval refinement (reranking, filtering)\n- Better chunking strategies\n\n**3. Modular RAG** (Flexible)\n- Interchangeable components\n- Adaptive retrieval decisions\n- Self-correction mechanisms\n- Can route to different pipelines based on query type"
              },
              {
                "title": "Common Naive RAG Limitations",
                "content": "**Problems with basic RAG:**\n\n1. **Question ambiguity** - Vague queries lead to irrelevant retrieval\n2. **Low retrieval accuracy** - Retrieved docs may not all be relevant\n3. **Limited knowledge** - Knowledge base may lack needed information\n4. **Context window limits** - Over-retrieving hits capacity limits\n5. **No self-correction** - System can't detect or fix its own errors\n\nAdvanced paradigms address these limitations through adaptive retrieval and self-reflection."
              },
              {
                "title": "Self-Reflective RAG (Self-RAG)",
                "content": "Self-RAG enables models to dynamically decide when to retrieve and critique their own outputs.\n\n**Key Mechanisms:**\n- **Reflection tokens** - Model decides if retrieval is needed\n- **Critique tokens** - Model assesses response quality\n- **Adaptive retrieval** - Only retrieves when it adds value\n\n**How it works:**\n1. Receive query\n2. Decide: Is retrieval needed? (reflection)\n3. If yes, retrieve and generate\n4. Critique: Is the response grounded and relevant?\n5. If quality is low, retry with different approach",
                "code": "# Simplified Self-RAG pattern\ndef self_reflective_rag(query: str, retriever, llm):\n    # Step 1: Decide if retrieval is needed\n    needs_retrieval = llm.invoke(\n        f\"Does this query need external knowledge? Query: {query}\\n\"\n        \"Answer YES or NO only.\"\n    ).content.strip().upper() == \"YES\"\n    \n    if needs_retrieval:\n        docs = retriever.get_relevant_documents(query)\n        context = \"\\n\".join([d.page_content for d in docs])\n    else:\n        context = \"Use your general knowledge.\"\n    \n    # Step 2: Generate response\n    response = llm.invoke(\n        f\"Context: {context}\\nQuery: {query}\\nAnswer:\"\n    ).content\n    \n    # Step 3: Self-critique\n    critique = llm.invoke(\n        f\"Rate this response for the query '{query}':\\n\"\n        f\"Response: {response}\\n\"\n        \"Score 0-1 for: relevance, groundedness, completeness. \"\n        \"Return JSON: {{\\\"relevance\\\": X, \\\"grounded\\\": X, \\\"complete\\\": X}}\"\n    )\n    \n    scores = json.loads(critique.content)\n    avg_score = sum(scores.values()) / len(scores)\n    \n    # Step 4: Retry if quality is low\n    if avg_score < 0.6:\n        return self_reflective_rag_with_more_context(query, retriever, llm)\n    \n    return response"
              },
              {
                "title": "Corrective RAG (CRAG)",
                "content": "CRAG improves accuracy by evaluating retrieved documents and taking corrective actions.\n\n**Key Innovation:**\n- Uses an evaluator to assess document quality\n- Triggers one of three actions based on confidence:\n  - **Correct** - Retrieved docs are relevant, proceed\n  - **Incorrect** - Docs irrelevant, fallback to web search\n  - **Ambiguous** - Partially relevant, combine with web search\n\n**Benefits:**\n- Handles knowledge base gaps gracefully\n- Extends beyond static databases to web\n- Reduces impact of poor retrieval",
                "code": "# Corrective RAG pattern\nclass CorrectiveRAG:\n    def __init__(self, retriever, llm, web_search):\n        self.retriever = retriever\n        self.llm = llm\n        self.web_search = web_search  # SerpAPI, Tavily, etc.\n    \n    def query(self, question: str) -> str:\n        # Step 1: Retrieve from knowledge base\n        docs = self.retriever.get_relevant_documents(question)\n        \n        # Step 2: Evaluate retrieval quality\n        evaluation = self._evaluate_docs(question, docs)\n        \n        # Step 3: Take corrective action\n        if evaluation == \"CORRECT\":\n            context = self._format_docs(docs)\n        elif evaluation == \"INCORRECT\":\n            # Fallback to web search\n            web_results = self.web_search.run(question)\n            context = self._format_web_results(web_results)\n        else:  # AMBIGUOUS\n            # Combine both sources\n            web_results = self.web_search.run(question)\n            context = self._format_docs(docs) + \"\\n\" + self._format_web_results(web_results)\n        \n        # Step 4: Generate with corrected context\n        return self.llm.invoke(\n            f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n        ).content\n    \n    def _evaluate_docs(self, question: str, docs: list) -> str:\n        doc_texts = \"\\n\".join([d.page_content[:200] for d in docs])\n        result = self.llm.invoke(\n            f\"Evaluate if these documents answer the question.\\n\"\n            f\"Question: {question}\\nDocuments: {doc_texts}\\n\"\n            \"Reply: CORRECT (clearly relevant), INCORRECT (not relevant), \"\n            \"or AMBIGUOUS (partially relevant)\"\n        )\n        return result.content.strip().upper()"
              },
              {
                "title": "RAG Fusion",
                "content": "RAG Fusion improves recall by generating multiple query perspectives and combining results.\n\n**Process:**\n1. Generate multiple derivative queries from user input\n2. Retrieve documents for each query variant\n3. Combine and rerank using Reciprocal Rank Fusion (RRF)\n4. Generate response from fused context\n\n**Why it works:**\n- Different phrasings match different documents\n- Reduces risk of missing relevant content\n- RRF prioritizes docs that appear across multiple queries",
                "code": "from langchain.retrievers import MultiQueryRetriever\n\n# RAG Fusion with multi-query + RRF\ndef rag_fusion(query: str, retriever, llm, num_variants: int = 3):\n    # Step 1: Generate query variants\n    variants_prompt = f\"\"\"\n    Generate {num_variants} different phrasings of this question.\n    Original: {query}\n    Return as a list: [\"variant1\", \"variant2\", \"variant3\"]\n    \"\"\"\n    variants = json.loads(llm.invoke(variants_prompt).content)\n    all_queries = [query] + variants\n    \n    # Step 2: Retrieve for each query\n    all_docs = {}\n    for q in all_queries:\n        docs = retriever.get_relevant_documents(q)\n        for rank, doc in enumerate(docs):\n            doc_id = hash(doc.page_content)\n            if doc_id not in all_docs:\n                all_docs[doc_id] = {\"doc\": doc, \"ranks\": []}\n            all_docs[doc_id][\"ranks\"].append(rank + 1)\n    \n    # Step 3: Reciprocal Rank Fusion scoring\n    k = 60  # RRF constant\n    for doc_id, data in all_docs.items():\n        data[\"rrf_score\"] = sum(1 / (k + r) for r in data[\"ranks\"])\n    \n    # Step 4: Sort by RRF score and take top results\n    sorted_docs = sorted(\n        all_docs.values(), \n        key=lambda x: x[\"rrf_score\"], \n        reverse=True\n    )[:5]\n    \n    context = \"\\n\".join([d[\"doc\"].page_content for d in sorted_docs])\n    \n    return llm.invoke(f\"Context: {context}\\nQuestion: {query}\\nAnswer:\").content"
              },
              {
                "title": "GraphRAG: Knowledge Graphs for RAG",
                "content": "GraphRAG enhances retrieval by structuring knowledge as a graph rather than flat chunks.\n\n**Key Concepts:**\n- **Entities** - People, places, concepts extracted from text\n- **Relationships** - Connections between entities\n- **Graph traversal** - Follow relationships to find related context\n\n**Advantages:**\n- Captures relationships between concepts\n- Enables multi-hop reasoning\n- Better for complex queries requiring connected information\n- Reduces redundancy in retrieval",
                "code": "# GraphRAG concept using Neo4j\nfrom langchain_community.graphs import Neo4jGraph\nfrom langchain.chains import GraphCypherQAChain\n\n# Connect to knowledge graph\ngraph = Neo4jGraph(\n    url=\"bolt://localhost:7687\",\n    username=\"neo4j\",\n    password=\"password\"\n)\n\n# Natural language to graph query\ngraph_qa = GraphCypherQAChain.from_llm(\n    llm=ChatOpenAI(model=\"gpt-4\"),\n    graph=graph,\n    verbose=True\n)\n\n# Query traverses relationships\nresult = graph_qa.invoke(\n    \"Which products were purchased by customers who also bought ProductX?\"\n)\n# LLM generates Cypher: MATCH (c:Customer)-[:BOUGHT]->(p1:Product {name: 'ProductX'})\n#                       MATCH (c)-[:BOUGHT]->(p2:Product) WHERE p2 <> p1\n#                       RETURN DISTINCT p2.name"
              },
              {
                "title": "RAG vs Fine-tuning: When to Use Each",
                "content": "Understanding when to use RAG, fine-tuning, or both:\n\n**Use RAG when:**\n- Knowledge needs frequent updates\n- You need source citations\n- Data is private/organizational\n- Avoiding retraining costs is important\n- Real-time data access is needed\n\n**Use Fine-tuning when:**\n- Teaching specific output format/style\n- Domain-specific terminology is critical\n- Consistent behavior for a narrow task\n- Performance optimization is key\n\n**Use Both (RAG + Fine-tuning) when:**\n- Fine-tune for domain familiarity and output format\n- RAG for current, factual information\n- Best of both: domain expertise + accurate facts",
                "diagram": {
                  "title": "RAG vs Fine-tuning Decision",
                  "code": "flowchart TD\n    Q[\"Need domain-specific knowledge?\"] -->|Yes| A{\"Frequent updates?\"}\n    A -->|Yes| RAG[\"Use RAG\"]\n    A -->|No| B{\"Need specific output style?\"}\n    B -->|Yes| BOTH[\"RAG + Fine-tuning\"]\n    B -->|No| FT[\"Fine-tuning may suffice\"]\n    Q -->|No| C{\"Need citations?\"}\n    C -->|Yes| RAG\n    C -->|No| BASE[\"Base model may work\"]"
                }
              }
            ],
            "keyTakeaways": [
              "RAG has evolved from Naive â†’ Advanced â†’ Modular paradigms",
              "Self-RAG adds reflection and critique for adaptive retrieval",
              "Corrective RAG evaluates and falls back to web search when needed",
              "RAG Fusion generates query variants and uses RRF for better recall",
              "GraphRAG captures entity relationships for complex reasoning",
              "RAG and fine-tuning serve different purposes and can be combined"
            ]
          }
        },
        {
          "id": "rag-quiz",
          "title": "RAG Applications Quiz",
          "type": "quiz",
          "duration": "15 min",
          "questions": [
            {
              "question": "What is the main purpose of text chunking in RAG?",
              "options": [
                "Make files smaller",
                "Split documents into retrievable units that can be embedded and searched",
                "Compress data for storage",
                "Encrypt content for security"
              ],
              "correct": 1,
              "explanation": "Chunking splits documents into smaller pieces that can be individually embedded and retrieved based on relevance to a query."
            },
            {
              "question": "Why is chunk size considered a key hyperparameter in RAG systems?",
              "options": [
                "It only affects storage costs",
                "Larger chunks are always better for accuracy",
                "Too large loses specificity, too small loses semantic coherence",
                "It has no impact on retrieval quality"
              ],
              "correct": 2,
              "explanation": "Chunk size balances two concerns: chunks too large become too general and fail to match specific queries, while chunks too small lose semantic coherence and context."
            },
            {
              "question": "What are the four main components of a RAG system?",
              "options": [
                "Encoder, decoder, attention, output",
                "Knowledge base, retriever, integration layer, generator",
                "Database, API, cache, frontend",
                "Tokenizer, embedder, classifier, ranker"
              ],
              "correct": 1,
              "explanation": "A RAG system consists of: knowledge base (external data), retriever (searches for relevant data), integration layer (coordinates and augments prompts), and generator (LLM that creates outputs)."
            },
            {
              "question": "How does RAG differ from fine-tuning?",
              "options": [
                "RAG is faster, fine-tuning is more accurate",
                "RAG queries external data at runtime, fine-tuning trains on domain-specific data",
                "Fine-tuning is free, RAG is expensive",
                "They are the same technique with different names"
              ],
              "correct": 1,
              "explanation": "RAG connects a model to external knowledge bases at query time, while fine-tuning trains the model on domain-specific data. RAG is more cost-efficient for knowledge updates; both can be used together."
            },
            {
              "question": "What problem does Self-Reflective RAG (Self-RAG) solve?",
              "options": [
                "Faster retrieval speed",
                "Lower token costs",
                "Adaptive retrieval decisions and self-critique of response quality",
                "Better document parsing"
              ],
              "correct": 2,
              "explanation": "Self-RAG uses reflection tokens to decide when retrieval is needed and critique tokens to assess response quality, enabling the model to correct itself when answers are poor."
            },
            {
              "question": "What action does Corrective RAG (CRAG) take when retrieved documents are irrelevant?",
              "options": [
                "Returns an error message",
                "Falls back to web search for additional information",
                "Asks the user to rephrase the question",
                "Uses random documents instead"
              ],
              "correct": 1,
              "explanation": "CRAG evaluates retrieved documents and when they're judged irrelevant, it falls back to web search to find better information, extending beyond the static knowledge base."
            },
            {
              "question": "How does RAG Fusion improve retrieval recall?",
              "options": [
                "Uses larger embeddings",
                "Generates multiple query variants and combines results with Reciprocal Rank Fusion",
                "Increases the number of retrieved documents",
                "Uses multiple LLMs in parallel"
              ],
              "correct": 1,
              "explanation": "RAG Fusion generates multiple phrasings of the query, retrieves for each, then uses RRF to combine and rank results, reducing the chance of missing relevant content."
            },
            {
              "question": "What advantage does GraphRAG provide over traditional vector-based RAG?",
              "options": [
                "Faster similarity search",
                "Lower storage costs",
                "Captures entity relationships for multi-hop reasoning",
                "Works without an LLM"
              ],
              "correct": 2,
              "explanation": "GraphRAG structures knowledge as entities and relationships, enabling traversal of connections between concepts for complex queries requiring related information."
            },
            {
              "question": "Why can RAG models include citations in their responses?",
              "options": [
                "All LLMs can cite sources by default",
                "The retrieved chunks come from identifiable sources that can be referenced",
                "Citations are generated randomly",
                "Only fine-tuned models can cite"
              ],
              "correct": 1,
              "explanation": "Since RAG retrieves specific chunks from the knowledge base, the system knows exactly which sources were used and can include them as citations, increasing user trust and verifiability."
            },
            {
              "question": "What is 'faithfulness' in RAG evaluation?",
              "options": [
                "Speed of retrieval",
                "Whether the generated answer is supported by the retrieved context",
                "Number of sources used",
                "Model temperature setting"
              ],
              "correct": 1,
              "explanation": "Faithfulness measures whether the generated answer is actually supported by and grounded in the retrieved context, avoiding hallucinations."
            },
            {
              "question": "Which RAG paradigm represents the most flexible, component-based approach?",
              "options": [
                "Naive RAG",
                "Advanced RAG",
                "Modular RAG",
                "Basic RAG"
              ],
              "correct": 2,
              "explanation": "Modular RAG uses interchangeable components, adaptive retrieval decisions, and self-correction mechanisms, allowing routing to different pipelines based on query type."
            },
            {
              "question": "What is the primary benefit of hybrid search (semantic + keyword) in RAG?",
              "options": [
                "It's faster than either alone",
                "Combines conceptual matching with exact term matching for diverse queries",
                "Uses less memory",
                "Requires no embeddings"
              ],
              "correct": 1,
              "explanation": "Hybrid search combines semantic (embedding) search that finds conceptually similar content with keyword (BM25) search that finds exact term matches, handling diverse query types."
            }
          ],
          "references": {
            "lessonRefs": [
              "embeddings",
              "vector-databases",
              "chunking-strategies"
            ],
            "externalRefs": [
              {
                "title": "LlamaIndex Docs",
                "url": "https://docs.llamaindex.ai/"
              },
              {
                "title": "Pinecone Learning Center",
                "url": "https://www.pinecone.io/learn/"
              }
            ]
          }
        }
      ]
    }
  ]
}