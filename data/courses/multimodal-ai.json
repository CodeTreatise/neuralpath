{
  "id": "multimodal-ai",
  "title": "Multimodal AI Systems",
  "description": "Build AI applications that understand and generate across text, images, audio, and video. Multimodal AI enables systems to process and reason across multiple types of data simultaneously, creating more natural and capable AI experiences that mirror human perception.",
  "icon": "ðŸŽ¨",
  "level": "intermediate",
  "duration": "4 weeks",
  "validationSources": [
    "https://www.ibm.com/topics/multimodal-ai",
    "https://www.ibm.com/think/topics/multimodal-ai",
    "https://www.ibm.com/topics/computer-vision"
  ],
  "prerequisites": ["Used ChatGPT or Claude for text tasks", "Basic Python (can read and run scripts)", "Comfort with APIs"],
  "prerequisitesClarification": "If you've used AI for text tasks and want to extend to images, audio, and video, you're ready. No computer vision or audio processing background needed.",
  "whatYouNeed": [
    "OpenAI API key (~$15-30 for all examples)",
    "Python 3.8+ installed",
    "Sample images and audio files (we provide some)",
    "No GPU required - all models run via API"
  ],
  "learningOutcomes": [
    "Understand what multimodal AI is and why it matters",
    "Build vision applications with GPT-4V and Claude",
    "Use CLIP for image search and classification",
    "Create audio applications with Whisper and text-to-speech",
    "Combine modalities for rich AI experiences"
  ],
  "modules": [
    {
      "id": "multimodal-intro",
      "title": "Introduction to Multimodal AI",
      "lessons": [
        {
          "id": "what-is-multimodal",
          "title": "What is Multimodal AI?",
          "duration": "20 min",
          "content": {
            "overview": "Most AI you've used only handles text. Multimodal AI can see, hear, and create across different types of data. Let's understand what this means.",
            "sections": [
              {
                "title": "Beyond Text: The Multimodal Revolution",
                "content": "**What does \"multimodal\" mean?**\n\n\"Modal\" refers to a type of data:\n- **Text** = one modality\n- **Images** = another modality\n- **Audio** = another modality\n- **Video** = another modality\n\n**Multimodal AI** can work with multiple types at once.\n\n**Examples you may have used:**\n- Asking ChatGPT to analyze a photo\n- Voice assistants (Siri, Alexa) that hear and speak\n- Google Lens identifying objects in photos\n- YouTube auto-generating captions\n\n**Why does this matter?**\nHumans communicate with more than text. Multimodal AI can:\n- Read documents AND look at diagrams\n- Listen to meetings AND transcribe them\n- Watch videos AND summarize them",
                "diagram": {
                  "title": "Modalities in AI",
                  "code": "flowchart LR\n    subgraph Input[\"What AI Can Understand\"]\n        T[\"ðŸ“ Text\"]\n        I[\"ðŸ–¼ï¸ Images\"]\n        A[\"ðŸ”Š Audio\"]\n        V[\"ðŸŽ¬ Video\"]\n    end\n    \n    subgraph AI[\"Multimodal AI\"]\n        M[\"ðŸ§  Unified\\nUnderstanding\"]\n    end\n    \n    subgraph Output[\"What AI Can Create\"]\n        TO[\"ðŸ“ Text\"]\n        IO[\"ðŸ–¼ï¸ Images\"]\n        AO[\"ðŸ”Š Audio\"]\n    end\n    \n    T --> M\n    I --> M\n    A --> M\n    V --> M\n    M --> TO\n    M --> IO\n    M --> AO"
                }
              },
              {
                "title": "Key Multimodal Models",
                "content": "**Vision + Language (see and talk):**\n- **GPT-4V / GPT-4o**: OpenAI's models that can see images\n- **Claude 3**: Anthropic's vision-capable AI\n- **Gemini**: Google's natively multimodal model\n- **LLaVA**: Open-source alternative\n\n**Audio (hear and speak):**\n- **Whisper**: OpenAI's speech-to-text (transcription)\n- **TTS APIs**: Text-to-speech generation\n- **Eleven Labs**: High-quality voice synthesis\n\n**Images (create pictures):**\n- **DALL-E 3**: OpenAI's image generator\n- **Midjourney**: Popular image generation\n- **Stable Diffusion**: Open-source image generation\n\n**Special purpose:**\n- **CLIP**: Connects images and text (search, classify)\n- **Whisper**: Transcribes audio to text\n\n| Task | Model | What it does |\n|------|-------|-------------|\n| Describe an image | GPT-4V, Claude | \"What's in this photo?\" |\n| Transcribe speech | Whisper | Audio â†’ Text |\n| Generate speech | TTS, Eleven Labs | Text â†’ Audio |\n| Search images by text | CLIP | \"Find sunset photos\" |\n| Generate images | DALL-E, Midjourney | Text â†’ Image |"
              },
              {
                "title": "What You'll Build",
                "content": "**In this course, you'll learn to:**\n\n1. **Analyze images** - Extract text, describe photos, read charts\n2. **Search images with text** - \"Find all screenshots with error messages\"\n3. **Transcribe audio** - Convert podcasts/meetings to text\n4. **Generate speech** - Make your app talk\n5. **Combine modalities** - Build multimodal RAG (search documents + images)\n\n**No special equipment needed!** All models run via API. You just need Python and API keys.",
                "code": "# Quick preview of what you'll build:\n\nfrom openai import OpenAI\nclient = OpenAI()\n\n# 1. Describe an image\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/photo.jpg\"}}\n        ]\n    }]\n)\nprint(response.choices[0].message.content)\n\n# 2. Transcribe audio\nwith open(\"meeting.mp3\", \"rb\") as audio_file:\n    transcript = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\nprint(transcript.text)\n\n# 3. Generate speech\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello! Welcome to multimodal AI.\"\n)\nresponse.stream_to_file(\"output.mp3\")"
              }
            ],
            "keyTakeaways": [
              "Multimodal = AI that works with multiple types of data (text, images, audio)",
              "GPT-4V and Claude can see images, Whisper transcribes audio",
              "CLIP connects images and text for search/classification",
              "Everything runs via API - no special hardware needed"
            ],
            "beginnerQuestions": [
              {"q": "Do I need a powerful computer for this?", "a": "No! All models run on cloud APIs. You just send requests and get results."},
              {"q": "Is multimodal AI more expensive?", "a": "Generally yes - images and audio cost more than text. We'll cover pricing."},
              {"q": "Can I run these models locally?", "a": "Some (like Whisper, LLaVA), but we focus on APIs for simplicity. See the Small Language Models course for local options."}
            ],
            "references": [
              {"title": "OpenAI Images & Vision Guide", "url": "https://platform.openai.com/docs/guides/images-vision"},
              {"title": "OpenAI Speech-to-Text (Whisper)", "url": "https://platform.openai.com/docs/guides/speech-to-text"},
              {"title": "Google Gemini Video Understanding", "url": "https://ai.google.dev/gemini-api/docs/video-understanding"},
              {"title": "OpenAI Audio Guide", "url": "https://platform.openai.com/docs/guides/audio"}
            ]
          }
        }
      ]
    },
    {
      "id": "vision-language-models",
      "title": "Vision-Language Models",
      "lessons": [
        {
          "id": "understanding-vlms",
          "title": "Working with Vision-Language Models",
          "duration": "35 min",
          "content": {
            "overview": "Vision-Language Models (VLMs) can see and reason about images. They power image captioning, visual Q&A, document understanding, and more.",
            "sections": [
              {
                "title": "How VLMs Work (Simplified)",
                "content": "**You don't need to understand the architecture to use VLMs**, but here's the basic idea:\n\n1. **Image Encoder**: Converts image pixels to numbers the AI understands\n2. **Text Encoder**: Converts your question to numbers\n3. **Language Model**: Combines both and generates a text response\n\n**Think of it like:**\nThe AI has \"eyes\" (image encoder) connected to its \"brain\" (language model). It can look at the image and talk about what it sees.\n\n**Key Models:**\n- **GPT-4V/GPT-4o**: OpenAI - best overall quality\n- **Claude 3**: Anthropic - great for documents\n- **Gemini**: Google - fast and capable\n- **LLaVA**: Open-source option",
                "diagram": {
                  "title": "Vision-Language Model Architecture",
                  "code": "flowchart LR\n    I[ðŸ–¼ï¸ Image] --> VE[ðŸ‘ï¸ Vision Encoder]\n    VE --> IT[\"Image as 'tokens'\"]\n    T[ðŸ’¬ Your Question] --> TT[Text tokens]\n    IT --> LLM[ðŸ§  Language Model]\n    TT --> LLM\n    LLM --> O[ðŸ“ Answer]\n    \n    style VE fill:#10b981\n    style LLM fill:#3b82f6"
                }
              },
              {
                "title": "GPT-4V and GPT-4o",
                "content": "OpenAI's vision models are accessible via API:",
                "code": "from openai import OpenAI\nimport base64\nfrom pathlib import Path\n\nclient = OpenAI()\n\ndef encode_image(image_path):\n    \"\"\"Encode image to base64 for API\"\"\"\n    with open(image_path, \"rb\") as f:\n        return base64.b64encode(f.read()).decode(\"utf-8\")\n\ndef analyze_image(image_path, prompt=\"Describe this image in detail\"):\n    \"\"\"Analyze an image with GPT-4o\"\"\"\n    base64_image = encode_image(image_path)\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                            \"detail\": \"high\"  # high, low, or auto\n                        }\n                    }\n                ]\n            }\n        ],\n        max_tokens=1000\n    )\n    \n    return response.choices[0].message.content\n\n# Basic usage\ndescription = analyze_image(\"photo.jpg\")\nprint(description)\n\n# Specific analysis\nanalysis = analyze_image(\n    \"chart.png\",\n    \"Extract all data points from this chart and present them as a table\"\n)\nprint(analysis)\n\n# Multiple images\ndef compare_images(image_paths, prompt=\"Compare these images\"):\n    content = [{\"type\": \"text\", \"text\": prompt}]\n    \n    for path in image_paths:\n        content.append({\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{encode_image(path)}\"\n            }\n        })\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": content}],\n        max_tokens=1000\n    )\n    \n    return response.choices[0].message.content\n\ncomparison = compare_images([\"before.jpg\", \"after.jpg\"])"
              },
              {
                "title": "Claude Vision",
                "content": "Anthropic's Claude also has excellent vision capabilities:",
                "code": "from anthropic import Anthropic\nimport base64\n\nclient = Anthropic()\n\ndef analyze_with_claude(image_path, prompt):\n    \"\"\"Analyze image with Claude\"\"\"\n    with open(image_path, \"rb\") as f:\n        image_data = base64.standard_b64encode(f.read()).decode(\"utf-8\")\n    \n    # Determine media type\n    suffix = image_path.lower().split(\".\")[-1]\n    media_types = {\"jpg\": \"image/jpeg\", \"jpeg\": \"image/jpeg\", \"png\": \"image/png\", \"gif\": \"image/gif\", \"webp\": \"image/webp\"}\n    media_type = media_types.get(suffix, \"image/jpeg\")\n    \n    response = client.messages.create(\n        model=\"claude-sonnet-4-20250514\",\n        max_tokens=1024,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image\",\n                        \"source\": {\n                            \"type\": \"base64\",\n                            \"media_type\": media_type,\n                            \"data\": image_data\n                        }\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": prompt\n                    }\n                ]\n            }\n        ]\n    )\n    \n    return response.content[0].text\n\n# Document analysis\nresult = analyze_with_claude(\n    \"receipt.jpg\",\n    \"Extract all items and prices from this receipt. Format as JSON.\"\n)\nprint(result)"
              },
              {
                "title": "Understanding the Detail Parameter (OpenAI)",
                "content": "OpenAI's vision models have a `detail` parameter that controls how the image is processed - think of it like choosing between 'quick glance' vs 'careful examination' (Source: OpenAI Images & Vision Guide):\n\n**Three Modes:**\n- **`low`**: Quick mode - image is resized to 512x512. Uses only 85 tokens regardless of size. Best for: thumbnails, simple images, when you need speed\n- **`high`**: Detailed mode - image is analyzed at full resolution using multiple 512x512 tiles. Token cost varies by image size. Best for: documents, diagrams, text extraction\n- **`auto`**: Default - model decides based on image size\n\n**Token Cost Examples (High Detail):**\n- 1024x1024 image: ~765 tokens (4 tiles + base)\n- 2048x2048 image: ~2,380 tokens (16 tiles + base)\n- Formula: 170 Ã— tiles + 85 base tokens\n\n**Supported Formats:** PNG, JPEG, WEBP, non-animated GIF (up to 50MB per image)",
                "code": "# Low detail - fast and cheap (85 tokens)\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                    \"detail\": \"low\"  # Only 85 tokens!\n                }\n            }\n        ]\n    }]\n)\n\n# High detail - for documents and complex images\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Extract all text from this document.\"},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                    \"detail\": \"high\"  # Full resolution analysis\n                }\n            }\n        ]\n    }]\n)\n\n# When processing many images, use low detail to save costs\ndef batch_classify_images(image_paths, categories):\n    \"\"\"Classify many images cheaply using low detail mode\"\"\"\n    results = []\n    for path in image_paths:\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": f\"Classify this image into one of: {categories}\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{encode_image(path)}\",\n                            \"detail\": \"low\"  # Save tokens for bulk processing\n                        }\n                    }\n                ]\n            }]\n        )\n        results.append(response.choices[0].message.content)\n    return results"
              },
              {
                "title": "Practical Applications",
                "content": "Common VLM use cases:",
                "code": "class VisionApplications:\n    def __init__(self, client):\n        self.client = client\n    \n    def extract_text_from_image(self, image_path):\n        \"\"\"OCR: Extract text from images\"\"\"\n        return self._query(\n            image_path,\n            \"Extract ALL text from this image. Preserve formatting where possible.\"\n        )\n    \n    def analyze_chart(self, image_path):\n        \"\"\"Extract data from charts and graphs\"\"\"\n        return self._query(\n            image_path,\n            \"\"\"Analyze this chart/graph:\n1. What type of chart is this?\n2. What is being measured?\n3. Extract the key data points\n4. What are the main insights?\n\nProvide data in structured format.\"\"\"\n        )\n    \n    def describe_for_accessibility(self, image_path):\n        \"\"\"Generate alt-text for accessibility\"\"\"\n        return self._query(\n            image_path,\n            \"\"\"Generate a concise but comprehensive alt-text description \nfor this image that would help a visually impaired person \nunderstand what's shown. Focus on key visual elements and context.\"\"\"\n        )\n    \n    def analyze_ui(self, image_path):\n        \"\"\"Analyze UI screenshots\"\"\"\n        return self._query(\n            image_path,\n            \"\"\"Analyze this UI screenshot:\n1. What application/website is this?\n2. List all visible UI elements (buttons, forms, menus)\n3. Describe the layout and navigation\n4. Identify any usability issues\"\"\"\n        )\n    \n    def moderate_image(self, image_path):\n        \"\"\"Content moderation for images\"\"\"\n        response = self._query(\n            image_path,\n            \"\"\"Analyze this image for content moderation.\nCheck for: violence, nudity, hate symbols, illegal content.\n\nRespond with JSON:\n{\"safe\": true/false, \"categories\": [...], \"confidence\": 0-1, \"reason\": \"...\"}\"\"\"\n        )\n        return response\n    \n    def _query(self, image_path, prompt):\n        base64_img = encode_image(image_path)\n        response = self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"}}\n                ]\n            }],\n            max_tokens=1000\n        )\n        return response.choices[0].message.content\n\n# Usage\napps = VisionApplications(OpenAI())\ntext = apps.extract_text_from_image(\"document.png\")\nalt_text = apps.describe_for_accessibility(\"hero-image.jpg\")"
              }
            ],
            "keyTakeaways": [
              "VLMs combine vision encoders with language models",
              "GPT-4o, Claude, and Gemini all support vision",
              "Use detail=low (85 tokens) for quick classification, detail=high for text/documents",
              "Common uses: OCR, chart analysis, accessibility, moderation"
            ]
          }
        }
      ]
    },
    {
      "id": "clip-embeddings",
      "title": "CLIP and Image-Text Embeddings",
      "lessons": [
        {
          "id": "understanding-clip",
          "title": "Understanding CLIP",
          "duration": "40 min",
          "content": {
            "overview": "CLIP (Contrastive Language-Image Pre-training) learns to connect images and text in a shared embedding space, enabling powerful zero-shot image classification and similarity search.",
            "sections": [
              {
                "title": "How CLIP Works",
                "content": "CLIP was trained on 400M image-text pairs from the internet:\n\n**Training Process:**\n1. Encode images with Vision Transformer\n2. Encode text with Transformer\n3. Train to maximize similarity between matching pairs\n4. Minimize similarity between non-matching pairs\n\n**Capabilities:**\n- Zero-shot image classification\n- Image-text similarity scoring\n- Image search by text query\n- Text search by image query",
                "diagram": {
                  "title": "CLIP Architecture",
                  "code": "flowchart TB\n    subgraph Training\n        I[Images] --> VE[Vision Encoder]\n        T[Text Descriptions] --> TE[Text Encoder]\n        VE --> IE[Image Embeddings]\n        TE --> TE2[Text Embeddings]\n        IE --> C[Contrastive Loss]\n        TE2 --> C\n    end\n    \n    subgraph Inference\n        I2[Query Image] --> VE2[Vision Encoder]\n        T2[\"Labels / Query Text\"] --> TE3[Text Encoder]\n        VE2 --> S[Similarity Score]\n        TE3 --> S\n        S --> M[Match / Rank]\n    end"
                }
              },
              {
                "title": "Using CLIP with OpenAI",
                "content": "OpenAI provides CLIP-based embeddings:",
                "code": "from openai import OpenAI\nimport numpy as np\n\nclient = OpenAI()\n\ndef get_image_embedding(image_path):\n    \"\"\"Get CLIP embedding for an image\"\"\"\n    # Note: As of 2024, use vision models for embeddings\n    # Direct CLIP API: this is conceptual\n    with open(image_path, \"rb\") as f:\n        image_data = f.read()\n    \n    # For now, use text description as proxy\n    # or use local CLIP model\n    pass\n\ndef get_text_embedding(text):\n    \"\"\"Get text embedding (compatible with image embeddings)\"\"\"\n    response = client.embeddings.create(\n        model=\"text-embedding-3-large\",\n        input=text\n    )\n    return response.data[0].embedding\n\n# Using Hugging Face CLIP directly\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport torch\n\nclass CLIPEmbedder:\n    def __init__(self, model_name=\"openai/clip-vit-large-patch14\"):\n        self.model = CLIPModel.from_pretrained(model_name)\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.model.eval()\n    \n    def embed_image(self, image_path):\n        \"\"\"Get embedding for an image\"\"\"\n        image = Image.open(image_path)\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n        \n        with torch.no_grad():\n            embedding = self.model.get_image_features(**inputs)\n        \n        # Normalize\n        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n        return embedding.squeeze().numpy()\n    \n    def embed_text(self, text):\n        \"\"\"Get embedding for text\"\"\"\n        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True)\n        \n        with torch.no_grad():\n            embedding = self.model.get_text_features(**inputs)\n        \n        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n        return embedding.squeeze().numpy()\n    \n    def similarity(self, image_path, text):\n        \"\"\"Compute image-text similarity\"\"\"\n        img_emb = self.embed_image(image_path)\n        txt_emb = self.embed_text(text)\n        return np.dot(img_emb, txt_emb)\n\n# Usage\nclip = CLIPEmbedder()\n\n# Zero-shot classification\nlabels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a bird\"]\nimage_path = \"pet.jpg\"\n\nscores = [clip.similarity(image_path, label) for label in labels]\npredicted = labels[np.argmax(scores)]\nprint(f\"Predicted: {predicted}\")"
              },
              {
                "title": "Building Image Search",
                "content": "Use CLIP for semantic image search:",
                "code": "import numpy as np\nfrom pathlib import Path\nimport json\n\nclass ImageSearchEngine:\n    def __init__(self, embedder):\n        self.embedder = embedder\n        self.index = []  # List of {path, embedding}\n    \n    def index_directory(self, directory):\n        \"\"\"Index all images in a directory\"\"\"\n        image_extensions = {\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\"}\n        \n        for path in Path(directory).rglob(\"*\"):\n            if path.suffix.lower() in image_extensions:\n                try:\n                    embedding = self.embedder.embed_image(str(path))\n                    self.index.append({\n                        \"path\": str(path),\n                        \"embedding\": embedding\n                    })\n                    print(f\"Indexed: {path}\")\n                except Exception as e:\n                    print(f\"Error indexing {path}: {e}\")\n        \n        print(f\"Indexed {len(self.index)} images\")\n    \n    def search_by_text(self, query, top_k=5):\n        \"\"\"Find images matching text query\"\"\"\n        query_embedding = self.embedder.embed_text(query)\n        \n        results = []\n        for item in self.index:\n            score = np.dot(query_embedding, item[\"embedding\"])\n            results.append({\"path\": item[\"path\"], \"score\": float(score)})\n        \n        results.sort(key=lambda x: x[\"score\"], reverse=True)\n        return results[:top_k]\n    \n    def search_by_image(self, image_path, top_k=5):\n        \"\"\"Find similar images\"\"\"\n        query_embedding = self.embedder.embed_image(image_path)\n        \n        results = []\n        for item in self.index:\n            if item[\"path\"] == image_path:\n                continue  # Skip self\n            score = np.dot(query_embedding, item[\"embedding\"])\n            results.append({\"path\": item[\"path\"], \"score\": float(score)})\n        \n        results.sort(key=lambda x: x[\"score\"], reverse=True)\n        return results[:top_k]\n    \n    def save_index(self, filepath):\n        \"\"\"Save index to file\"\"\"\n        data = [\n            {\"path\": item[\"path\"], \"embedding\": item[\"embedding\"].tolist()}\n            for item in self.index\n        ]\n        with open(filepath, \"w\") as f:\n            json.dump(data, f)\n    \n    def load_index(self, filepath):\n        \"\"\"Load index from file\"\"\"\n        with open(filepath, \"r\") as f:\n            data = json.load(f)\n        self.index = [\n            {\"path\": item[\"path\"], \"embedding\": np.array(item[\"embedding\"])}\n            for item in data\n        ]\n\n# Usage\nclip = CLIPEmbedder()\nsearch = ImageSearchEngine(clip)\n\n# Index photos\nsearch.index_directory(\"./photos\")\n\n# Search by text\nresults = search.search_by_text(\"sunset over mountains\")\nfor r in results:\n    print(f\"{r['path']}: {r['score']:.3f}\")\n\n# Find similar images\nsimilar = search.search_by_image(\"./photos/beach.jpg\")"
              },
              {
                "title": "CLIP for Content Filtering",
                "content": "Use CLIP for automated content categorization:",
                "code": "class ContentFilter:\n    def __init__(self, embedder):\n        self.embedder = embedder\n        \n        # Pre-compute category embeddings\n        self.categories = {\n            \"safe\": [\n                \"a photo of nature\", \"a photo of buildings\",\n                \"a photo of food\", \"a photo of people\",\n                \"a photo of animals\", \"a photo of art\"\n            ],\n            \"potentially_unsafe\": [\n                \"violent content\", \"explicit content\",\n                \"disturbing imagery\", \"graphic content\"\n            ]\n        }\n        \n        self.category_embeddings = {}\n        for category, descriptions in self.categories.items():\n            embeddings = [self.embedder.embed_text(d) for d in descriptions]\n            # Average the embeddings\n            self.category_embeddings[category] = np.mean(embeddings, axis=0)\n    \n    def classify(self, image_path):\n        \"\"\"Classify image as safe or potentially unsafe\"\"\"\n        img_emb = self.embedder.embed_image(image_path)\n        \n        scores = {}\n        for category, cat_emb in self.category_embeddings.items():\n            scores[category] = float(np.dot(img_emb, cat_emb))\n        \n        return {\n            \"safe\": scores[\"safe\"] > scores[\"potentially_unsafe\"],\n            \"scores\": scores,\n            \"confidence\": abs(scores[\"safe\"] - scores[\"potentially_unsafe\"])\n        }\n    \n    def filter_batch(self, image_paths, threshold=0.3):\n        \"\"\"Filter batch of images, return only safe ones\"\"\"\n        safe_images = []\n        \n        for path in image_paths:\n            result = self.classify(path)\n            if result[\"safe\"] and result[\"confidence\"] > threshold:\n                safe_images.append(path)\n        \n        return safe_images\n\n# Usage\nfilter = ContentFilter(CLIPEmbedder())\n\nresult = filter.classify(\"user_upload.jpg\")\nif result[\"safe\"]:\n    print(\"Image is safe to display\")\nelse:\n    print(\"Image flagged for review\")"
              }
            ],
            "keyTakeaways": [
              "CLIP maps images and text to a shared embedding space",
              "Enables zero-shot classification without training",
              "Powers image search by text and similarity search",
              "Useful for content filtering and categorization"
            ]
          }
        }
      ]
    },
    {
      "id": "audio-models",
      "title": "Audio AI Models",
      "lessons": [
        {
          "id": "speech-to-text",
          "title": "Speech-to-Text with Whisper",
          "duration": "35 min",
          "content": {
            "overview": "OpenAI's Whisper is a powerful speech recognition model supporting 99 languages, transcription, and translation.",
            "sections": [
              {
                "title": "Whisper Overview",
                "content": "**Whisper Capabilities:**\n- Automatic speech recognition (ASR)\n- Language detection\n- Translation to English\n- Timestamp generation\n- Supports 99 languages\n\n**Model Sizes:**\n- tiny (39M), base (74M), small (244M)\n- medium (769M), large (1.5B), large-v3 (best quality)\n\n**Use Cases:**\n- Meeting transcription\n- Podcast transcription\n- Voice notes\n- Accessibility (captions)\n- Voice commands",
                "code": "from openai import OpenAI\n\nclient = OpenAI()\n\n# Basic transcription\ndef transcribe_audio(audio_path):\n    \"\"\"Transcribe audio file to text\"\"\"\n    with open(audio_path, \"rb\") as f:\n        transcript = client.audio.transcriptions.create(\n            model=\"whisper-1\",\n            file=f,\n            response_format=\"text\"\n        )\n    return transcript\n\n# Transcription with timestamps\ndef transcribe_with_timestamps(audio_path):\n    \"\"\"Get word-level timestamps\"\"\"\n    with open(audio_path, \"rb\") as f:\n        result = client.audio.transcriptions.create(\n            model=\"whisper-1\",\n            file=f,\n            response_format=\"verbose_json\",\n            timestamp_granularities=[\"word\", \"segment\"]\n        )\n    \n    return {\n        \"text\": result.text,\n        \"segments\": result.segments,\n        \"words\": result.words\n    }\n\n# Translation to English\ndef translate_to_english(audio_path):\n    \"\"\"Translate audio to English\"\"\"\n    with open(audio_path, \"rb\") as f:\n        translation = client.audio.translations.create(\n            model=\"whisper-1\",\n            file=f\n        )\n    return translation.text\n\n# Usage\ntranscript = transcribe_audio(\"meeting.mp3\")\nprint(transcript)\n\ndetailed = transcribe_with_timestamps(\"podcast.mp3\")\nfor segment in detailed[\"segments\"]:\n    print(f\"[{segment.start:.2f}s] {segment.text}\")"
              },
              {
                "title": "Local Whisper with faster-whisper",
                "content": "Run Whisper locally for faster/cheaper processing:",
                "code": "# pip install faster-whisper\n\nfrom faster_whisper import WhisperModel\nimport torch\n\nclass LocalTranscriber:\n    def __init__(self, model_size=\"base\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n        compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n        self.model = WhisperModel(\n            model_size,\n            device=device,\n            compute_type=compute_type\n        )\n    \n    def transcribe(self, audio_path, language=None):\n        \"\"\"Transcribe audio file\"\"\"\n        segments, info = self.model.transcribe(\n            audio_path,\n            language=language,\n            beam_size=5,\n            word_timestamps=True\n        )\n        \n        result = {\n            \"language\": info.language,\n            \"language_probability\": info.language_probability,\n            \"segments\": []\n        }\n        \n        for segment in segments:\n            result[\"segments\"].append({\n                \"start\": segment.start,\n                \"end\": segment.end,\n                \"text\": segment.text,\n                \"words\": [\n                    {\"word\": w.word, \"start\": w.start, \"end\": w.end}\n                    for w in segment.words\n                ] if segment.words else []\n            })\n        \n        return result\n    \n    def transcribe_with_speakers(self, audio_path):\n        \"\"\"Transcribe and attempt speaker diarization\"\"\"\n        # Note: For proper diarization, use pyannote-audio\n        result = self.transcribe(audio_path)\n        \n        # Simple speaker change detection based on pauses\n        speakers = []\n        current_speaker = 0\n        \n        for i, seg in enumerate(result[\"segments\"]):\n            if i > 0:\n                gap = seg[\"start\"] - result[\"segments\"][i-1][\"end\"]\n                if gap > 1.5:  # Long pause might indicate speaker change\n                    current_speaker = (current_speaker + 1) % 2\n            \n            speakers.append(current_speaker)\n        \n        return {\n            \"segments\": result[\"segments\"],\n            \"speakers\": speakers\n        }\n\n# Usage\ntranscriber = LocalTranscriber(model_size=\"medium\")\nresult = transcriber.transcribe(\"interview.mp3\")\n\nfor seg in result[\"segments\"]:\n    print(f\"[{seg['start']:.1f}s - {seg['end']:.1f}s] {seg['text']}\")"
              },
              {
                "title": "Building a Transcription Pipeline",
                "content": "Complete audio processing pipeline:",
                "code": "from pathlib import Path\nimport subprocess\nimport json\n\nclass AudioPipeline:\n    def __init__(self, whisper_model=\"base\"):\n        self.transcriber = LocalTranscriber(model_size=whisper_model)\n    \n    def preprocess_audio(self, input_path, output_path=None):\n        \"\"\"\n        Preprocess audio for optimal transcription:\n        - Convert to mono\n        - Resample to 16kHz\n        - Normalize volume\n        \"\"\"\n        if output_path is None:\n            output_path = Path(input_path).with_suffix(\".processed.wav\")\n        \n        cmd = [\n            \"ffmpeg\", \"-i\", str(input_path),\n            \"-ac\", \"1\",  # Mono\n            \"-ar\", \"16000\",  # 16kHz\n            \"-af\", \"loudnorm\",  # Normalize\n            \"-y\", str(output_path)\n        ]\n        \n        subprocess.run(cmd, capture_output=True, check=True)\n        return str(output_path)\n    \n    def process_long_audio(self, audio_path, chunk_minutes=10):\n        \"\"\"\n        Process long audio files in chunks\n        \"\"\"\n        # Get duration\n        result = subprocess.run(\n            [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n             \"-of\", \"json\", audio_path],\n            capture_output=True, text=True\n        )\n        duration = float(json.loads(result.stdout)[\"format\"][\"duration\"])\n        \n        chunk_seconds = chunk_minutes * 60\n        all_segments = []\n        offset = 0\n        \n        while offset < duration:\n            # Extract chunk\n            chunk_path = f\"temp_chunk_{offset}.wav\"\n            subprocess.run([\n                \"ffmpeg\", \"-i\", audio_path,\n                \"-ss\", str(offset),\n                \"-t\", str(chunk_seconds),\n                \"-y\", chunk_path\n            ], capture_output=True)\n            \n            # Transcribe chunk\n            result = self.transcriber.transcribe(chunk_path)\n            \n            # Adjust timestamps and add to results\n            for seg in result[\"segments\"]:\n                seg[\"start\"] += offset\n                seg[\"end\"] += offset\n                all_segments.append(seg)\n            \n            # Cleanup\n            Path(chunk_path).unlink()\n            offset += chunk_seconds\n        \n        return {\"segments\": all_segments}\n    \n    def generate_subtitles(self, audio_path, output_format=\"srt\"):\n        \"\"\"Generate subtitle file from audio\"\"\"\n        result = self.transcriber.transcribe(audio_path)\n        \n        if output_format == \"srt\":\n            return self._to_srt(result[\"segments\"])\n        elif output_format == \"vtt\":\n            return self._to_vtt(result[\"segments\"])\n    \n    def _to_srt(self, segments):\n        \"\"\"Convert to SRT format\"\"\"\n        lines = []\n        for i, seg in enumerate(segments, 1):\n            start = self._format_time_srt(seg[\"start\"])\n            end = self._format_time_srt(seg[\"end\"])\n            lines.append(f\"{i}\")\n            lines.append(f\"{start} --> {end}\")\n            lines.append(seg[\"text\"].strip())\n            lines.append(\"\")\n        return \"\\n\".join(lines)\n    \n    def _format_time_srt(self, seconds):\n        hours = int(seconds // 3600)\n        minutes = int((seconds % 3600) // 60)\n        secs = int(seconds % 60)\n        millis = int((seconds % 1) * 1000)\n        return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}\"\n\n# Usage\npipeline = AudioPipeline(whisper_model=\"medium\")\n\n# Process a video file\nsubtitles = pipeline.generate_subtitles(\"video.mp4\", output_format=\"srt\")\nwith open(\"video.srt\", \"w\") as f:\n    f.write(subtitles)"
              }
            ],
            "keyTakeaways": [
              "Whisper supports 99 languages with high accuracy",
              "Use OpenAI API or run locally with faster-whisper",
              "Preprocess audio for better results",
              "Generate subtitles and timestamps easily"
            ]
          }
        },
        {
          "id": "text-to-speech",
          "title": "Text-to-Speech Generation",
          "duration": "30 min",
          "content": {
            "overview": "Generate natural-sounding speech from text using modern TTS models from OpenAI, ElevenLabs, and open-source alternatives.",
            "sections": [
              {
                "title": "OpenAI TTS",
                "content": "OpenAI offers high-quality TTS:",
                "code": "from openai import OpenAI\nfrom pathlib import Path\n\nclient = OpenAI()\n\ndef text_to_speech(text, voice=\"alloy\", model=\"tts-1-hd\", output_path=\"speech.mp3\"):\n    \"\"\"\n    Convert text to speech\n    \n    Voices: alloy, echo, fable, onyx, nova, shimmer\n    Models: tts-1 (faster), tts-1-hd (higher quality)\n    \"\"\"\n    response = client.audio.speech.create(\n        model=model,\n        voice=voice,\n        input=text\n    )\n    \n    with open(output_path, \"wb\") as f:\n        f.write(response.content)\n    \n    return output_path\n\n# Basic usage\ntext_to_speech(\n    \"Hello! Welcome to our AI-powered application.\",\n    voice=\"nova\"\n)\n\n# Streaming for long text\ndef stream_tts(text, output_path):\n    \"\"\"Stream TTS for long content\"\"\"\n    with client.audio.speech.with_streaming_response.create(\n        model=\"tts-1\",\n        voice=\"alloy\",\n        input=text\n    ) as response:\n        with open(output_path, \"wb\") as f:\n            for chunk in response.iter_bytes():\n                f.write(chunk)\n\n# Generate audio for all voices (for comparison)\ndef generate_voice_samples(text):\n    voices = [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]\n    for voice in voices:\n        output = f\"sample_{voice}.mp3\"\n        text_to_speech(text, voice=voice, output_path=output)\n        print(f\"Generated: {output}\")"
              },
              {
                "title": "ElevenLabs for Voice Cloning",
                "content": "ElevenLabs offers advanced features like voice cloning:",
                "code": "# pip install elevenlabs\n\nfrom elevenlabs import generate, save, Voice, VoiceSettings\nfrom elevenlabs.client import ElevenLabs\n\nclient = ElevenLabs(api_key=\"your-api-key\")\n\ndef elevenlabs_tts(text, voice_id=None):\n    \"\"\"Generate speech with ElevenLabs\"\"\"\n    audio = client.generate(\n        text=text,\n        voice=voice_id or \"Rachel\",  # Default voice\n        model=\"eleven_multilingual_v2\"\n    )\n    \n    save(audio, \"output.mp3\")\n    return \"output.mp3\"\n\n# Clone a voice from audio sample\ndef clone_voice(name, audio_files, description=\"\"):\n    \"\"\"Clone a voice from audio samples\"\"\"\n    voice = client.clone(\n        name=name,\n        description=description,\n        files=audio_files  # List of audio file paths\n    )\n    return voice\n\n# Use cloned voice\ndef use_cloned_voice(voice_id, text):\n    audio = client.generate(\n        text=text,\n        voice=Voice(\n            voice_id=voice_id,\n            settings=VoiceSettings(\n                stability=0.5,\n                similarity_boost=0.75,\n                style=0.5,\n                use_speaker_boost=True\n            )\n        )\n    )\n    save(audio, \"cloned_speech.mp3\")"
              },
              {
                "title": "Building Voice Applications",
                "content": "Complete voice application example:",
                "code": "from openai import OpenAI\nimport tempfile\nimport os\n\nclass VoiceAssistant:\n    def __init__(self):\n        self.client = OpenAI()\n        self.conversation_history = []\n    \n    def listen(self, audio_path):\n        \"\"\"Transcribe user speech\"\"\"\n        with open(audio_path, \"rb\") as f:\n            transcript = self.client.audio.transcriptions.create(\n                model=\"whisper-1\",\n                file=f\n            )\n        return transcript.text\n    \n    def think(self, user_message):\n        \"\"\"Process with LLM\"\"\"\n        self.conversation_history.append({\n            \"role\": \"user\",\n            \"content\": user_message\n        })\n        \n        response = self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful voice assistant. Keep responses concise and conversational.\"},\n                *self.conversation_history\n            ]\n        )\n        \n        reply = response.choices[0].message.content\n        self.conversation_history.append({\n            \"role\": \"assistant\",\n            \"content\": reply\n        })\n        \n        return reply\n    \n    def speak(self, text, output_path=None):\n        \"\"\"Convert response to speech\"\"\"\n        if output_path is None:\n            output_path = tempfile.mktemp(suffix=\".mp3\")\n        \n        response = self.client.audio.speech.create(\n            model=\"tts-1-hd\",\n            voice=\"nova\",\n            input=text\n        )\n        \n        with open(output_path, \"wb\") as f:\n            f.write(response.content)\n        \n        return output_path\n    \n    def process_voice(self, audio_input_path):\n        \"\"\"Full voice interaction: listen -> think -> speak\"\"\"\n        # Transcribe\n        user_text = self.listen(audio_input_path)\n        print(f\"User: {user_text}\")\n        \n        # Process\n        response_text = self.think(user_text)\n        print(f\"Assistant: {response_text}\")\n        \n        # Speak\n        audio_output = self.speak(response_text)\n        \n        return {\n            \"user_text\": user_text,\n            \"response_text\": response_text,\n            \"audio_output\": audio_output\n        }\n\n# Usage\nassistant = VoiceAssistant()\nresult = assistant.process_voice(\"user_question.mp3\")\n# Play result[\"audio_output\"]"
              }
            ],
            "keyTakeaways": [
              "OpenAI TTS provides 6 voices with HD quality option",
              "ElevenLabs enables voice cloning from samples",
              "Combine STT + LLM + TTS for voice assistants",
              "Consider streaming for long content"
            ]
          }
        }
      ]
    },
    {
      "id": "advanced-multimodal",
      "title": "Advanced Multimodal Applications",
      "lessons": [
        {
          "id": "video-understanding",
          "title": "Video Understanding",
          "duration": "35 min",
          "content": {
            "overview": "Process videos by combining frame extraction with vision models and audio transcription for complete understanding.",
            "sections": [
              {
                "title": "Frame-Based Video Analysis",
                "content": "Extract frames and analyze with VLMs:",
                "code": "import cv2\nimport base64\nfrom openai import OpenAI\nimport numpy as np\n\nclient = OpenAI()\n\nclass VideoAnalyzer:\n    def __init__(self):\n        self.client = OpenAI()\n    \n    def extract_frames(self, video_path, interval_seconds=1):\n        \"\"\"Extract frames at regular intervals\"\"\"\n        cap = cv2.VideoCapture(video_path)\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        frame_interval = int(fps * interval_seconds)\n        \n        frames = []\n        frame_count = 0\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            if frame_count % frame_interval == 0:\n                # Convert to base64\n                _, buffer = cv2.imencode(\".jpg\", frame)\n                base64_frame = base64.b64encode(buffer).decode(\"utf-8\")\n                \n                frames.append({\n                    \"timestamp\": frame_count / fps,\n                    \"base64\": base64_frame\n                })\n            \n            frame_count += 1\n        \n        cap.release()\n        return frames\n    \n    def analyze_video(self, video_path, prompt=\"Describe what happens in this video\"):\n        \"\"\"Analyze video content\"\"\"\n        # Extract key frames\n        frames = self.extract_frames(video_path, interval_seconds=2)\n        \n        # Limit frames for API (max ~10-20 for cost/context)\n        if len(frames) > 10:\n            indices = np.linspace(0, len(frames) - 1, 10, dtype=int)\n            frames = [frames[i] for i in indices]\n        \n        # Build message content\n        content = [{\"type\": \"text\", \"text\": prompt + \"\\n\\nHere are frames from the video:\"}]\n        \n        for i, frame in enumerate(frames):\n            content.append({\n                \"type\": \"text\",\n                \"text\": f\"\\n[Frame at {frame['timestamp']:.1f}s]\"\n            })\n            content.append({\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{frame['base64']}\",\n                    \"detail\": \"low\"  # Save tokens\n                }\n            })\n        \n        response = self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": content}],\n            max_tokens=1000\n        )\n        \n        return response.choices[0].message.content\n    \n    def analyze_with_audio(self, video_path, prompt=\"Summarize this video\"):\n        \"\"\"Full video analysis with audio\"\"\"\n        import subprocess\n        import tempfile\n        \n        # Extract audio\n        audio_path = tempfile.mktemp(suffix=\".mp3\")\n        subprocess.run([\n            \"ffmpeg\", \"-i\", video_path,\n            \"-vn\", \"-acodec\", \"mp3\",\n            audio_path\n        ], capture_output=True)\n        \n        # Transcribe audio\n        with open(audio_path, \"rb\") as f:\n            transcript = self.client.audio.transcriptions.create(\n                model=\"whisper-1\",\n                file=f\n            )\n        \n        # Analyze visuals\n        visual_description = self.analyze_video(video_path)\n        \n        # Combine for final analysis\n        response = self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"\"\"Analyze this video based on:\n\nVISUAL CONTENT:\n{visual_description}\n\nAUDIO TRANSCRIPT:\n{transcript.text}\n\n{prompt}\"\"\"\n            }],\n            max_tokens=1000\n        )\n        \n        return {\n            \"visual\": visual_description,\n            \"transcript\": transcript.text,\n            \"analysis\": response.choices[0].message.content\n        }\n\n# Usage\nanalyzer = VideoAnalyzer()\nresult = analyzer.analyze_with_audio(\"tutorial.mp4\", \"Create a summary and list of key topics\")"
              },
              {
                "title": "Gemini Native Video",
                "content": "Google's Gemini can process videos directly - think of it as having an AI that can 'watch' your video rather than just looking at screenshots. Gemini supports multiple input methods: YouTube URLs (preview feature), file uploads via Files API, and inline base64 data. You can even clip specific segments and control frame sampling rate (Source: Google Gemini Video Understanding docs):",
                "code": "from google import genai\nfrom google.genai import types\nimport time\n\nclient = genai.Client(api_key=\"your-api-key\")\n\n# === METHOD 1: YouTube URL (Preview Feature) ===\n# Gemini can analyze YouTube videos directly!\ndef analyze_youtube_video(youtube_url, prompt):\n    \"\"\"\n    Analyze a YouTube video directly by URL.\n    Note: This is a preview feature with usage limits.\n    \"\"\"\n    response = client.models.generate_content(\n        model='models/gemini-2.5-flash',\n        contents=types.Content(\n            parts=[\n                types.Part(\n                    file_data=types.FileData(file_uri=youtube_url)\n                ),\n                types.Part(text=prompt)\n            ]\n        )\n    )\n    return response.text\n\n# Usage - Just pass the YouTube URL!\nanalysis = analyze_youtube_video(\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"What happens in this video? Describe the key moments.\"\n)\n\n# === METHOD 2: Files API Upload ===\ndef analyze_uploaded_video(video_path, prompt):\n    \"\"\"\n    Upload and analyze video files (MP4, MOV, AVI, WebM, etc.)\n    Supports videos up to 2GB.\n    \"\"\"\n    # Upload video to Files API\n    video_file = client.files.upload(file=video_path)\n    \n    # Wait for processing (required for larger videos)\n    while video_file.state.name == \"PROCESSING\":\n        time.sleep(5)\n        video_file = client.files.get(name=video_file.name)\n    \n    if video_file.state.name == \"FAILED\":\n        raise Exception(f\"Video processing failed: {video_file.error}\")\n    \n    # Generate content\n    response = client.models.generate_content(\n        model='models/gemini-2.5-flash',\n        contents=[video_file, prompt]\n    )\n    return response.text\n\n# === METHOD 3: Video Clipping ===\n# Analyze only a specific portion of the video\ndef analyze_video_segment(video_path, start_seconds, end_seconds, prompt):\n    \"\"\"\n    Clip a specific segment of the video for analysis.\n    Great for long videos where you only need part of it.\n    \"\"\"\n    video_file = client.files.upload(file=video_path)\n    \n    # Wait for processing\n    while video_file.state.name == \"PROCESSING\":\n        time.sleep(5)\n        video_file = client.files.get(name=video_file.name)\n    \n    response = client.models.generate_content(\n        model='models/gemini-2.5-flash',\n        contents=types.Content(\n            parts=[\n                types.Part(\n                    file_data=types.FileData(file_uri=video_file.uri),\n                    # Specify the segment to analyze\n                    video_metadata=types.VideoMetadata(\n                        start_offset=f\"{start_seconds}s\",\n                        end_offset=f\"{end_seconds}s\"\n                    )\n                ),\n                types.Part(text=prompt)\n            ]\n        )\n    )\n    return response.text\n\n# Analyze only 5 minutes starting at the 20-minute mark\nresult = analyze_video_segment(\n    \"long_lecture.mp4\",\n    start_seconds=1200,  # 20:00\n    end_seconds=1500,    # 25:00\n    prompt=\"Summarize the key concepts explained in this segment.\"\n)\n\n# === METHOD 4: Custom Frame Rate ===\n# Control how many frames per second Gemini samples\ndef analyze_with_custom_fps(video_path, fps, prompt):\n    \"\"\"\n    Control frame sampling rate.\n    Lower FPS = fewer tokens, good for slow-moving content.\n    Higher FPS = more detail, good for action sequences.\n    Default is 1 FPS, minimum is 0.1 FPS.\n    \"\"\"\n    video_file = client.files.upload(file=video_path)\n    \n    while video_file.state.name == \"PROCESSING\":\n        time.sleep(5)\n        video_file = client.files.get(name=video_file.name)\n    \n    response = client.models.generate_content(\n        model='models/gemini-2.5-flash',\n        contents=types.Content(\n            parts=[\n                types.Part(\n                    file_data=types.FileData(file_uri=video_file.uri),\n                    video_metadata=types.VideoMetadata(fps=fps)\n                ),\n                types.Part(text=prompt)\n            ]\n        )\n    )\n    return response.text\n\n# Low FPS for a presentation (mostly static slides)\nresult = analyze_with_custom_fps(\"presentation.mp4\", fps=0.5, prompt=\"Extract all text from the slides.\")\n\n# Higher FPS for sports footage\nresult = analyze_with_custom_fps(\"game_highlights.mp4\", fps=5, prompt=\"Describe each play in detail.\")\n\n# === TIMESTAMP QUERIES ===\n# Ask about specific moments using MM:SS format\nresponse = client.models.generate_content(\n    model='models/gemini-2.5-flash',\n    contents=[video_file, \"What is the speaker explaining at 05:30?\"]\n)"
              }
            ],
            "keyTakeaways": [
              "Extract frames at intervals for video analysis with traditional VLMs",
              "Combine visual and audio analysis for complete understanding",
              "Gemini supports native video: YouTube URLs, file uploads, clipping, custom FPS",
              "Use lower FPS for static content (presentations), higher for action",
              "Clip long videos to analyze only relevant segments"
            ]
          }
        },
        {
          "id": "multimodal-rag",
          "title": "Multimodal RAG",
          "duration": "40 min",
          "content": {
            "overview": "Build RAG systems that can retrieve and reason over both text and images for richer knowledge bases.",
            "sections": [
              {
                "title": "Multimodal Document Processing",
                "content": "Process documents with text and images:",
                "code": "from openai import OpenAI\nimport fitz  # PyMuPDF\nimport base64\nfrom pathlib import Path\n\nclass MultimodalDocProcessor:\n    def __init__(self):\n        self.client = OpenAI()\n    \n    def process_pdf(self, pdf_path):\n        \"\"\"Extract text and images from PDF\"\"\"\n        doc = fitz.open(pdf_path)\n        pages = []\n        \n        for page_num, page in enumerate(doc):\n            page_data = {\n                \"page\": page_num + 1,\n                \"text\": page.get_text(),\n                \"images\": []\n            }\n            \n            # Extract images\n            for img_idx, img in enumerate(page.get_images()):\n                xref = img[0]\n                pix = fitz.Pixmap(doc, xref)\n                \n                if pix.n > 4:  # CMYK to RGB\n                    pix = fitz.Pixmap(fitz.csRGB, pix)\n                \n                img_data = pix.tobytes(\"png\")\n                base64_img = base64.b64encode(img_data).decode()\n                \n                # Generate description\n                description = self._describe_image(base64_img)\n                \n                page_data[\"images\"].append({\n                    \"index\": img_idx,\n                    \"base64\": base64_img,\n                    \"description\": description\n                })\n            \n            pages.append(page_data)\n        \n        return pages\n    \n    def _describe_image(self, base64_img):\n        \"\"\"Get description for an image\"\"\"\n        response = self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"Describe this image in detail for document indexing.\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_img}\"}}\n                ]\n            }],\n            max_tokens=200\n        )\n        return response.choices[0].message.content\n    \n    def create_chunks(self, pages, chunk_size=500):\n        \"\"\"Create chunks combining text and image descriptions\"\"\"\n        chunks = []\n        \n        for page in pages:\n            # Add text chunks\n            text = page[\"text\"]\n            for i in range(0, len(text), chunk_size):\n                chunks.append({\n                    \"type\": \"text\",\n                    \"page\": page[\"page\"],\n                    \"content\": text[i:i+chunk_size]\n                })\n            \n            # Add image descriptions as chunks\n            for img in page[\"images\"]:\n                chunks.append({\n                    \"type\": \"image\",\n                    \"page\": page[\"page\"],\n                    \"content\": img[\"description\"],\n                    \"image_base64\": img[\"base64\"]\n                })\n        \n        return chunks"
              },
              {
                "title": "Multimodal Vector Store",
                "content": "Index both text and images:",
                "code": "import chromadb\nfrom chromadb.utils import embedding_functions\nimport numpy as np\n\nclass MultimodalVectorStore:\n    def __init__(self, collection_name=\"multimodal_docs\"):\n        self.client = chromadb.Client()\n        self.openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n            model_name=\"text-embedding-3-small\"\n        )\n        self.collection = self.client.get_or_create_collection(\n            name=collection_name,\n            embedding_function=self.openai_ef\n        )\n        self.image_data = {}  # Store images separately\n    \n    def add_chunks(self, chunks):\n        \"\"\"Add chunks to vector store\"\"\"\n        ids = []\n        documents = []\n        metadatas = []\n        \n        for i, chunk in enumerate(chunks):\n            chunk_id = f\"chunk_{i}\"\n            ids.append(chunk_id)\n            documents.append(chunk[\"content\"])\n            \n            metadata = {\n                \"type\": chunk[\"type\"],\n                \"page\": chunk[\"page\"]\n            }\n            \n            # Store image data separately\n            if chunk[\"type\"] == \"image\" and \"image_base64\" in chunk:\n                self.image_data[chunk_id] = chunk[\"image_base64\"]\n                metadata[\"has_image\"] = True\n            \n            metadatas.append(metadata)\n        \n        self.collection.add(\n            ids=ids,\n            documents=documents,\n            metadatas=metadatas\n        )\n    \n    def query(self, query_text, n_results=5, include_images=True):\n        \"\"\"Query the multimodal store\"\"\"\n        results = self.collection.query(\n            query_texts=[query_text],\n            n_results=n_results\n        )\n        \n        # Enrich with image data\n        enriched_results = []\n        for i, doc_id in enumerate(results[\"ids\"][0]):\n            result = {\n                \"id\": doc_id,\n                \"content\": results[\"documents\"][0][i],\n                \"metadata\": results[\"metadatas\"][0][i],\n                \"distance\": results[\"distances\"][0][i] if \"distances\" in results else None\n            }\n            \n            if include_images and doc_id in self.image_data:\n                result[\"image_base64\"] = self.image_data[doc_id]\n            \n            enriched_results.append(result)\n        \n        return enriched_results\n\nclass MultimodalRAG:\n    def __init__(self):\n        self.processor = MultimodalDocProcessor()\n        self.store = MultimodalVectorStore()\n        self.client = OpenAI()\n    \n    def index_document(self, doc_path):\n        \"\"\"Index a document\"\"\"\n        pages = self.processor.process_pdf(doc_path)\n        chunks = self.processor.create_chunks(pages)\n        self.store.add_chunks(chunks)\n    \n    def query(self, question):\n        \"\"\"Query with multimodal context\"\"\"\n        results = self.store.query(question, n_results=5)\n        \n        # Build context with images\n        content = [{\n            \"type\": \"text\",\n            \"text\": f\"\"\"Answer the question based on the following context.\n\nContext:\n\"\"\"\n        }]\n        \n        for r in results:\n            content.append({\n                \"type\": \"text\",\n                \"text\": f\"\\n[Page {r['metadata']['page']}] {r['content']}\\n\"\n            })\n            \n            if \"image_base64\" in r:\n                content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/png;base64,{r['image_base64']}\",\n                        \"detail\": \"low\"\n                    }\n                })\n        \n        content.append({\n            \"type\": \"text\",\n            \"text\": f\"\\nQuestion: {question}\"\n        })\n        \n        response = self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": content}],\n            max_tokens=1000\n        )\n        \n        return response.choices[0].message.content\n\n# Usage\nrag = MultimodalRAG()\nrag.index_document(\"technical_manual.pdf\")\nanswer = rag.query(\"What does the diagram on page 5 show?\")"
              }
            ],
            "keyTakeaways": [
              "Process documents to extract both text and images",
              "Generate descriptions for images to enable text search",
              "Include relevant images in RAG context",
              "VLMs can reason over retrieved images directly"
            ]
          }
        },
        {
          "id": "multimodal-architectures",
          "title": "Multimodal Architectures & Fusion Strategies",
          "duration": "30 min",
          "content": {
            "overview": "Understanding the theoretical foundations of multimodal AI helps you choose the right architecture for your use case. Learn fusion strategies, core challenges, and how leading multimodal models are designed.",
            "sections": [
              {
                "title": "Three Characteristics of Multimodal Data",
                "content": "Research from Carnegie Mellon (cited by IBM) identifies three core characteristics that make multimodal AI challenging:\n\n**1. Heterogeneity**\n- Different modalities have fundamentally different qualities, structures, and representations\n- A text description of a sunset is very different from a photo of the same scene\n- Audio is sequential/temporal, images are spatial, text is symbolic\n\n**2. Connections**\n- Modalities share complementary information\n- Statistical correlations: words that appear with certain visual patterns\n- Semantic correspondence: text \"dog\" corresponds to visual features of dogs\n- Temporal sync: audio and video frames must align in time\n\n**3. Interactions**\n- When combined, modalities influence each other\n- Visual context can disambiguate text (\"bank\" with river image vs building image)\n- Text can focus attention on specific image regions\n- Audio tone can change text interpretation",
                "diagram": {
                  "title": "Multimodal Data Characteristics",
                  "code": "flowchart TB\n    subgraph Heterogeneity\n        T[\"ðŸ“ Text\\n(Symbolic)\"]\n        I[\"ðŸ–¼ï¸ Image\\n(Spatial)\"]\n        A[\"ðŸ”Š Audio\\n(Temporal)\"]\n    end\n    \n    subgraph Connections\n        SC[Statistical Correlation]\n        SM[Semantic Mapping]\n        TS[Temporal Sync]\n    end\n    \n    subgraph Interactions\n        INT[\"Mutual Influence\\nContext & Disambiguation\"]\n    end\n    \n    T --> SC\n    I --> SC\n    I --> SM\n    T --> SM\n    A --> TS\n    I --> TS\n    \n    SC --> INT\n    SM --> INT\n    TS --> INT"
                }
              },
              {
                "title": "Fusion Strategies",
                "content": "How and when to combine different modalities is a critical architectural decision:\n\n**Early Fusion (Input-Level)**\n- Combine modalities before encoding\n- Create unified representation from the start\n- Pros: Rich cross-modal learning, captures low-level interactions\n- Cons: Requires aligned inputs, computationally expensive\n- Example: Concatenating image patches with text tokens before transformer\n\n**Mid Fusion (Feature-Level)**\n- Encode modalities separately, combine intermediate representations\n- Multiple fusion points during processing\n- Pros: Balances modality-specific and cross-modal learning\n- Cons: Complex architecture design\n- Example: Cross-attention between image features and text at specific layers\n\n**Late Fusion (Decision-Level)**\n- Process each modality with separate models\n- Combine final outputs/predictions\n- Pros: Simple, modular, use best model per modality\n- Cons: Limited cross-modal interaction, may miss subtle connections\n- Example: Separate image and text classifiers, combine scores",
                "diagram": {
                  "title": "Fusion Strategies Comparison",
                  "code": "flowchart LR\n    subgraph Early[\"Early Fusion\"]\n        direction TB\n        I1[Image] --> C1[Combine]\n        T1[Text] --> C1\n        C1 --> E1[Encoder] --> O1[Output]\n    end\n    \n    subgraph Mid[\"Mid Fusion\"]\n        direction TB\n        I2[Image] --> IE[Img Enc]\n        T2[Text] --> TE[Txt Enc]\n        IE --> CA[Cross\\nAttention]\n        TE --> CA\n        CA --> O2[Output]\n    end\n    \n    subgraph Late[\"Late Fusion\"]\n        direction TB\n        I3[Image] --> IM[Img Model] --> IP[Pred]\n        T3[Text] --> TM[Txt Model] --> TP[Pred]\n        IP --> CMB[Combine]\n        TP --> CMB\n        CMB --> O3[Output]\n    end\n    \n    style C1 fill:#dcfce7\n    style CA fill:#fef3c7\n    style CMB fill:#fce7f3"
                },
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nimport torch.nn as nn\n\nclass EarlyFusion(nn.Module):\n    \"\"\"Early fusion: combine inputs before encoding.\"\"\"\n    def __init__(self, img_dim, txt_dim, hidden_dim):\n        super().__init__()\n        self.img_proj = nn.Linear(img_dim, hidden_dim)\n        self.txt_proj = nn.Linear(txt_dim, hidden_dim)\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(hidden_dim, 8), num_layers=6\n        )\n        self.classifier = nn.Linear(hidden_dim, 1)\n    \n    def forward(self, img_features, txt_features):\n        # Project to common space\n        img = self.img_proj(img_features)  # [B, N_img, D]\n        txt = self.txt_proj(txt_features)  # [B, N_txt, D]\n        \n        # Concatenate and encode together\n        combined = torch.cat([img, txt], dim=1)  # [B, N_img + N_txt, D]\n        encoded = self.encoder(combined)\n        \n        # Pool and classify\n        pooled = encoded.mean(dim=1)\n        return self.classifier(pooled)\n\n\nclass MidFusion(nn.Module):\n    \"\"\"Mid fusion: cross-attention between modalities.\"\"\"\n    def __init__(self, img_dim, txt_dim, hidden_dim):\n        super().__init__()\n        self.img_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(img_dim, 8), num_layers=3\n        )\n        self.txt_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(txt_dim, 8), num_layers=3\n        )\n        # Cross-attention: text attends to images\n        self.cross_attn = nn.MultiheadAttention(hidden_dim, 8)\n        self.classifier = nn.Linear(hidden_dim, 1)\n    \n    def forward(self, img_features, txt_features):\n        # Encode each modality separately\n        img_enc = self.img_encoder(img_features)\n        txt_enc = self.txt_encoder(txt_features)\n        \n        # Cross-attention: text queries, image keys/values\n        fused, _ = self.cross_attn(txt_enc, img_enc, img_enc)\n        \n        pooled = fused.mean(dim=1)\n        return self.classifier(pooled)\n\n\nclass LateFusion(nn.Module):\n    \"\"\"Late fusion: separate models, combined predictions.\"\"\"\n    def __init__(self, img_model, txt_model, hidden_dim):\n        super().__init__()\n        self.img_model = img_model  # Pre-trained image model\n        self.txt_model = txt_model  # Pre-trained text model\n        self.combiner = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    \n    def forward(self, img_input, txt_input):\n        # Get predictions from each model\n        img_out = self.img_model(img_input)  # [B, D]\n        txt_out = self.txt_model(txt_input)  # [B, D]\n        \n        # Combine and classify\n        combined = torch.cat([img_out, txt_out], dim=-1)\n        return self.combiner(combined)"
                }
              },
              {
                "title": "Six Multimodal AI Challenges",
                "content": "Research identifies six core challenges for multimodal systems:\n\n**1. Representation**\nHow to represent multimodal data while reflecting heterogeneity and interconnections?\n- Joint embedding spaces (CLIP)\n- Attention mechanisms for cross-modal features\n\n**2. Alignment**\nHow to identify connections across modalities?\n- Temporal alignment (sync audio to video)\n- Spatial alignment (ground text in image regions)\n- Semantic alignment (match concepts across modalities)\n\n**3. Reasoning**\nHow to compose knowledge from multimodal evidence?\n- Multi-step inference combining visual and textual cues\n- Chain-of-thought with multimodal context\n\n**4. Generation**\nHow to generate coherent cross-modal outputs?\n- Text-to-image with consistency\n- Video generation with temporal coherence\n\n**5. Transference**\nHow to transfer knowledge between modalities?\n- Zero-shot transfer from text to images\n- Cross-modal learning with limited paired data\n\n**6. Quantification**\nHow to evaluate multimodal performance?\n- Cross-modal retrieval metrics\n- Generation quality across modalities"
              },
              {
                "title": "Real-World Multimodal Architectures",
                "content": "How leading models implement multimodal AI:\n\n**CLIP (Contrastive Learning)**\n- Separate image and text encoders\n- Trained with contrastive loss to align embeddings\n- Late fusion: similarity in shared embedding space\n\n**GPT-4V / GPT-4o**\n- Vision encoder converts images to tokens\n- Early-to-mid fusion: image tokens processed alongside text\n- Single transformer processes both modalities\n\n**Gemini (Native Multimodal)**\n- Designed multimodal from the start\n- Video, audio, images, text as first-class inputs\n- Mid fusion with cross-modal attention throughout\n\n**LLaVA (Vision-Language Adapter)**\n- Frozen vision encoder + frozen LLM\n- Learnable projection layer bridges modalities\n- Efficient: only train the adapter\n\n**Whisper (Audio)**\n- Audio spectrogram as input\n- Transformer encoder-decoder\n- Early fusion: audio features encoded like tokens"
              },
              {
                "title": "Choosing an Architecture",
                "content": "| Scenario | Recommended Fusion | Reason |\n|----------|-------------------|--------|\n| Limited paired data | Late fusion | Can use pretrained unimodal models |\n| Fine-grained interaction needed | Early/Mid fusion | Captures subtle cross-modal cues |\n| Real-time requirements | Late fusion | Parallel processing, easier optimization |\n| Modalities highly complementary | Mid fusion | Cross-attention captures relationships |\n| One modality dominant | Mid fusion with primary encoder | Let strong modality guide |\n| Maximum accuracy, resources available | Early fusion | Richest representation |\n\n**Practical Tips:**\n- Start with late fusion using pretrained models\n- Add cross-attention if needed for your task\n- Early fusion requires more data and compute\n- Consider hybrid approaches for complex tasks"
              }
            ],
            "keyTakeaways": [
              "Multimodal data has heterogeneity, connections, and interactions",
              "Three fusion strategies: early (input), mid (feature), late (decision)",
              "Six challenges: representation, alignment, reasoning, generation, transference, quantification",
              "Match fusion strategy to your data, compute, and task requirements"
            ]
          }
        },
        {
          "id": "multimodal-quiz",
          "title": "Multimodal AI Quiz",
          "type": "quiz",
          "duration": "15 min",
          "questions": [
            {
              "question": "What are the three fusion strategies for combining modalities in multimodal AI?",
              "options": [
                "Input, Output, Process fusion",
                "Early, Mid, and Late fusion",
                "Text, Image, Audio fusion",
                "Parallel, Sequential, Hybrid fusion"
              ],
              "correct": 1,
              "explanation": "IBM identifies three fusion strategies: Early fusion (combine before encoding), Mid fusion (combine at intermediate stages), and Late fusion (separate models combine outputs). Choice depends on task and data characteristics."
            },
            {
              "question": "According to multimodal AI research, what are the three core characteristics of multimodal data?",
              "options": [
                "Speed, Accuracy, Cost",
                "Heterogeneity, Connections, Interactions",
                "Training, Validation, Testing",
                "Input, Processing, Output"
              ],
              "correct": 1,
              "explanation": "Carnegie Mellon research (cited by IBM) identifies heterogeneity (diverse data qualities), connections (complementary information between modalities), and interactions (how modalities influence each other) as core characteristics."
            },
            {
              "question": "What is 'alignment' as a multimodal AI challenge?",
              "options": [
                "Making text left-justified",
                "Identifying connections between elements across modalities (temporal, spatial)",
                "Training models faster",
                "Compressing multimodal data"
              ],
              "correct": 1,
              "explanation": "Alignment is the challenge of identifying connections and correspondences across modalitiesâ€”for example, syncing audio to video (temporal alignment) or matching text descriptions to image regions (spatial alignment)."
            },
            {
              "question": "What does 'multimodal' mean in the context of AI?",
              "options": [
                "AI that runs on multiple computers",
                "AI that can process and understand multiple types of data like text, images, and audio",
                "AI that uses multiple programming languages",
                "AI that has multiple output formats"
              ],
              "correct": 1,
              "explanation": "Multimodal AI can process and reason across multiple types of data (modalities) such as text, images, audio, and video, enabling more comprehensive understanding."
            },
            {
              "question": "What is a Vision-Language Model (VLM)?",
              "options": [
                "A model that only processes images",
                "A model that can understand both images and text, enabling visual question answering",
                "A model that translates between languages",
                "A model for generating 3D graphics"
              ],
              "correct": 1,
              "explanation": "Vision-Language Models combine visual and language understanding, allowing them to answer questions about images, describe visuals, and reason across both modalities."
            },
            {
              "question": "What is OpenAI's Whisper model used for?",
              "options": [
                "Image generation",
                "Speech-to-text transcription",
                "Text summarization",
                "Code completion"
              ],
              "correct": 1,
              "explanation": "Whisper is OpenAI's speech-to-text model that can transcribe audio into text with high accuracy across multiple languages."
            },
            {
              "question": "What is CLIP used for in multimodal AI?",
              "options": [
                "Cutting and editing videos",
                "Connecting images and text for tasks like search and classification",
                "Compressing audio files",
                "Generating code from images"
              ],
              "correct": 1,
              "explanation": "CLIP (Contrastive Language-Image Pre-training) creates a shared embedding space for images and text, enabling text-based image search and zero-shot classification."
            },
            {
              "question": "How do you typically send an image to GPT-4V via the API?",
              "options": [
                "As a file upload only",
                "As a base64-encoded string or URL in the message content",
                "As a separate API call",
                "Images cannot be sent to GPT-4V"
              ],
              "correct": 1,
              "explanation": "GPT-4V accepts images either as base64-encoded data or as URLs, included in the message content alongside text in a multipart format."
            },
            {
              "question": "What is multimodal RAG?",
              "options": [
                "A type of image format",
                "RAG systems that can retrieve and reason over both text and images",
                "A music generation technique",
                "A video compression algorithm"
              ],
              "correct": 1,
              "explanation": "Multimodal RAG extends traditional RAG to include images alongside text, allowing systems to retrieve relevant visuals and use them in generating responses."
            },
            {
              "question": "What is the 'detail' parameter in GPT-4V's image_url?",
              "options": [
                "The image file format",
                "Controls resolution/quality level for image analysis (low, high, auto)",
                "The image compression ratio",
                "The image color depth"
              ],
              "correct": 1,
              "explanation": "The 'detail' parameter controls how much detail GPT-4V uses when analyzing images: 'low' is faster and cheaper, 'high' provides more detailed analysis."
            },
            {
              "question": "How can you analyze a video with current AI models that don't support native video?",
              "options": [
                "Videos cannot be analyzed by AI",
                "Extract frames at intervals and analyze them as images",
                "Convert video to audio only",
                "Compress the video to a single image"
              ],
              "correct": 1,
              "explanation": "For models without native video support, you can extract frames at regular intervals and send them as a series of images for analysis."
            },
            {
              "question": "What is Text-to-Speech (TTS) in multimodal AI?",
              "options": [
                "Converting images to sound",
                "Converting written text into spoken audio",
                "Translating speech between languages",
                "Compressing text files"
              ],
              "correct": 1,
              "explanation": "Text-to-Speech converts written text into natural-sounding spoken audio, enabling voice interfaces and audio content generation."
            },
            {
              "question": "Why is image description important in multimodal RAG systems?",
              "options": [
                "To make images load faster",
                "To enable text-based search over visual content",
                "To reduce storage costs",
                "To protect image copyrights"
              ],
              "correct": 1,
              "explanation": "Generating text descriptions of images allows them to be indexed and searched using text embeddings, enabling semantic retrieval of visual content."
            }
          ],
          "references": {
            "lessonRefs": [
              "what-is-multimodal",
              "understanding-vlms",
              "multimodal-rag"
            ],
            "externalRefs": [
              {
                "title": "OpenAI Vision Guide",
                "url": "https://platform.openai.com/docs/guides/images-vision"
              },
              {
                "title": "IBM Multimodal AI",
                "url": "https://www.ibm.com/topics/multimodal-ai"
              }
            ]
          }
        }
      ]
    }
  ]
}
