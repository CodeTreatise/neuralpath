{
  "id": "computer-vision",
  "title": "Computer Vision with PyTorch",
  "icon": "ðŸ‘ï¸",
  "description": "Build vision AI systems from image classification to object detection and beyond. Computer vision is a subfield of AI that equips machines with the ability to process, analyze and interpret visual inputs such as images and videos, using machine learning to derive meaningful information.",
  "level": "intermediate",
  "duration": "6 weeks",
  "totalLessons": 19,
  "validationSources": [
    "https://www.ibm.com/topics/computer-vision",
    "https://www.ibm.com/topics/convolutional-neural-networks"
  ],
  "keyThinkers": [
    {
      "name": "Yann LeCun",
      "contribution": "Convolutional Neural Networks (1989) - developed LeNet for handwritten digit recognition, establishing foundational CNN architecture with convolution, pooling, and backpropagation"
    },
    {
      "name": "Alex Krizhevsky",
      "contribution": "AlexNet (2012, with Sutskever & Hinton) - won ImageNet by large margin, catalyzed the deep learning revolution in computer vision with GPU training and ReLU activation"
    },
    {
      "name": "Kaiming He",
      "contribution": "ResNet (2015) - introduced skip connections enabling 152+ layer networks, also developed He initialization and Mask R-CNN for instance segmentation"
    },
    {
      "name": "Ross Girshick",
      "contribution": "R-CNN family (2014-2017) - pioneered region-based object detection with R-CNN, Fast R-CNN, and Faster R-CNN, establishing two-stage detection paradigm"
    },
    {
      "name": "Joseph Redmon",
      "contribution": "YOLO (2016) - You Only Look Once revolutionized real-time object detection with single-stage architecture achieving 45+ FPS"
    },
    {
      "name": "Olaf Ronneberger",
      "contribution": "U-Net (2015) - encoder-decoder architecture with skip connections for biomedical image segmentation, became standard for segmentation tasks"
    },
    {
      "name": "Alexey Dosovitskiy",
      "contribution": "Vision Transformer (2020, Google) - demonstrated pure transformer architecture can match/exceed CNNs for image classification when trained at scale"
    },
    {
      "name": "Ian Goodfellow",
      "contribution": "Generative Adversarial Networks (2014) - introduced adversarial training for image generation, spawning entire field of generative modeling"
    },
    {
      "name": "Alec Radford",
      "contribution": "CLIP (2021, OpenAI) - Contrastive Language-Image Pre-training enabled zero-shot image classification and revolutionized vision-language models"
    }
  ],
  "prerequisites": [
    "Python Fundamentals",
    "Deep Learning Basics"
  ],
  "outcomes": [
    "Build CNNs for image classification",
    "Implement object detection models",
    "Apply transfer learning effectively",
    "Deploy vision models in production",
    "Work with Vision Transformers",
    "Perform face recognition and pose estimation",
    "Deploy models on edge devices"
  ],
  "modules": [
    {
      "title": "CV Fundamentals",
      "description": "CV Fundamentals module",
      "lessons": [
        {
          "id": "intro-cv",
          "title": "Introduction to Computer Vision",
          "type": "lesson",
          "duration": "20 min",
          "content": {
            "overview": "Computer Vision is a subfield of AI that equips machines with the ability to process, analyze and interpret visual inputs. It can be pictured as the interaction between three broad processes: Recognition (identifying objects, people, places), Reconstruction (deriving 3D characteristics), and Reorganization (inferring relationships between entities).",
            "sections": [
              {
                "title": "What is Computer Vision?",
                "content": "Computer vision is a field of AI that trains computers to interpret and understand the visual world. Using digital images and deep learning models, machines can identify and classify objects, and react to what they 'see'.\n\n**The CV Pipeline:**\n1. Data Gathering - collect images/videos from cameras, sensors\n2. Preprocessing - cleaning, enhancement, augmentation\n3. Model Selection - CNNs, Vision Transformers (ViT)\n4. Model Training - feature extraction, backpropagation, optimization",
                "keyPoints": [
                  "Images are represented as numerical matrices",
                  "CNNs are the primary deep learning model for image processing",
                  "Vision Transformers (ViT) match or exceed CNN performance",
                  "Applications: autonomous driving, medical imaging, security, AR/VR"
                ]
              },
              {
                "title": "Image Fundamentals",
                "codeExample": {
                  "language": "python",
                  "code": "import numpy as np\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load image\nimg = cv2.imread('image.jpg')\nprint(f'Shape: {img.shape}')  # (height, width, channels)\nprint(f'Dtype: {img.dtype}')  # uint8 (0-255)\n\n# Color spaces\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nimg_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n\n# Display\nplt.figure(figsize=(12, 4))\nplt.subplot(131), plt.imshow(img_rgb), plt.title('RGB')\nplt.subplot(132), plt.imshow(img_gray, cmap='gray'), plt.title('Grayscale')\nplt.subplot(133), plt.imshow(img_hsv), plt.title('HSV')\nplt.show()"
                }
              },
              {
                "title": "Core CV Tasks",
                "content": "Computer vision encompasses several key tasks:\n\n**Understanding Tasks:**\n1. **Image Classification**: Categorize images into predefined classes\n2. **Object Detection**: Locate and classify objects with bounding boxes\n3. **Semantic Segmentation**: Classify every pixel by semantic class\n4. **Instance Segmentation**: Separate individual object instances\n5. **Panoptic Segmentation**: Combine semantic + instance segmentation\n\n**Analysis Tasks:**\n6. **Facial Recognition**: Identify individuals from facial features\n7. **Pose Estimation**: Detect body keypoints and gestures\n8. **Object Tracking**: Follow objects across video frames\n9. **OCR**: Extract text from images\n10. **Scene Understanding**: Infer relationships between objects\n\n**Generation Tasks:**\n11. **Image Generation**: Create images (GANs, Diffusion, VAEs)\n12. **Visual Inspection**: Automate defect detection",
                "diagram": {
                  "title": "Computer Vision Tasks Hierarchy",
                  "code": "flowchart TB\n    subgraph Understanding[\"Image Understanding\"]\n        IC[Image Classification]\n        OD[Object Detection]\n        SS[Semantic Segmentation]\n        IS[Instance Segmentation]\n    end\n    \n    subgraph Analysis[\"Advanced Analysis\"]\n        PE[Pose Estimation]\n        FR[Face Recognition]\n        OCR[Text Recognition]\n    end\n    \n    subgraph Generation[\"Image Generation\"]\n        GAN[GANs]\n        DM[Diffusion Models]\n        SR[Super Resolution]\n    end\n    \n    IC --> OD --> SS --> IS\n    Understanding --> Analysis\n    Understanding --> Generation"
                }
              }
            ],
            "keyTakeaways": [
              "Images are 3D tensors: (height, width, channels)",
              "Different color spaces serve different purposes",
              "CNNs are the backbone of modern computer vision",
              "CV tasks range from classification to generation"
            ]
          }
        }
      ]
    },
    {
      "title": "Image Classification",
      "description": "Image Classification module",
      "lessons": [
        {
          "id": "transfer-learning",
          "title": "Transfer Learning for Vision",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Transfer learning leverages pre-trained models to achieve high accuracy with less data and training time. It's the most practical approach for real-world CV applications.",
            "sections": [
              {
                "title": "Pre-trained Models",
                "content": "Models trained on ImageNet (1.2M images, 1000 classes) have learned rich visual features that transfer well to other tasks.",
                "diagram": {
                  "title": "Transfer Learning Strategies",
                  "code": "flowchart TD\n    subgraph FE[Feature Extraction]\n        I1[Input] --> B1[Frozen Base]\n        B1 --> H1[New Head]\n        H1 --> O1[Output]\n        style B1 fill:#e5e7eb,stroke:#9ca3af\n        style H1 fill:#93c5fd,stroke:#3b82f6\n    end\n    \n    subgraph FT[Fine-Tuning]\n        I2[Input] --> B2[Trainable Base]\n        B2 --> H2[New Head]\n        H2 --> O2[Output]\n        style B2 fill:#93c5fd,stroke:#3b82f6\n        style H2 fill:#93c5fd,stroke:#3b82f6\n    end"
                },
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nimport torchvision.models as models\n\n# Load pre-trained models\nresnet50 = models.resnet50(pretrained=True)\nefficientnet = models.efficientnet_b0(pretrained=True)\nvit = models.vit_b_16(pretrained=True)  # Vision Transformer\n\n# Check available models\nprint([m for m in dir(models) if not m.startswith('_')])"
                }
              },
              {
                "title": "Fine-tuning Strategy",
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Load pretrained ResNet\nmodel = models.resnet50(pretrained=True)\n\n# Freeze base layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace classifier head\nnum_classes = 5  # Your number of classes\nmodel.fc = nn.Sequential(\n    nn.Linear(model.fc.in_features, 512),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(512, num_classes)\n)\n\n# Only train the new head\noptimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n\n# Later: unfreeze and fine-tune all layers with lower LR\nfor param in model.parameters():\n    param.requires_grad = True\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
                }
              },
              {
                "title": "Data Augmentation",
                "codeExample": {
                  "language": "python",
                  "code": "from torchvision import transforms\n\n# Training augmentations\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                        [0.229, 0.224, 0.225])\n])\n\n# Validation (no augmentation)\nval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                        [0.229, 0.224, 0.225])\n])"
                }
              }
            ],
            "keyTakeaways": [
              "Pre-trained models provide powerful feature extractors",
              "Freeze base, train head first, then fine-tune all",
              "Data augmentation prevents overfitting",
              "Use ImageNet normalization for pre-trained models"
            ]
          }
        }
      ]
    },
    {
      "title": "Advanced Topics",
      "description": "Advanced Topics module",
      "lessons": [
        {
          "id": "segmentation",
          "title": "Image Segmentation",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "Segmentation assigns a class label to every pixel in an image. It's crucial for medical imaging, autonomous driving, and image editing.",
            "sections": [
              {
                "title": "Types of Segmentation",
                "content": "There are three main types:\n\n1. **Semantic Segmentation**: Classify each pixel (all cars = same class)\n2. **Instance Segmentation**: Separate individual objects (car1, car2)\n3. **Panoptic Segmentation**: Combine both approaches"
              },
              {
                "title": "U-Net Architecture",
                "diagram": {
                  "title": "U-Net Architecture",
                  "code": "flowchart TB\n    subgraph Encoder[Encoder (Contracting Path)]\n        E1[Conv Block 1] --> P1[MaxPool]\n        P1 --> E2[Conv Block 2]\n        E2 --> P2[MaxPool]\n        P2 --> E3[Conv Block 3]\n        E3 --> P3[MaxPool]\n        P3 --> E4[Conv Block 4]\n        E4 --> P4[MaxPool]\n    end\n    \n    subgraph Bridge[Bottleneck]\n        P4 --> B[Conv Block 5]\n    end\n    \n    subgraph Decoder[Decoder (Expansive Path)]\n        B --> U1[UpConv 1]\n        U1 --> D1[Conv Block 4]\n        D1 --> U2[UpConv 2]\n        U2 --> D2[Conv Block 3]\n        D2 --> U3[UpConv 3]\n        U3 --> D3[Conv Block 2]\n        D3 --> U4[UpConv 4]\n        U4 --> D4[Conv Block 1]\n    end\n    \n    D4 --> Out[1x1 Conv (Output)]\n    \n    E4 -.->|Skip Connection| D1\n    E3 -.->|Skip Connection| D2\n    E2 -.->|Skip Connection| D3\n    E1 -.->|Skip Connection| D4\n    \n    style Encoder fill:#dbeafe,stroke:#2563eb\n    style Bridge fill:#fef3c7,stroke:#d97706\n    style Decoder fill:#fce7f3,stroke:#db2777"
                },
                "codeExample": {
                  "language": "python",
                  "code": "import segmentation_models_pytorch as smp\n\n# U-Net with ResNet encoder\nmodel = smp.Unet(\n    encoder_name='resnet34',\n    encoder_weights='imagenet',\n    in_channels=3,\n    classes=21  # Number of segmentation classes\n)\n\n# Other architectures\nmodel = smp.DeepLabV3Plus(encoder_name='efficientnet-b4')\nmodel = smp.FPN(encoder_name='resnet50')\nmodel = smp.PSPNet(encoder_name='resnet101')"
                }
              },
              {
                "title": "SAM - Segment Anything",
                "codeExample": {
                  "language": "python",
                  "code": "from segment_anything import sam_model_registry, SamPredictor\nimport cv2\n\n# Load SAM model\nsam = sam_model_registry['vit_h'](checkpoint='sam_vit_h.pth')\npredictor = SamPredictor(sam)\n\n# Set image\nimage = cv2.imread('image.jpg')\npredictor.set_image(image)\n\n# Segment with point prompt\nmasks, scores, logits = predictor.predict(\n    point_coords=np.array([[500, 375]]),  # Click point\n    point_labels=np.array([1]),  # 1 = foreground\n    multimask_output=True\n)\n\n# Use best mask\nbest_mask = masks[np.argmax(scores)]"
                }
              }
            ],
            "keyTakeaways": [
              "U-Net's encoder-decoder with skip connections is foundational",
              "IoU (Intersection over Union) is the key metric",
              "SAM provides zero-shot segmentation capabilities",
              "Pre-trained encoders dramatically improve results"
            ]
          },
          "authorCredits": {
            "researchers": ["Ronneberger", "Fischer", "Brox", "Kirillov", "Girshick", "He", "DollÃ¡r"],
            "note": "U-Net was introduced by Olaf Ronneberger et al. (2015) for biomedical image segmentation with its encoder-decoder + skip connections architecture. Segment Anything (SAM) was developed by Alexander Kirillov et al. at Meta AI (2023), enabling zero-shot segmentation with prompt-based interaction."
          }
        },
        {
          "id": "pose-estimation",
          "title": "Pose Estimation",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Pose estimation detects human body keypoints (joints) in images. It's used in fitness apps, AR, motion capture, and action recognition.",
            "sections": [
              {
                "title": "Pose Estimation Approaches",
                "content": "Two main approaches:\n\n**Top-Down**: First detect people, then estimate pose for each\n- More accurate for individual poses\n- Slower with many people\n\n**Bottom-Up**: Detect all keypoints, then group into people\n- Faster with crowds\n- OpenPose, HigherHRNet"
              },
              {
                "title": "Using MediaPipe",
                "codeExample": {
                  "language": "python",
                  "code": "import mediapipe as mp\nimport cv2\n\n# Initialize pose detector\nmp_pose = mp.solutions.pose\nmp_draw = mp.solutions.drawing_utils\n\npose = mp_pose.Pose(\n    static_image_mode=False,\n    model_complexity=1,\n    min_detection_confidence=0.5\n)\n\n# Process video\ncap = cv2.VideoCapture(0)\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    results = pose.process(rgb)\n    \n    if results.pose_landmarks:\n        mp_draw.draw_landmarks(\n            frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n        \n        # Access specific landmarks\n        nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n        print(f'Nose: ({nose.x:.2f}, {nose.y:.2f})')\n    \n    cv2.imshow('Pose', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break"
                }
              },
              {
                "title": "YOLO Pose",
                "codeExample": {
                  "language": "python",
                  "code": "from ultralytics import YOLO\n\n# Load YOLOv8 pose model\nmodel = YOLO('yolov8n-pose.pt')  # nano pose model\n\n# Inference\nresults = model('image.jpg')\n\n# Process keypoints\nfor result in results:\n    keypoints = result.keypoints\n    \n    if keypoints is not None:\n        # Shape: (num_people, 17, 3) - x, y, confidence\n        kpts = keypoints.data[0]  # First person\n        \n        # COCO keypoint order:\n        # 0: nose, 1-2: eyes, 3-4: ears, 5-6: shoulders\n        # 7-8: elbows, 9-10: wrists, 11-12: hips\n        # 13-14: knees, 15-16: ankles\n        \n        left_wrist = kpts[9]\n        print(f'Left wrist: ({left_wrist[0]:.0f}, {left_wrist[1]:.0f})')"
                }
              }
            ],
            "keyTakeaways": [
              "MediaPipe is great for real-time single-person pose",
              "YOLO Pose handles multi-person efficiently",
              "17 keypoints in COCO format is standard",
              "PCK and OKS are pose estimation metrics"
            ]
          }
        }
      ]
    },
    {
      "title": "Deployment",
      "description": "Deployment module",
      "lessons": [
        {
          "id": "edge-deployment",
          "title": "Edge & Mobile Deployment",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Deploy CV models on edge devices like mobile phones, Raspberry Pi, and NVIDIA Jetson for real-time inference.",
            "sections": [
              {
                "title": "Mobile-Optimized Architectures",
                "codeExample": {
                  "language": "python",
                  "code": "import torchvision.models as models\n\n# Mobile-optimized models\nmobilenet = models.mobilenet_v3_small(pretrained=True)  # 2.5M params\nshufflenet = models.shufflenet_v2_x1_0(pretrained=True)  # 2.3M params\nefficientnet_lite = models.efficientnet_b0(pretrained=True)  # 5.3M params\n\n# Compare model size and speed\nimport time\n\nmodels_to_test = [\n    ('MobileNetV3-Small', models.mobilenet_v3_small(pretrained=True)),\n    ('EfficientNet-B0', models.efficientnet_b0(pretrained=True)),\n    ('ResNet-18', models.resnet18(pretrained=True)),\n]\n\nx = torch.randn(1, 3, 224, 224)\nfor name, model in models_to_test:\n    model.eval()\n    params = sum(p.numel() for p in model.parameters()) / 1e6\n    start = time.time()\n    for _ in range(100):\n        with torch.no_grad():\n            model(x)\n    fps = 100 / (time.time() - start)\n    print(f'{name}: {params:.1f}M params, {fps:.0f} FPS')"
                }
              },
              {
                "title": "TensorFlow Lite for Mobile",
                "codeExample": {
                  "language": "python",
                  "code": "import tensorflow as tf\n\n# Convert to TFLite\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model')\n\n# Optimize for mobile\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16]\n\ntflite_model = converter.convert()\nwith open('model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\n# Run inference\ninterpreter = tf.lite.Interpreter(model_path='model.tflite')\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\noutput = interpreter.get_tensor(output_details[0]['index'])"
                }
              },
              {
                "title": "NVIDIA Jetson Deployment",
                "codeExample": {
                  "language": "python",
                  "code": "# On Jetson - use jetson-inference for optimized models\nimport jetson.inference\nimport jetson.utils\n\n# Load optimized image classification\nnet = jetson.inference.imageNet('googlenet')  # Pre-optimized for Jetson\n\n# Camera input\ncamera = jetson.utils.videoSource('/dev/video0')\ndisplay = jetson.utils.videoOutput('display://0')\n\nwhile True:\n    img = camera.Capture()\n    class_id, confidence = net.Classify(img)\n    class_label = net.GetClassDesc(class_id)\n    \n    jetson.utils.cudaDrawText(img, f'{class_label}: {confidence:.1%}', 5, 5)\n    display.Render(img)\n\n# Object detection\nnet = jetson.inference.detectNet('ssd-mobilenet-v2', threshold=0.5)\ndetections = net.Detect(img)"
                }
              }
            ],
            "keyTakeaways": [
              "MobileNet and ShuffleNet are designed for edge devices",
              "TFLite is the standard for Android/iOS deployment",
              "Jetson provides GPU acceleration for edge AI",
              "Benchmark on target device, not just desktop"
            ]
          }
        }
      ]
    },
    {
      "title": "Additional Topics",
      "description": "Additional course content",
      "lessons": [
        {
          "id": "cnn-architecture",
          "title": "Convolutional Neural Networks",
          "type": "lesson",
          "duration": "35 min",
          "content": {
            "overview": "CNNs are specialized neural networks designed to process grid-like data such as images. They use three main types of layers: convolutional layers (feature extraction), pooling layers (dimensionality reduction), and fully-connected layers (classification). CNNs automatically learn spatial hierarchies of features through the convolution operation.",
            "sections": [
              {
                "title": "Convolution Operation",
                "content": "A convolution slides a filter (kernel) across the image. The filter is a 2D array of weights that detects specific patterns. A dot product is calculated between input pixels and filter weights, creating a feature map (activation map). The filter 'convolves' across the entire image to extract features like edges, textures, and shapes.\n\n**Hyperparameters:**\n- **Number of filters**: Affects output depth\n- **Stride**: Distance the kernel moves (larger = smaller output)\n- **Padding**: Zero-padding preserves spatial dimensions",
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nimport torch.nn as nn\n\n# 2D Convolution layer\nconv = nn.Conv2d(\n    in_channels=3,      # RGB input\n    out_channels=32,    # 32 filters\n    kernel_size=3,      # 3x3 kernel\n    stride=1,           # Step size\n    padding=1           # Preserve spatial size\n)\n\n# Input: (batch, channels, height, width)\nx = torch.randn(1, 3, 224, 224)\nout = conv(x)\nprint(f'Output shape: {out.shape}')  # [1, 32, 224, 224]"
                }
              },
              {
                "title": "Building a CNN",
                "codeExample": {
                  "language": "python",
                  "code": "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        \n        # Pooling\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # Fully connected\n        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, x):\n        # Conv blocks\n        x = self.pool(F.relu(self.conv1(x)))  # 224->112\n        x = self.pool(F.relu(self.conv2(x)))  # 112->56\n        x = self.pool(F.relu(self.conv3(x)))  # 56->28\n        \n        # Flatten and classify\n        x = x.view(x.size(0), -1)\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.fc2(x)\n        return x"
                }
              },
              {
                "title": "Key CNN Components",
                "content": "Modern CNNs combine several key building blocks:\n\n1. **Convolution**: Extract spatial features\n2. **Activation (ReLU)**: Add non-linearity\n3. **Pooling**: Downsample and add translation invariance\n4. **Batch Normalization**: Stabilize training\n5. **Dropout**: Prevent overfitting\n6. **Skip Connections**: Enable deeper networks (ResNet)",
                "diagram": {
                  "title": "Standard CNN Architecture",
                  "code": "flowchart LR\n    Input[Input Image] --> Conv1[Conv + ReLU]\n    Conv1 --> Pool1[MaxPool]\n    Pool1 --> Conv2[Conv + ReLU]\n    Conv2 --> Pool2[MaxPool]\n    Pool2 --> Flat[Flatten]\n    Flat --> FC1[Dense Layer]\n    FC1 --> FC2[Output Layer]\n    \n    style Input fill:#fff,stroke:#333\n    style Conv1 fill:#dbeafe,stroke:#2563eb\n    style Pool1 fill:#fce7f3,stroke:#db2777\n    style Conv2 fill:#dbeafe,stroke:#2563eb\n    style Pool2 fill:#fce7f3,stroke:#db2777\n    style FC1 fill:#dcfce7,stroke:#16a34a"
                }
              }
            ],
            "keyTakeaways": [
              "Convolutions learn local patterns hierarchically",
              "Pooling reduces spatial dimensions and adds invariance",
              "Modern architectures use skip connections for depth",
              "Batch normalization is essential for training stability"
            ]
          },
          "authorCredits": {
            "researchers": ["LeCun", "Bottou", "Bengio", "Haffner", "Krizhevsky", "Sutskever", "Hinton"],
            "note": "Convolutional Neural Networks were pioneered by Yann LeCun et al. (1998) with LeNet for digit recognition. AlexNet by Krizhevsky, Sutskever, and Hinton (2012) demonstrated CNNs could dramatically outperform traditional methods on ImageNet, launching the deep learning era in computer vision."
          }
        },
        {
          "id": "object-detection",
          "title": "Object Detection",
          "type": "lesson",
          "duration": "35 min",
          "content": {
            "overview": "Object detection identifies and localizes multiple objects in an image, returning bounding boxes and class labels. It's essential for autonomous driving, surveillance, and more.",
            "sections": [
              {
                "title": "Detection Approaches",
                "content": "Object detection has evolved through several paradigms:\n\n1. **Two-stage detectors**: R-CNN â†’ Fast R-CNN â†’ Faster R-CNN\n   - First propose regions, then classify\n   - More accurate, slower\n\n2. **Single-stage detectors**: YOLO, SSD, RetinaNet\n   - Directly predict boxes and classes\n   - Faster, good for real-time\n\n3. **Transformer-based**: DETR, DINO\n   - End-to-end with attention\n   - State-of-the-art accuracy"
              },
              {
                "title": "YOLO with Ultralytics",
                "codeExample": {
                  "language": "python",
                  "code": "from ultralytics import YOLO\n\n# Load pre-trained YOLOv8\nmodel = YOLO('yolov8n.pt')  # nano, small, medium, large, xlarge\n\n# Inference\nresults = model('image.jpg')\n\n# Process results\nfor result in results:\n    boxes = result.boxes  # Bounding boxes\n    for box in boxes:\n        x1, y1, x2, y2 = box.xyxy[0].tolist()\n        confidence = box.conf[0].item()\n        class_id = int(box.cls[0].item())\n        class_name = model.names[class_id]\n        print(f'{class_name}: {confidence:.2f} at [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]')\n\n# Save annotated image\nresults[0].save('output.jpg')"
                }
              },
              {
                "title": "Training Custom Detector",
                "codeExample": {
                  "language": "python",
                  "code": "from ultralytics import YOLO\n\n# Load base model\nmodel = YOLO('yolov8n.pt')\n\n# Train on custom dataset\n# Dataset format: images/ and labels/ folders\n# YOLO format: class_id x_center y_center width height (normalized)\nresults = model.train(\n    data='dataset.yaml',  # Dataset config\n    epochs=100,\n    imgsz=640,\n    batch=16,\n    patience=20,  # Early stopping\n    device='cuda'\n)\n\n# Validate\nmetrics = model.val()\nprint(f'mAP50: {metrics.box.map50:.3f}')\nprint(f'mAP50-95: {metrics.box.map:.3f}')\n\n# Export for deployment\nmodel.export(format='onnx')"
                }
              }
            ],
            "keyTakeaways": [
              "YOLO is the go-to for real-time detection",
              "mAP (mean Average Precision) is the key metric",
              "Anchor-free models are simpler and often better",
              "Always start with pre-trained weights"
            ]
          },
          "authorCredits": {
            "researchers": ["Girshick", "Donahue", "Darrell", "Malik", "Redmon", "Divvala", "Farhadi"],
            "note": "Region-based detection was pioneered by Ross Girshick et al. with R-CNN (2014), Fast R-CNN, and Faster R-CNN. YOLO (You Only Look Once) was introduced by Joseph Redmon et al. (2016), achieving real-time detection by treating it as a single regression problem."
          }
        },
        {
          "id": "image-preprocessing",
          "title": "Image Preprocessing & Augmentation",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Proper image preprocessing and augmentation are essential for training robust computer vision models. Learn techniques to prepare data and artificially expand your dataset.",
            "sections": [
              {
                "title": "Image Preprocessing",
                "codeExample": {
                  "language": "python",
                  "code": "import cv2\nimport numpy as np\nfrom PIL import Image\n\n# Resize with aspect ratio preservation\ndef resize_with_padding(img, target_size):\n    h, w = img.shape[:2]\n    scale = min(target_size[0]/h, target_size[1]/w)\n    new_h, new_w = int(h*scale), int(w*scale)\n    resized = cv2.resize(img, (new_w, new_h))\n    \n    # Add padding\n    pad_h = (target_size[0] - new_h) // 2\n    pad_w = (target_size[1] - new_w) // 2\n    padded = cv2.copyMakeBorder(resized, pad_h, pad_h, pad_w, pad_w,\n                                 cv2.BORDER_CONSTANT, value=(0,0,0))\n    return padded\n\n# Histogram equalization (improve contrast)\nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nequalized = cv2.equalizeHist(img_gray)\n\n# CLAHE (adaptive histogram equalization)\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\nclahe_img = clahe.apply(img_gray)"
                }
              },
              {
                "title": "Advanced Augmentation with Albumentations",
                "codeExample": {
                  "language": "python",
                  "code": "import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Comprehensive augmentation pipeline\ntrain_transform = A.Compose([\n    A.RandomResizedCrop(224, 224, scale=(0.8, 1.0)),\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.5),\n    A.OneOf([\n        A.GaussNoise(var_limit=(10, 50)),\n        A.GaussianBlur(blur_limit=3),\n        A.MotionBlur(blur_limit=3),\n    ], p=0.3),\n    A.OneOf([\n        A.RandomBrightnessContrast(p=0.5),\n        A.ColorJitter(p=0.5),\n        A.HueSaturationValue(p=0.5),\n    ], p=0.5),\n    A.CoarseDropout(max_holes=8, max_height=20, max_width=20, p=0.3),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\n# Apply\naugmented = train_transform(image=image)['image']"
                }
              },
              {
                "title": "Augmentation for Detection",
                "codeExample": {
                  "language": "python",
                  "code": "import albumentations as A\n\n# Detection augmentations (must transform bboxes too)\ndetection_transform = A.Compose([\n    A.RandomResizedCrop(640, 640, scale=(0.5, 1.0)),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.Normalize(),\n], bbox_params=A.BboxParams(\n    format='yolo',  # or 'pascal_voc', 'coco'\n    label_fields=['class_labels'],\n    min_visibility=0.3  # Drop boxes that become too small\n))\n\n# Apply with bboxes\nresult = detection_transform(\n    image=image,\n    bboxes=[[0.5, 0.5, 0.2, 0.3]],  # YOLO format\n    class_labels=[0]\n)"
                }
              }
            ],
            "keyTakeaways": [
              "Always preserve aspect ratio when resizing",
              "Albumentations is faster than torchvision transforms",
              "Use OneOf to apply random choice of augmentations",
              "Detection augmentations must transform bboxes together"
            ]
          }
        },
        {
          "id": "modern-architectures",
          "title": "Modern CNN Architectures",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "Explore the evolution of CNN architectures from AlexNet to modern efficient networks like EfficientNet and ConvNeXt.",
            "sections": [
              {
                "title": "Architecture Timeline",
                "content": "Key milestones in CNN development:\n\n- **AlexNet (2012)**: Deep CNNs work! 8 layers, ReLU, Dropout\n- **VGG (2014)**: Deeper is better. 16-19 layers, 3x3 convs\n- **GoogLeNet/Inception (2014)**: Multi-scale features with inception modules\n- **ResNet (2015)**: Skip connections enable 152+ layers\n- **DenseNet (2017)**: Connect every layer to every other layer\n- **EfficientNet (2019)**: Optimal scaling of depth/width/resolution\n- **ConvNeXt (2022)**: Modernized ResNet competitive with Transformers"
              },
              {
                "title": "ResNet and Skip Connections",
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n    \n    def forward(self, x):\n        residual = x  # Save input\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual  # Skip connection\n        return F.relu(out)\n\n# Why skip connections work:\n# - Gradient flows directly through identity mapping\n# - Easier to learn residual F(x) than full mapping H(x)\n# - Enables training of very deep networks (100+ layers)"
                }
              },
              {
                "title": "EfficientNet Scaling",
                "codeExample": {
                  "language": "python",
                  "code": "import torchvision.models as models\n\n# EfficientNet family (B0 to B7)\n# Each version scales depth, width, and resolution together\nmodel = models.efficientnet_b0(pretrained=True)  # 5.3M params, 224px\nmodel = models.efficientnet_b4(pretrained=True)  # 19M params, 380px\nmodel = models.efficientnet_v2_m(pretrained=True)  # Faster training\n\n# ConvNeXt - modernized ResNet\nmodel = models.convnext_tiny(pretrained=True)  # Competitive with ViT\nmodel = models.convnext_base(pretrained=True)\n\n# Choosing architecture:\n# - Mobile/Edge: MobileNetV3, EfficientNet-B0\n# - Server: EfficientNet-B4+, ConvNeXt, ViT\n# - Accuracy-first: ConvNeXt-L, ViT-L, Swin-L"
                }
              }
            ],
            "keyTakeaways": [
              "ResNet's skip connections solved the degradation problem",
              "EfficientNet scales depth, width, and resolution together",
              "ConvNeXt shows CNNs can match Transformers",
              "Choose architecture based on compute budget"
            ]
          }
        },
        {
          "id": "3d-vision",
          "title": "3D Vision & Depth Estimation",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "3D computer vision derives three-dimensional characteristics from 2D images. This includes depth estimation, 3D reconstruction, point cloud processing, and spatial understanding - essential for robotics, AR/VR, and autonomous systems.",
            "sections": [
              {
                "title": "Depth Estimation",
                "content": "Depth estimation predicts distance from camera to each pixel:\n\n**Approaches:**\n1. **Monocular**: Single image â†’ depth map (learned)\n2. **Stereo**: Two cameras â†’ depth from disparity\n3. **Multi-view**: Multiple images â†’ depth via triangulation\n4. **Sensor-based**: LiDAR, structured light, ToF sensors\n\n**Applications:**\n- Autonomous vehicles (obstacle distance)\n- AR/VR (occlusion, scene placement)\n- Robotics (manipulation, navigation)\n- 3D photography (portrait mode)",
                "codeExample": {
                  "language": "python",
                  "code": "from transformers import DPTForDepthEstimation, DPTImageProcessor\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load DPT (Dense Prediction Transformer) for depth\nprocessor = DPTImageProcessor.from_pretrained('Intel/dpt-large')\nmodel = DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\n\n# Estimate depth from single image\nimage = Image.open('scene.jpg')\ninputs = processor(images=image, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    depth = outputs.predicted_depth\n\n# Normalize for visualization\ndepth_np = depth.squeeze().numpy()\ndepth_normalized = (depth_np - depth_np.min()) / (depth_np.max() - depth_np.min())\n\nplt.figure(figsize=(12, 5))\nplt.subplot(121), plt.imshow(image), plt.title('RGB')\nplt.subplot(122), plt.imshow(depth_normalized, cmap='plasma'), plt.title('Depth')\nplt.colorbar()\nplt.show()\n\n# Metric depth estimation (actual distances in meters)\nfrom transformers import pipeline\ndepth_estimator = pipeline('depth-estimation', model='depth-anything/Depth-Anything-V2-Large')\nresult = depth_estimator(image)"
                }
              },
              {
                "title": "Point Cloud Processing",
                "content": "Point clouds are 3D representations from depth sensors:\n\n**Format**: Each point has (x, y, z) coordinates + optional attributes (color, normal)\n\n**Tasks:**\n- Classification (what object is this point cloud?)\n- Segmentation (label each point)\n- Detection (find objects in scene)\n- Registration (align multiple scans)",
                "codeExample": {
                  "language": "python",
                  "code": "import open3d as o3d\nimport numpy as np\n\n# Load point cloud\npcd = o3d.io.read_point_cloud('scene.ply')\nprint(f'Points: {len(pcd.points)}')\n\n# Visualize\no3d.visualization.draw_geometries([pcd])\n\n# Preprocessing\npcd_down = pcd.voxel_down_sample(voxel_size=0.02)  # Downsample\npcd_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(\n    radius=0.1, max_nn=30\n))\n\n# Plane segmentation (find floor)\nplane_model, inliers = pcd_down.segment_plane(\n    distance_threshold=0.01,\n    ransac_n=3,\n    num_iterations=1000\n)\nfloor = pcd_down.select_by_index(inliers)\nobjects = pcd_down.select_by_index(inliers, invert=True)\n\n# Clustering objects\nlabels = np.array(objects.cluster_dbscan(eps=0.05, min_points=10))\nprint(f'Found {labels.max() + 1} objects')\n\n# Color by cluster\ncolors = plt.get_cmap('tab20')(labels / (labels.max() + 1))[:, :3]\nobjects.colors = o3d.utility.Vector3dVector(colors)"
                }
              },
              {
                "title": "3D Object Detection",
                "content": "Detect and localize objects in 3D space:\n\n**Input types:**\n- Point clouds from LiDAR\n- RGB-D images (RGB + Depth)\n- Multi-view images\n\n**Popular architectures:**\n- PointNet/PointNet++ (direct point processing)\n- VoxelNet (voxel-based)\n- SECOND, CenterPoint (pillar-based)\n- BEVFormer (bird's eye view)",
                "codeExample": {
                  "language": "python",
                  "code": "# PointNet++ for 3D classification (PyTorch Geometric)\nimport torch\nimport torch.nn as nn\nfrom torch_geometric.nn import PointNetConv, global_max_pool\n\nclass PointNetPlusPlus(nn.Module):\n    \"\"\"Simplified PointNet++ for classification.\"\"\"\n    def __init__(self, num_classes):\n        super().__init__()\n        self.sa1 = PointNetConv(local_nn=nn.Sequential(\n            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 128)\n        ))\n        self.sa2 = PointNetConv(local_nn=nn.Sequential(\n            nn.Linear(128 + 3, 256), nn.ReLU(), nn.Linear(256, 512)\n        ))\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.5),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, pos, batch):\n        # Hierarchical point set learning\n        x = self.sa1(None, pos, batch)\n        x = self.sa2(x, pos, batch)\n        x = global_max_pool(x, batch)  # Global feature\n        return self.classifier(x)\n\n# For autonomous driving: use MMDetection3D\n# from mmdet3d.apis import inference_detector, init_model\n# model = init_model(config, checkpoint)\n# result = inference_detector(model, pcd_file)"
                }
              },
              {
                "title": "Neural Radiance Fields (NeRF)",
                "content": "NeRF represents 3D scenes as neural networks:\n\n**How it works:**\n1. Input: 3D position (x, y, z) + viewing direction\n2. Output: Color (RGB) + density\n3. Volume rendering creates 2D views\n4. Train by comparing to actual photos\n\n**Applications:**\n- Novel view synthesis\n- 3D asset creation\n- Virtual tours\n- VFX and filmmaking\n\n**Variants:** Instant-NGP (fast), 3D Gaussian Splatting (real-time)",
                "codeExample": {
                  "language": "python",
                  "code": "# Using nerfstudio for NeRF training\n# pip install nerfstudio\n\n# Process images\n# ns-process-data images --data ./images --output-dir ./data\n\n# Train NeRF\n# ns-train nerfacto --data ./data\n\n# Export mesh\n# ns-export mesh --load-config outputs/data/nerfacto/config.yml\n\n# For quick experimentation: instant-ngp\nimport pyngp as ngp\n\n# Load trained NeRF\nnerf = ngp.Testbed()\nnerf.load_snapshot('scene.msgpack')\n\n# Render from new viewpoint\ncamera_matrix = np.array([...])  # 4x4 camera pose\nimage = nerf.render(camera_matrix, width=800, height=600)\n\n# 3D Gaussian Splatting - even faster\n# Real-time rendering of NeRF-quality scenes\n# Train: python train.py -s ./data -m ./output\n# Render: python render.py -m ./output --camera_path orbit"
                }
              },
              {
                "title": "Depth in Vision-Language Models",
                "content": "Modern VLMs understand 3D spatial relationships:\n\n**Capabilities:**\n- Describe relative positions ('behind', 'on top of')\n- Estimate rough distances\n- Reason about scene geometry\n- Grounding in 3D space",
                "codeExample": {
                  "language": "python",
                  "code": "# Using LLaVA for spatial understanding\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\nfrom PIL import Image\n\nprocessor = LlavaNextProcessor.from_pretrained('llava-hf/llava-v1.6-mistral-7b-hf')\nmodel = LlavaNextForConditionalGeneration.from_pretrained(\n    'llava-hf/llava-v1.6-mistral-7b-hf', torch_dtype=torch.float16\n).to('cuda')\n\nimage = Image.open('room.jpg')\nprompt = 'USER: <image>\\nDescribe the spatial arrangement of objects in this room. Which objects are closest to the camera?\\nASSISTANT:'\n\ninputs = processor(images=image, text=prompt, return_tensors='pt').to('cuda')\noutput = model.generate(**inputs, max_new_tokens=200)\nresponse = processor.decode(output[0], skip_special_tokens=True)\nprint(response)\n\n# For precise 3D grounding, use specialized models:\n# - ScanRefer (language to 3D point)\n# - 3D-LLM (language grounded in 3D)\n# - LEO (language-embodied AI)"
                }
              }
            ],
            "keyTakeaways": [
              "Monocular depth estimation works from single images using learned priors",
              "Point clouds are processed with PointNet or voxel-based methods",
              "NeRF and Gaussian Splatting enable photorealistic novel view synthesis",
              "3D understanding is crucial for robotics, AR/VR, and autonomous systems"
            ]
          }
        },
        {
          "id": "vision-transformers",
          "title": "Vision Transformers (ViT)",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "Vision Transformers apply the Transformer architecture to images, achieving state-of-the-art results by treating images as sequences of patches.",
            "sections": [
              {
                "title": "How ViT Works",
                "content": "Vision Transformer (ViT) treats an image as a sequence of patches:\n\n1. **Patch Embedding**: Split image into 16x16 patches â†’ linear projection\n2. **Position Embedding**: Add learnable position information\n3. **Transformer Encoder**: Self-attention across all patches\n4. **Classification**: CLS token through MLP head\n\nA 224x224 image â†’ 196 patches of 16x16 â†’ sequence like text!"
              },
              {
                "title": "Using Pre-trained ViT",
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nimport torchvision.models as models\nfrom transformers import ViTForImageClassification, ViTImageProcessor\n\n# PyTorch Vision\nvit = models.vit_b_16(pretrained=True)\n\n# Hugging Face (more models)\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\n# Inference\nfrom PIL import Image\nimage = Image.open('image.jpg')\ninputs = processor(images=image, return_tensors='pt')\noutputs = model(**inputs)\npredicted_class = outputs.logits.argmax(-1).item()"
                }
              },
              {
                "title": "ViT Variants",
                "codeExample": {
                  "language": "python",
                  "code": "from transformers import AutoModel\n\n# Different ViT architectures\n# ViT - Original Vision Transformer\nmodel = AutoModel.from_pretrained('google/vit-base-patch16-224')\n\n# DeiT - Data-efficient Image Transformer (trains with less data)\nmodel = AutoModel.from_pretrained('facebook/deit-base-patch16-224')\n\n# Swin Transformer - Shifted windows for efficiency\nmodel = AutoModel.from_pretrained('microsoft/swin-base-patch4-window7-224')\n\n# BEiT - BERT-style pre-training for vision\nmodel = AutoModel.from_pretrained('microsoft/beit-base-patch16-224')\n\n# CLIP - Vision-language model\nfrom transformers import CLIPModel\nclip = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')"
                }
              }
            ],
            "keyTakeaways": [
              "ViT treats images as sequences of patches",
              "Self-attention captures global relationships",
              "Requires more data than CNNs to train from scratch",
              "Swin Transformer adds efficiency with shifted windows"
            ]
          },
          "authorCredits": {
            "researchers": ["Dosovitskiy", "Beyer", "Kolesnikov", "Weissenborn", "Liu", "Lin", "Radford"],
            "note": "Vision Transformer (ViT) was introduced by Alexey Dosovitskiy et al. at Google (2020), showing pure transformers match/exceed CNNs at scale. Swin Transformer by Ze Liu et al. at Microsoft (2021) added hierarchical shifted windows for efficiency. CLIP by Alec Radford et al. at OpenAI (2021) enabled zero-shot classification via contrastive vision-language pretraining."
          }
        },
        {
          "id": "self-supervised-learning",
          "title": "Self-Supervised & Contrastive Learning",
          "type": "lesson",
          "duration": "35 min",
          "content": {
            "overview": "Self-supervised learning trains vision models on unlabeled data by creating artificial supervision signals. Contrastive methods like SimCLR and CLIP have revolutionized how we train visual representations, enabling zero-shot classification and powerful transfer learning without manual labeling.",
            "sections": [
              {
                "title": "Why Self-Supervised Learning?",
                "content": "**The Problem with Supervised Learning:**\n- ImageNet: 14 million images, hand-labeled over years\n- Medical/specialized domains: expensive expert labels\n- Labels are biased toward predefined categories\n\n**Self-Supervised Solution:**\n- Learn from structure in the data itself\n- No manual labels needed\n- Often produces more generalizable representations\n- CLIP was trained on 400M image-text pairs from the internet\n\n**Key Paradigms:**\n1. **Contrastive Learning**: Learn by comparing similar/dissimilar pairs\n2. **Masked Image Modeling**: Predict masked patches (like BERT for images)\n3. **Distillation**: Learn from teacher network's outputs",
                "keyPoints": [
                  "Self-supervised models often outperform supervised ones on transfer tasks",
                  "Removes the labeling bottleneck for new domains",
                  "Foundation models (CLIP, DINO) are pre-trained this way"
                ]
              },
              {
                "title": "Contrastive Learning: SimCLR",
                "content": "SimCLR learns representations by maximizing agreement between augmented views of the same image:\n\n1. Take an image, create two random augmentations\n2. Encode both through the same network\n3. These are 'positive' pairs (should be similar)\n4. All other images in batch are 'negative' pairs\n5. Contrastive loss pushes positives together, negatives apart\n\n**NT-Xent Loss (Normalized Temperature-scaled Cross Entropy):**\n$$\\mathcal{L} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k)/\\tau)}$$",
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nimport torch.nn as nn\nimport torchvision.transforms as T\n\n# SimCLR augmentation pipeline (key to success)\nsimclr_augment = T.Compose([\n    T.RandomResizedCrop(224, scale=(0.2, 1.0)),\n    T.RandomHorizontalFlip(),\n    T.RandomApply([T.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n    T.RandomGrayscale(p=0.2),\n    T.RandomApply([T.GaussianBlur(kernel_size=23)], p=0.5),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Create two views of each image\ndef get_views(image):\n    return simclr_augment(image), simclr_augment(image)\n\n# Contrastive loss\ndef nt_xent_loss(z_i, z_j, temperature=0.5):\n    batch_size = z_i.shape[0]\n    z = torch.cat([z_i, z_j], dim=0)  # (2N, D)\n    \n    # Cosine similarity matrix\n    sim = torch.mm(z, z.T) / temperature  # (2N, 2N)\n    \n    # Mask out self-similarity\n    mask = torch.eye(2 * batch_size, device=z.device).bool()\n    sim.masked_fill_(mask, float('-inf'))\n    \n    # Labels: positive pairs are at positions [i, i+N] and [i+N, i]\n    labels = torch.cat([torch.arange(batch_size, 2*batch_size),\n                        torch.arange(batch_size)], dim=0).to(z.device)\n    \n    return nn.CrossEntropyLoss()(sim, labels)"
                }
              },
              {
                "title": "CLIP: Vision-Language Contrastive Learning",
                "content": "CLIP learns visual representations from natural language supervision:\n\n**Training:**\n- 400M image-text pairs from the internet\n- Contrastive loss between image and text embeddings\n- Image encoder: ResNet or ViT\n- Text encoder: Transformer\n\n**Zero-Shot Classification:**\n- No fine-tuning needed for new classes\n- Create text prompts: 'a photo of a [class]'\n- Find which text has highest similarity to image\n\n**Why CLIP matters:**\n- First truly general-purpose visual model\n- Works on any visual concept describable in language\n- Foundation for Stable Diffusion, LLaVA, etc.",
                "codeExample": {
                  "language": "python",
                  "code": "from transformers import CLIPProcessor, CLIPModel\nimport torch\nfrom PIL import Image\n\n# Load CLIP\nmodel = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\nprocessor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n\n# Zero-shot classification\nimage = Image.open('mystery_animal.jpg')\nclass_names = ['dog', 'cat', 'bird', 'fish', 'elephant']\n\n# Create text prompts\ntexts = [f'a photo of a {c}' for c in class_names]\n\ninputs = processor(\n    text=texts,\n    images=image,\n    return_tensors='pt',\n    padding=True\n)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits_per_image  # (1, num_classes)\n    probs = logits.softmax(dim=1)\n\nfor name, prob in zip(class_names, probs[0]):\n    print(f'{name}: {prob:.2%}')\n\n# CLIP for image search\nimage_embeddings = model.get_image_features(**processor(images=images, return_tensors='pt'))\ntext_embedding = model.get_text_features(**processor(text=['sunset over mountains'], return_tensors='pt'))\nsimilarities = (text_embedding @ image_embeddings.T).squeeze()"
                }
              },
              {
                "title": "DINO: Self-Distillation with No Labels",
                "content": "DINO learns by matching student predictions to teacher predictions:\n\n**Key Ideas:**\n- Student and teacher see different augmented views\n- Teacher is exponential moving average of student\n- No negative pairs needed (unlike SimCLR)\n- Learns semantic segmentation without labels!\n\n**DINOv2** provides state-of-the-art visual features for many downstream tasks.",
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Load DINOv2\nprocessor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\nmodel = AutoModel.from_pretrained('facebook/dinov2-base')\n\n# Extract features\nimage = Image.open('image.jpg')\ninputs = processor(images=image, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    cls_token = outputs.last_hidden_state[:, 0]  # Global feature\n    patch_tokens = outputs.last_hidden_state[:, 1:]  # Local features\n\n# Use for downstream tasks\nprint(f'CLS embedding: {cls_token.shape}')  # (1, 768)\nprint(f'Patch features: {patch_tokens.shape}')  # (1, 196, 768)\n\n# DINO discovers semantic segmentation without labels!\n# Visualize attention heads to see object parts\nattention = model.get_last_self_attention(inputs['pixel_values'])\n# Different heads attend to different semantic regions"
                }
              },
              {
                "title": "Masked Image Modeling",
                "content": "Inspired by BERT, mask patches and predict them:\n\n**MAE (Masked Autoencoder):**\n- Mask 75% of patches (aggressive!)\n- Encoder only processes visible patches\n- Decoder reconstructs masked patches\n- Very efficient training\n\n**BEiT:**\n- Tokenize images with discrete VAE\n- Predict token IDs of masked patches\n- More like original BERT",
                "codeExample": {
                  "language": "python",
                  "code": "from transformers import ViTMAEForPreTraining, ViTMAEImageProcessor\nimport torch\n\n# Load MAE\nprocessor = ViTMAEImageProcessor.from_pretrained('facebook/vit-mae-base')\nmodel = ViTMAEForPreTraining.from_pretrained('facebook/vit-mae-base')\n\n# Pre-training step (reconstruction)\nimage = Image.open('image.jpg')\ninputs = processor(images=image, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    loss = outputs.loss  # Reconstruction loss\n    reconstructed = outputs.logits  # Pixel predictions for masked patches\n\n# For downstream: use the encoder only\nfrom transformers import ViTModel\nencoder = ViTModel.from_pretrained('facebook/vit-mae-base')\nfeatures = encoder(**processor(images=image, return_tensors='pt')).last_hidden_state"
                }
              }
            ],
            "keyTakeaways": [
              "Self-supervised learning removes the labeling bottleneck",
              "SimCLR uses contrastive learning between augmented views",
              "CLIP enables zero-shot classification via language",
              "DINO learns semantic segmentation without any labels",
              "MAE trains efficiently by reconstructing masked patches"
            ]
          },
          "authorCredits": {
            "researchers": ["Chen", "Kornblith", "Norouzi", "Hinton", "Radford", "Kim", "Caron", "Touvron", "He", "Fan"],
            "note": "SimCLR was introduced by Ting Chen et al. at Google (2020). CLIP was developed by Alec Radford et al. at OpenAI (2021), enabling zero-shot vision. DINO was created by Mathilde Caron et al. at Facebook AI (2021). MAE by Kaiming He et al. (2022) showed aggressive masking works for images."
          }
        },
        {
          "id": "face-recognition",
          "title": "Face Detection & Recognition",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "Face technology powers authentication, photo organization, and AR filters. Learn face detection, alignment, embedding, and recognition.",
            "sections": [
              {
                "title": "Face Detection",
                "codeExample": {
                  "language": "python",
                  "code": "# Using RetinaFace (accurate)\nfrom retinaface import RetinaFace\nfaces = RetinaFace.detect_faces('image.jpg')\n\nfor face_id, face_data in faces.items():\n    bbox = face_data['facial_area']  # [x1, y1, x2, y2]\n    landmarks = face_data['landmarks']\n    # landmarks: left_eye, right_eye, nose, mouth_left, mouth_right\n\n# Using MediaPipe (fast, real-time)\nimport mediapipe as mp\n\nface_detection = mp.solutions.face_detection.FaceDetection(\n    model_selection=1, min_detection_confidence=0.5)\n\nresults = face_detection.process(rgb_image)\nfor detection in results.detections:\n    bbox = detection.location_data.relative_bounding_box"
                }
              },
              {
                "title": "Face Embeddings",
                "codeExample": {
                  "language": "python",
                  "code": "from deepface import DeepFace\nimport numpy as np\n\n# Extract face embedding (512-dimensional vector)\nembedding = DeepFace.represent(\n    img_path='face.jpg',\n    model_name='ArcFace',  # VGG-Face, Facenet, Facenet512, ArcFace\n    detector_backend='retinaface'\n)[0]['embedding']\n\n# Compare two faces\nresult = DeepFace.verify(\n    img1_path='person1.jpg',\n    img2_path='person2.jpg',\n    model_name='ArcFace'\n)\nprint(f\"Same person: {result['verified']}\")\nprint(f\"Distance: {result['distance']:.3f}\")\n\n# Find matches in database\nresults = DeepFace.find(\n    img_path='query.jpg',\n    db_path='face_database/',\n    model_name='ArcFace'\n)"
                }
              },
              {
                "title": "Face Recognition Pipeline",
                "diagram": {
                  "title": "Face Recognition Workflow",
                  "code": "flowchart LR\n    Input[Input Image] --> Detect[Face Detection]\n    Detect --> Align[Face Alignment]\n    Align --> Embed[Feature Extraction\\n(Embedding)]\n    Embed --> Match{Matching}\n    \n    subgraph Database\n        DB[(Face DB)]\n    end\n    \n    DB --> Match\n    Match -->|Score > Threshold| ID[Identified]\n    Match -->|Score < Threshold| Unk[Unknown]\n    \n    style Input fill:#fff,stroke:#333\n    style Detect fill:#dbeafe,stroke:#2563eb\n    style Embed fill:#fce7f3,stroke:#db2777\n    style Match fill:#fef3c7,stroke:#d97706"
                },
                "codeExample": {
                  "language": "python",
                  "code": "import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass FaceRecognizer:\n    def __init__(self):\n        self.known_embeddings = {}  # name -> embedding\n    \n    def register(self, name, image_path):\n        embedding = DeepFace.represent(\n            image_path, model_name='ArcFace'\n        )[0]['embedding']\n        self.known_embeddings[name] = embedding\n    \n    def recognize(self, image_path, threshold=0.5):\n        query_embedding = DeepFace.represent(\n            image_path, model_name='ArcFace'\n        )[0]['embedding']\n        \n        best_match, best_score = None, -1\n        for name, known_emb in self.known_embeddings.items():\n            score = cosine_similarity(\n                [query_embedding], [known_emb]\n            )[0][0]\n            if score > best_score:\n                best_score, best_match = score, name\n        \n        if best_score > threshold:\n            return best_match, best_score\n        return 'Unknown', best_score"
                }
              }
            ],
            "keyTakeaways": [
              "Face detection â†’ alignment â†’ embedding â†’ matching",
              "ArcFace and Facenet produce discriminative embeddings",
              "Cosine similarity measures face similarity",
              "Consider privacy and bias in face recognition systems"
            ]
          }
        },
        {
          "id": "ocr",
          "title": "OCR and Document Understanding",
          "type": "lesson",
          "duration": "25 min",
          "content": {
            "overview": "Optical Character Recognition (OCR) extracts text from images. Modern OCR goes beyond text to understand document structure.",
            "sections": [
              {
                "title": "Traditional OCR with Tesseract",
                "codeExample": {
                  "language": "python",
                  "code": "import pytesseract\nfrom PIL import Image\nimport cv2\n\n# Basic OCR\nimage = Image.open('document.png')\ntext = pytesseract.image_to_string(image)\nprint(text)\n\n# With bounding boxes\ndata = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\nfor i, word in enumerate(data['text']):\n    if word.strip():\n        x, y, w, h = (data['left'][i], data['top'][i],\n                      data['width'][i], data['height'][i])\n        conf = data['conf'][i]\n        print(f'{word}: ({x}, {y}) confidence: {conf}%')\n\n# Specify language\ntext = pytesseract.image_to_string(image, lang='eng+fra')"
                }
              },
              {
                "title": "Deep Learning OCR with EasyOCR",
                "codeExample": {
                  "language": "python",
                  "code": "import easyocr\n\n# Initialize reader (downloads model on first use)\nreader = easyocr.Reader(['en', 'fr'])  # Languages\n\n# Detect and recognize\nresults = reader.readtext('image.jpg')\n\nfor bbox, text, confidence in results:\n    print(f'{text} ({confidence:.2%})')\n    # bbox = [[x1,y1], [x2,y2], [x3,y3], [x4,y4]] - polygon\n\n# Scene text (text in natural images)\nresults = reader.readtext('street_sign.jpg', detail=0)  # Just text\n\n# PDF extraction\nimport pdf2image\npages = pdf2image.convert_from_path('document.pdf')\nfor i, page in enumerate(pages):\n    text = reader.readtext(np.array(page), detail=0)\n    print(f'Page {i+1}: {\" \".join(text)}')"
                }
              },
              {
                "title": "Document AI with LayoutLM",
                "codeExample": {
                  "language": "python",
                  "code": "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\nfrom PIL import Image\n\n# Load model for document understanding\nprocessor = LayoutLMv3Processor.from_pretrained('microsoft/layoutlmv3-base')\nmodel = LayoutLMv3ForTokenClassification.from_pretrained(\n    'microsoft/layoutlmv3-base-finetuned-funsd'  # Form understanding\n)\n\n# Process document image\nimage = Image.open('form.png').convert('RGB')\nencoding = processor(image, return_tensors='pt')\n\n# Get predictions\noutputs = model(**encoding)\npredictions = outputs.logits.argmax(-1).squeeze().tolist()\n\n# LayoutLM understands:\n# - Document structure (headers, paragraphs, tables)\n# - Form fields (questions, answers)\n# - Key-value pairs"
                }
              }
            ],
            "keyTakeaways": [
              "Tesseract is free and works for clean documents",
              "EasyOCR handles scene text and multiple languages",
              "LayoutLM understands document structure, not just text",
              "Preprocess images (binarize, deskew) for better OCR"
            ]
          }
        },
        {
          "id": "video-understanding",
          "title": "Video Understanding",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "Video adds the temporal dimension to vision. Learn to process videos, track objects, and classify actions.",
            "sections": [
              {
                "title": "Video Processing Basics",
                "codeExample": {
                  "language": "python",
                  "code": "import cv2\nimport torch\nfrom torchvision.io import read_video\n\n# OpenCV video reading\ncap = cv2.VideoCapture('video.mp4')\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nframes = []\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frames.append(frame)\ncap.release()\n\n# TorchVision (loads as tensor)\nvideo, audio, info = read_video('video.mp4')\nprint(f'Video shape: {video.shape}')  # (T, H, W, C)\n\n# Sample frames uniformly\ndef sample_frames(video, num_frames=16):\n    indices = torch.linspace(0, len(video)-1, num_frames).long()\n    return video[indices]"
                }
              },
              {
                "title": "Object Tracking",
                "codeExample": {
                  "language": "python",
                  "code": "from ultralytics import YOLO\n\n# Load YOLO with tracking\nmodel = YOLO('yolov8n.pt')\n\n# Track objects across frames\nresults = model.track(\n    source='video.mp4',\n    persist=True,  # Keep track IDs across frames\n    tracker='bytetrack.yaml'  # or botsort.yaml\n)\n\n# Process tracked results\nfor result in results:\n    boxes = result.boxes\n    if boxes.id is not None:\n        track_ids = boxes.id.int().tolist()\n        for box, track_id in zip(boxes, track_ids):\n            x1, y1, x2, y2 = box.xyxy[0].tolist()\n            cls = int(box.cls[0])\n            print(f'Track {track_id}: {model.names[cls]} at {x1:.0f},{y1:.0f}')\n\n# Save tracked video\nresults = model.track('video.mp4', save=True)"
                }
              },
              {
                "title": "Action Recognition",
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nfrom transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n\n# Load VideoMAE model\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-finetuned-kinetics')\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-kinetics')\n\n# Prepare video frames (16 frames of 224x224)\ndef prepare_video(video_path, num_frames=16):\n    frames = sample_frames_from_video(video_path, num_frames)\n    frames = [Image.fromarray(f) for f in frames]\n    return frames\n\nframes = prepare_video('action.mp4')\ninputs = processor(frames, return_tensors='pt')\n\n# Classify action\noutputs = model(**inputs)\npredicted_class = outputs.logits.argmax(-1).item()\naction = model.config.id2label[predicted_class]\nprint(f'Action: {action}')\n\n# Kinetics-400 has 400 action classes:\n# playing guitar, riding bike, cooking, dancing, etc."
                }
              }
            ],
            "keyTakeaways": [
              "Sample frames uniformly for action recognition",
              "ByteTrack and BoTSORT are SOTA multi-object trackers",
              "VideoMAE extends Vision Transformers to video",
              "3D CNNs (I3D, SlowFast) also work for video"
            ]
          }
        },
        {
          "id": "image-generation",
          "title": "Image Generation",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "Generative models create new images from noise or text. Learn about GANs, VAEs, and Diffusion models.",
            "sections": [
              {
                "title": "Generative Approaches",
                "content": "Three main paradigms for image generation:\n\n**GANs (Generative Adversarial Networks)**\n- Generator vs Discriminator game\n- Fast inference, training can be unstable\n- StyleGAN, BigGAN\n\n**VAEs (Variational Autoencoders)**\n- Learn latent space, probabilistic\n- Smooth interpolation, slightly blurry\n\n**Diffusion Models**\n- Iteratively denoise from pure noise\n- Current SOTA quality\n- Stable Diffusion, DALL-E, Imagen"
              },
              {
                "title": "Stable Diffusion",
                "codeExample": {
                  "language": "python",
                  "code": "from diffusers import StableDiffusionPipeline\nimport torch\n\n# Load Stable Diffusion\npipe = StableDiffusionPipeline.from_pretrained(\n    'stabilityai/stable-diffusion-2-1',\n    torch_dtype=torch.float16\n)\npipe = pipe.to('cuda')\n\n# Text-to-image\nimage = pipe(\n    prompt='a photo of an astronaut riding a horse on mars',\n    negative_prompt='blurry, low quality',\n    num_inference_steps=50,\n    guidance_scale=7.5\n).images[0]\n\nimage.save('generated.png')\n\n# Image-to-image\nfrom diffusers import StableDiffusionImg2ImgPipeline\nimg2img = StableDiffusionImg2ImgPipeline.from_pretrained(...)\nresult = img2img(prompt='...', image=init_image, strength=0.75)"
                }
              },
              {
                "title": "ControlNet for Guided Generation",
                "codeExample": {
                  "language": "python",
                  "code": "from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\nimport torch\n\n# Load ControlNet (edge detection guided)\ncontrolnet = ControlNetModel.from_pretrained(\n    'lllyasviel/sd-controlnet-canny',\n    torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    'runwayml/stable-diffusion-v1-5',\n    controlnet=controlnet,\n    torch_dtype=torch.float16\n).to('cuda')\n\n# Generate with edge control\nimport cv2\nfrom PIL import Image\n\nimage = cv2.imread('input.jpg')\nedges = cv2.Canny(image, 100, 200)\ncontrol_image = Image.fromarray(edges)\n\nresult = pipe(\n    prompt='detailed painting of a castle',\n    image=control_image,\n    num_inference_steps=30\n).images[0]"
                }
              }
            ],
            "keyTakeaways": [
              "Diffusion models are current SOTA for quality",
              "Negative prompts help avoid unwanted features",
              "ControlNet adds structural guidance",
              "CFG scale controls prompt adherence vs creativity"
            ]
          },
          "authorCredits": {
            "researchers": ["Goodfellow", "Pouget-Abadie", "Karras", "Laine", "Aila", "Ho", "Jain", "Abbeel", "Rombach", "Blattmann"],
            "note": "GANs were introduced by Ian Goodfellow et al. (2014). StyleGAN by Tero Karras et al. at NVIDIA achieved photorealistic face generation. Diffusion models were formalized by Jonathan Ho et al. (2020) with DDPM. Stable Diffusion by Robin Rombach et al. (2022) made diffusion practical via latent space compression."
          }
        },
        {
          "id": "model-optimization",
          "title": "Model Optimization & Deployment",
          "type": "lesson",
          "duration": "30 min",
          "content": {
            "overview": "Production CV models need to be fast and efficient. Learn quantization, pruning, and deployment strategies.",
            "sections": [
              {
                "title": "Model Quantization",
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nfrom torch.quantization import quantize_dynamic, get_default_qconfig\n\n# Dynamic quantization (easy, good for CPU)\nmodel_quantized = quantize_dynamic(\n    model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8\n)\n\n# Static quantization (better accuracy, needs calibration)\nmodel.qconfig = get_default_qconfig('fbgemm')  # or 'qnnpack' for mobile\nmodel_prepared = torch.quantization.prepare(model)\n\n# Calibrate with representative data\nwith torch.no_grad():\n    for batch in calibration_loader:\n        model_prepared(batch)\n\nmodel_quantized = torch.quantization.convert(model_prepared)\n\n# Size comparison\nprint(f'Original: {os.path.getsize(\"model.pt\") / 1e6:.1f} MB')\nprint(f'Quantized: {os.path.getsize(\"model_q.pt\") / 1e6:.1f} MB')  # ~4x smaller"
                }
              },
              {
                "title": "ONNX Export",
                "codeExample": {
                  "language": "python",
                  "code": "import torch\nimport onnx\nimport onnxruntime as ort\n\n# Export to ONNX\ndummy_input = torch.randn(1, 3, 224, 224)\ntorch.onnx.export(\n    model,\n    dummy_input,\n    'model.onnx',\n    input_names=['input'],\n    output_names=['output'],\n    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n)\n\n# Verify\nonnx_model = onnx.load('model.onnx')\nonnx.checker.check_model(onnx_model)\n\n# Run with ONNX Runtime\nsession = ort.InferenceSession('model.onnx', providers=['CUDAExecutionProvider'])\ninputs = {session.get_inputs()[0].name: input_tensor.numpy()}\noutputs = session.run(None, inputs)"
                }
              },
              {
                "title": "TensorRT Optimization",
                "codeExample": {
                  "language": "python",
                  "code": "# Convert ONNX to TensorRT\nimport tensorrt as trt\n\nlogger = trt.Logger(trt.Logger.WARNING)\nbuilder = trt.Builder(logger)\nnetwork = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\nparser = trt.OnnxParser(network, logger)\n\nwith open('model.onnx', 'rb') as f:\n    parser.parse(f.read())\n\nconfig = builder.create_builder_config()\nconfig.set_flag(trt.BuilderFlag.FP16)  # Enable FP16\n\nengine = builder.build_engine(network, config)\n\n# Or use torch-tensorrt for easier conversion\nimport torch_tensorrt\n\nmodel_trt = torch_tensorrt.compile(\n    model,\n    inputs=[torch_tensorrt.Input((1, 3, 224, 224))],\n    enabled_precisions={torch.float16}\n)\n# 2-5x faster inference on NVIDIA GPUs"
                }
              }
            ],
            "keyTakeaways": [
              "Quantization reduces model size 4x with minimal accuracy loss",
              "ONNX provides framework-agnostic deployment",
              "TensorRT optimizes inference on NVIDIA GPUs",
              "Profile before optimizing - find the actual bottleneck"
            ]
          }
        },
        {
          "id": "cv-best-practices",
          "title": "CV Best Practices",
          "type": "lesson",
          "duration": "20 min",
          "content": {
            "overview": "Learn practical tips and common pitfalls in computer vision projects from data collection to production.",
            "sections": [
              {
                "title": "Data Quality",
                "content": "**Data Collection**\n- Collect diverse data: lighting, angles, backgrounds\n- Balance classes or use weighted sampling\n- Include edge cases and failure modes\n\n**Labeling**\n- Use consistent labeling guidelines\n- Have multiple annotators for quality\n- Track inter-annotator agreement\n\n**Splits**\n- Ensure no data leakage between train/val/test\n- Stratify by important attributes\n- Consider temporal splits for time-sensitive data"
              },
              {
                "title": "Training Best Practices",
                "codeExample": {
                  "language": "python",
                  "code": "# Learning rate finder\nfrom torch_lr_finder import LRFinder\n\nlr_finder = LRFinder(model, optimizer, criterion)\nlr_finder.range_test(train_loader, end_lr=10, num_iter=100)\nlr_finder.plot()\nbest_lr = lr_finder.suggestion()\n\n# Cosine annealing with warmup\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n\n# Mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\nfor inputs, labels in train_loader:\n    optimizer.zero_grad()\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()"
                }
              },
              {
                "title": "Evaluation & Debugging",
                "codeExample": {
                  "language": "python",
                  "code": "# Confusion matrix analysis\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes)\n\n# Per-class metrics\nprint(classification_report(y_true, y_pred, target_names=classes))\n\n# Visualize failure cases\nwrong_indices = np.where(y_pred != y_true)[0]\nfor idx in wrong_indices[:10]:\n    img = test_dataset[idx][0]\n    print(f'True: {classes[y_true[idx]]}, Pred: {classes[y_pred[idx]]}')\n    plt.imshow(img.permute(1,2,0))\n    plt.show()\n\n# GradCAM for interpretability\nfrom pytorch_grad_cam import GradCAM\ncam = GradCAM(model, target_layers=[model.layer4[-1]])\ngrayscale_cam = cam(input_tensor=input_tensor)"
                }
              }
            ],
            "keyTakeaways": [
              "Diverse, quality data beats bigger models",
              "Always visualize your data and predictions",
              "LR finder and mixed precision accelerate training",
              "GradCAM helps understand model decisions"
            ]
          }
        },
        {
          "id": "cv-project",
          "title": "Capstone: End-to-End CV Project",
          "type": "project",
          "duration": "90 min",
          "content": {
            "overview": "Build a complete computer vision application from data to deployment.",
            "sections": [
              {
                "title": "Project: Product Image Classifier",
                "content": "**Objective**: Build an image classification system for e-commerce product categories.\n\n**Requirements**:\n1. Train a model to classify products into categories\n2. Handle images of varying quality and backgrounds\n3. Deploy as a REST API\n4. Include confidence thresholds and error handling"
              },
              {
                "title": "Project Implementation",
                "codeExample": {
                  "language": "python",
                  "code": "# Complete pipeline\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport io\nfrom fastapi import FastAPI, UploadFile, File\n\n# 1. Model definition\nclass ProductClassifier:\n    def __init__(self, num_classes, checkpoint_path):\n        self.model = models.efficientnet_b0(pretrained=False)\n        self.model.classifier[-1] = nn.Linear(1280, num_classes)\n        self.model.load_state_dict(torch.load(checkpoint_path))\n        self.model.eval()\n        \n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                [0.229, 0.224, 0.225])\n        ])\n        \n        self.classes = ['electronics', 'clothing', 'furniture', 'toys', 'food']\n    \n    def predict(self, image_bytes):\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        tensor = self.transform(image).unsqueeze(0)\n        \n        with torch.no_grad():\n            logits = self.model(tensor)\n            probs = torch.softmax(logits, dim=1)[0]\n        \n        top_prob, top_idx = probs.max(0)\n        return {\n            'class': self.classes[top_idx],\n            'confidence': float(top_prob),\n            'all_probs': {c: float(p) for c, p in zip(self.classes, probs)}\n        }\n\n# 2. FastAPI service\napp = FastAPI()\nclassifier = ProductClassifier(5, 'model.pt')\n\n@app.post('/classify')\nasync def classify(file: UploadFile = File(...)):\n    image_bytes = await file.read()\n    result = classifier.predict(image_bytes)\n    \n    if result['confidence'] < 0.5:\n        result['warning'] = 'Low confidence prediction'\n    \n    return result"
                }
              }
            ],
            "keyTakeaways": [
              "Build complete pipelines, not just models",
              "Handle edge cases and low-confidence predictions",
              "APIs need input validation and error handling",
              "Monitor model performance in production"
            ]
          }
        },
        {
          "id": "cv-quiz",
          "title": "Computer Vision Quiz",
          "type": "quiz",
          "duration": "15 min",
          "questions": [
            {
              "question": "What is the main advantage of skip connections in ResNet?",
              "options": [
                "Faster inference",
                "Enables training deeper networks",
                "Reduces model size",
                "Improves color accuracy"
              ],
              "correct": 1,
              "explanation": "Skip connections allow gradients to flow directly through the network, solving the vanishing gradient problem and enabling training of networks with 100+ layers."
            },
            {
              "question": "Which metric is commonly used to evaluate object detection models?",
              "options": [
                "Accuracy",
                "F1 Score",
                "mAP (mean Average Precision)",
                "R-squared"
              ],
              "correct": 2,
              "explanation": "mAP measures both localization (IoU threshold) and classification accuracy across all classes, making it the standard metric for detection."
            },
            {
              "question": "What does a Vision Transformer (ViT) use as input instead of raw pixels?",
              "options": [
                "Edge maps",
                "Image patches",
                "Color histograms",
                "Fourier transforms"
              ],
              "correct": 1,
              "explanation": "ViT splits an image into fixed-size patches (typically 16x16), which are then linearly embedded and processed as a sequence like text tokens."
            },
            {
              "question": "Which approach is best for generating images from text prompts?",
              "options": [
                "CNNs",
                "GANs",
                "Diffusion Models",
                "Autoencoders"
              ],
              "correct": 2,
              "explanation": "Diffusion models (like Stable Diffusion, DALL-E 3) are the current state-of-the-art for text-to-image generation, producing high-quality, diverse images."
            },
            {
              "question": "What is the purpose of TensorRT?",
              "options": [
                "Training neural networks",
                "Data augmentation",
                "Optimizing inference on NVIDIA GPUs",
                "Image labeling"
              ],
              "correct": 2,
              "explanation": "TensorRT optimizes neural network inference through techniques like layer fusion, precision calibration, and kernel auto-tuning for NVIDIA GPUs."
            },
            {
              "question": "What are the three main layer types in a CNN?",
              "options": [
                "Input, hidden, output",
                "Convolutional, pooling, fully-connected",
                "Encoder, decoder, attention",
                "Embedding, transformer, head"
              ],
              "correct": 1,
              "explanation": "CNNs consist of convolutional layers (feature extraction), pooling layers (dimensionality reduction), and fully-connected layers (classification based on extracted features)."
            },
            {
              "question": "What is the difference between semantic and instance segmentation?",
              "options": [
                "Semantic is faster, instance is slower",
                "Semantic uses colors, instance uses edges",
                "Semantic classifies pixels by class, instance separates individual objects",
                "Semantic works on videos, instance works on images"
              ],
              "correct": 2,
              "explanation": "Semantic segmentation assigns each pixel a class label (all cars = same class), while instance segmentation distinguishes individual objects (car1, car2, car3)."
            },
            {
              "question": "What does a feature map (activation map) represent in a CNN?",
              "options": [
                "The original image in a different color space",
                "The output of a convolution detecting specific patterns",
                "A map of important pixels",
                "The weights of the network"
              ],
              "correct": 1,
              "explanation": "A feature map is the output from applying a filter to an image through convolution. Each filter is tuned to detect specific patterns like edges, textures, or shapes."
            },
            {
              "question": "What is the role of data augmentation in computer vision?",
              "options": [
                "To compress images for faster training",
                "To expand dataset diversity and prevent overfitting",
                "To convert images to grayscale",
                "To label images automatically"
              ],
              "correct": 1,
              "explanation": "Data augmentation applies transformations (rotation, flipping, color jitter) to training images to expand dataset size and diversity, helping prevent overfitting."
            },
            {
              "question": "What are R-CNN and YOLO primarily used for?",
              "options": [
                "Image classification",
                "Semantic segmentation",
                "Object detection",
                "Image generation"
              ],
              "correct": 2,
              "explanation": "R-CNN (region-based CNN) and YOLO (You Only Look Once) are object detection architectures. R-CNN uses two-stage detection, while YOLO performs single-stage detection for real-time speed."
            },
            {
              "question": "What is monocular depth estimation?",
              "options": [
                "Using two cameras to measure depth",
                "Predicting depth from a single image using learned priors",
                "Using LiDAR sensors for depth measurement",
                "Measuring depth with structured light"
              ],
              "correct": 1,
              "explanation": "Monocular depth estimation uses a single image to predict depth at each pixel, leveraging learned visual cues like perspective, occlusion, and texture gradients."
            },
            {
              "question": "What is the main advantage of NeRF (Neural Radiance Fields)?",
              "options": [
                "Faster training than CNNs",
                "Rendering photorealistic novel views from any angle",
                "Better object detection accuracy",
                "Lower memory requirements"
              ],
              "correct": 1,
              "explanation": "NeRF can synthesize photorealistic new views of a scene from arbitrary camera positions, by learning a continuous 3D representation from 2D images."
            },
            {
              "question": "What is the three R's framework in computer vision?",
              "options": [
                "Read, Recognize, Respond",
                "Recognition, Reconstruction, Reorganization",
                "Resize, Rotate, Reflect",
                "Render, Refine, Repeat"
              ],
              "correct": 1,
              "explanation": "The three R's of computer vision are Recognition (identifying objects, people, places), Reconstruction (deriving 3D characteristics), and Reorganization (inferring relationships between entities)."
            }
          ],
          "references": {
            "lessonRefs": [
              "cnn-basics",
              "transfer-learning",
              "object-detection"
            ],
            "externalRefs": [
              {
                "title": "PyTorch Vision",
                "url": "https://pytorch.org/vision/"
              },
              {
                "title": "CS231n Course",
                "url": "https://cs231n.stanford.edu/"
              }
            ]
          }
        }
      ]
    }
  ]
}