{
  "id": "transformers-deep-dive",
  "title": "Transformer Architecture Deep Dive",
  "description": "Detailed breakdown of the Transformer architecture and its variants",
  "sections": [
    { "id": "architecture", "title": "Architecture", "color": "#6366f1" },
    { "id": "component", "title": "Components", "color": "#10b981" },
    { "id": "variant", "title": "Variants", "color": "#f59e0b" },
    { "id": "model", "title": "Models", "color": "#ef4444" },
    { "id": "optimization", "title": "Optimizations", "color": "#8b5cf6" }
  ],
  "nodes": [
    {"id": "transformer", "label": "Transformer", "type": "root", "section": "architecture", "description": "Vaswani et al. 2017 - Attention Is All You Need", "difficulty": "Advanced", "url": "https://arxiv.org/abs/1706.03762"},
    
    {"id": "encoder", "label": "Encoder", "type": "concept", "section": "architecture", "description": "Bidirectional context processing", "difficulty": "Intermediate"},
    {"id": "decoder", "label": "Decoder", "type": "concept", "section": "architecture", "description": "Autoregressive generation", "difficulty": "Intermediate"},
    
    {"id": "input-embedding", "label": "Input Embedding", "type": "component", "section": "component", "description": "Token to vector", "difficulty": "Intermediate"},
    {"id": "positional-encoding", "label": "Positional Encoding", "type": "component", "section": "component", "description": "Sinusoidal or learned", "difficulty": "Advanced", "url": "https://kazemnejad.com/blog/transformer_architecture_positional_encoding/"},
    {"id": "self-attention", "label": "Self-Attention", "type": "component", "section": "component", "description": "Query-Key-Value mechanism", "difficulty": "Advanced", "url": "https://jalammar.github.io/illustrated-transformer/"},
    {"id": "masked-attention", "label": "Masked Self-Attention", "type": "component", "section": "component", "description": "Causal attention for decoder", "difficulty": "Advanced"},
    {"id": "cross-attention", "label": "Cross-Attention", "type": "component", "section": "component", "description": "Attend to encoder output", "difficulty": "Advanced"},
    
    {"id": "multi-head", "label": "Multi-Head Attention", "type": "component", "section": "component", "description": "Parallel attention heads", "difficulty": "Advanced"},
    {"id": "scaled-dot-product", "label": "Scaled Dot-Product", "type": "component", "section": "component", "description": "QK^T / sqrt(d_k)", "difficulty": "Advanced"},
    {"id": "attention-weights", "label": "Attention Weights", "type": "component", "section": "component", "description": "Softmax normalization", "difficulty": "Advanced"},
    
    {"id": "feed-forward", "label": "Feed Forward", "type": "component", "section": "component", "description": "Position-wise MLP", "difficulty": "Intermediate"},
    {"id": "layer-norm", "label": "Layer Normalization", "type": "component", "section": "component", "description": "Stabilize training", "difficulty": "Intermediate"},
    {"id": "residual", "label": "Residual Connections", "type": "component", "section": "component", "description": "Skip connections", "difficulty": "Intermediate"},
    
    {"id": "encoder-only", "label": "Encoder-Only", "type": "variant", "section": "variant", "description": "BERT, RoBERTa, ALBERT", "difficulty": "Intermediate"},
    {"id": "decoder-only", "label": "Decoder-Only", "type": "variant", "section": "variant", "description": "GPT, Llama, Claude", "difficulty": "Intermediate"},
    {"id": "encoder-decoder", "label": "Encoder-Decoder", "type": "variant", "section": "variant", "description": "T5, BART, mT5", "difficulty": "Intermediate"},
    
    {"id": "bert", "label": "BERT", "type": "model", "section": "model", "description": "Masked language modeling", "difficulty": "Intermediate", "url": "https://arxiv.org/abs/1810.04805"},
    {"id": "roberta", "label": "RoBERTa", "type": "model", "section": "model", "description": "Robustly optimized BERT", "difficulty": "Intermediate", "url": "https://arxiv.org/abs/1907.11692"},
    {"id": "electra", "label": "ELECTRA", "type": "model", "section": "model", "description": "Replaced token detection", "difficulty": "Advanced", "url": "https://arxiv.org/abs/2003.10555"},
    
    {"id": "gpt", "label": "GPT Family", "type": "model", "section": "model", "description": "GPT-2, GPT-3, GPT-4", "difficulty": "Intermediate", "url": "https://openai.com/research/gpt-4"},
    {"id": "llama", "label": "Llama Family", "type": "model", "section": "model", "description": "Meta's open models", "difficulty": "Intermediate", "url": "https://llama.meta.com/"},
    {"id": "mistral", "label": "Mistral", "type": "model", "section": "model", "description": "Efficient architecture", "difficulty": "Intermediate", "url": "https://mistral.ai/"},
    
    {"id": "t5", "label": "T5", "type": "model", "section": "model", "description": "Text-to-Text framework", "difficulty": "Intermediate", "url": "https://arxiv.org/abs/1910.10683"},
    {"id": "bart", "label": "BART", "type": "model", "section": "model", "description": "Denoising autoencoder", "difficulty": "Intermediate", "url": "https://arxiv.org/abs/1910.13461"},
    {"id": "flan", "label": "FLAN-T5", "type": "model", "section": "model", "description": "Instruction-tuned T5", "difficulty": "Advanced", "url": "https://arxiv.org/abs/2210.11416"},
    
    {"id": "sparse-attention", "label": "Sparse Attention", "type": "optimization", "section": "optimization", "description": "Longformer, BigBird", "difficulty": "Advanced", "url": "https://arxiv.org/abs/2004.05150"},
    {"id": "flash-attention", "label": "Flash Attention", "type": "optimization", "section": "optimization", "description": "Memory-efficient attention", "difficulty": "Advanced", "url": "https://arxiv.org/abs/2205.14135"},
    {"id": "moe", "label": "Mixture of Experts", "type": "optimization", "section": "optimization", "description": "Mixtral, Switch Transformer", "difficulty": "Advanced", "url": "https://arxiv.org/abs/2101.03961"},
    {"id": "rope", "label": "RoPE", "type": "optimization", "section": "optimization", "description": "Rotary Position Embedding", "difficulty": "Advanced", "url": "https://arxiv.org/abs/2104.09864"}
  ],
  "links": [
    {"source": "transformer", "target": "encoder", "type": "contains"},
    {"source": "transformer", "target": "decoder", "type": "contains"},
    
    {"source": "encoder", "target": "input-embedding", "type": "uses"},
    {"source": "encoder", "target": "positional-encoding", "type": "uses"},
    {"source": "encoder", "target": "self-attention", "type": "uses"},
    
    {"source": "decoder", "target": "masked-attention", "type": "uses"},
    {"source": "decoder", "target": "cross-attention", "type": "uses"},
    
    {"source": "self-attention", "target": "multi-head", "type": "implements"},
    {"source": "self-attention", "target": "scaled-dot-product", "type": "implements"},
    {"source": "multi-head", "target": "attention-weights", "type": "produces"},
    
    {"source": "cross-attention", "target": "feed-forward", "type": "feeds"},
    {"source": "feed-forward", "target": "layer-norm", "type": "feeds"},
    {"source": "layer-norm", "target": "residual", "type": "feeds"},
    
    {"source": "encoder", "target": "encoder-only", "type": "basis-for"},
    {"source": "decoder", "target": "decoder-only", "type": "basis-for"},
    {"source": "encoder", "target": "encoder-decoder", "type": "basis-for"},
    {"source": "decoder", "target": "encoder-decoder", "type": "basis-for"},
    
    {"source": "encoder-only", "target": "bert", "type": "example"},
    {"source": "encoder-only", "target": "roberta", "type": "example"},
    {"source": "encoder-only", "target": "electra", "type": "example"},
    
    {"source": "decoder-only", "target": "gpt", "type": "example"},
    {"source": "decoder-only", "target": "llama", "type": "example"},
    {"source": "decoder-only", "target": "mistral", "type": "example"},
    
    {"source": "encoder-decoder", "target": "t5", "type": "example"},
    {"source": "encoder-decoder", "target": "bart", "type": "example"},
    {"source": "encoder-decoder", "target": "flan", "type": "example"},
    
    {"source": "multi-head", "target": "sparse-attention", "type": "optimized-by"},
    {"source": "multi-head", "target": "flash-attention", "type": "optimized-by"},
    {"source": "decoder-only", "target": "moe", "type": "optimized-by"},
    {"source": "positional-encoding", "target": "rope", "type": "optimized-by"}
  ]
}
